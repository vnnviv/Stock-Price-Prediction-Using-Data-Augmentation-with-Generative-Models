[PHASE_1_(1)_(14) (5).ipynb](https://github.com/user-attachments/files/22991312/PHASE_1_.1._.14.5.ipynb)## Project Contents

  Colab Notebook ---> [Uploading PHASE_{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "86053a26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ad9d19-123e-4cd3-dc54-9b05f3b2d59f"
      },
      "source": [
        "!pip install laplace-torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: laplace-torch in /usr/local/lib/python3.12/dist-packages (0.2.2.2)\n",
            "Requirement already satisfied: asdfghjkl==0.1a4 in /usr/local/lib/python3.12/dist-packages (from laplace-torch) (0.1a4)\n",
            "Requirement already satisfied: backpack-for-pytorch in /usr/local/lib/python3.12/dist-packages (from laplace-torch) (1.7.1)\n",
            "Requirement already satisfied: curvlinops-for-pytorch>=2.0 in /usr/local/lib/python3.12/dist-packages (from laplace-torch) (2.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from laplace-torch) (1.26.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from laplace-torch) (3.4.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from laplace-torch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (from laplace-torch) (1.8.2)\n",
            "Requirement already satisfied: torchvision>=0.15 in /usr/local/lib/python3.12/dist-packages (from laplace-torch) (0.23.0+cu126)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from curvlinops-for-pytorch>=2.0->laplace-torch) (1.16.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.61.0 in /usr/local/lib/python3.12/dist-packages (from curvlinops-for-pytorch>=2.0->laplace-torch) (4.67.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from curvlinops-for-pytorch>=2.0->laplace-torch) (0.8.1)\n",
            "Requirement already satisfied: einconv in /usr/local/lib/python3.12/dist-packages (from curvlinops-for-pytorch>=2.0->laplace-torch) (0.1.0)\n",
            "Requirement already satisfied: unfoldNd<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from backpack-for-pytorch->laplace-torch) (0.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->laplace-torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.15->laplace-torch) (11.3.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics->laplace-torch) (25.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics->laplace-torch) (0.15.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->laplace-torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->laplace-torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ipHYANaAIht"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 2. Import Required Libraries\n",
        "# =====================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from scipy.stats import ttest_rel, wilcoxon\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "# =====================\n",
        "# 3. Data Loading and Preprocessing\n",
        "# =====================\n",
        "def get_stock_data(ticker, start, end):\n",
        "    print(f\"Downloading data for {ticker} from {start} to {end}...\")\n",
        "    df = yf.download(ticker, start=start, end=end)\n",
        "    print(\"Data download finished.\")\n",
        "    return df\n",
        "\n",
        "def scale_data(data):\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n",
        "    return scaled_data, scaler\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "print(\"Checked existing downloads. Please provide specific dataset identifiers for any additional datasets you'd like to download.\")"
      ],
      "metadata": {
        "id": "kdRhXhsSAVRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc74592-5499-48f6-eeb3-c03df38de1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checked existing downloads. Please provide specific dataset identifiers for any additional datasets you'd like to download.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# 1. Install Required Libraries (Run this once)\n",
        "# =====================\n",
        "!pip install yfinance\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib seaborn\n",
        "!pip install statsmodels\n",
        "!pip install pandas numpy\n",
        "!pip install scipy"
      ],
      "metadata": {
        "id": "OfoTAbeNAdoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41e7ab2-78cb-4857-971b-29c8e1525d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.65)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.32.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.4.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.5)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.15.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.5)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.26.4)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.16.1)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.1)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_model_with_early_stopping(model, X_full, y_full, model_save_path, epochs=100, patience=10):\n",
        "    \"\"\"\n",
        "    Trains an LSTM model with early stopping based on validation loss.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The LSTM model to train.\n",
        "        X_full (torch.Tensor): Full dataset features.\n",
        "        y_full (torch.Tensor): Full dataset targets.\n",
        "        model_save_path (str): Path to save the best model.\n",
        "        epochs (int): Maximum number of training epochs.\n",
        "        patience (int): Number of epochs to wait for improvement before stopping.\n",
        "    \"\"\"\n",
        "    # Split data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_full.cpu().numpy(), y_full.cpu().numpy(), test_size=0.2, random_state=42)\n",
        "    X_train, X_val = torch.tensor(X_train).float().to(X_full.device), torch.tensor(X_val).float().to(X_full.device)\n",
        "    y_train, y_val = torch.tensor(y_train).float().to(y_full.device), torch.tensor(y_val).float().to(y_full.device)\n",
        "\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    print(f\"Starting training with patience: {patience}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        outputs = model(X_train)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss_function(outputs, y_train)\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = loss_function(val_outputs, y_val)\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Validation loss improved. Model saved to {model_save_path}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == patience:\n",
        "                print(f\"Early stopping triggered after {epochs_no_improve} epochs without improvement.\")\n",
        "                break\n",
        "\n",
        "    # Load the best model found during training\n",
        "    model.load_state_dict(torch.load(model_save_path))\n",
        "    print(f'Final model loaded from {model_save_path} (best performing model during training).')"
      ],
      "metadata": {
        "id": "gTfJRiBbdh-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# =====================\n",
        "# Helper Functions (Keep these as they were)\n",
        "# =====================\n",
        "\n",
        "def get_stock_data(ticker, start, end):\n",
        "    df = yf.download(ticker, start=start, end=end)\n",
        "    df = df[['Close']].dropna()\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "def generate_synthetic_data(real_data, noise_level=0.05):\n",
        "    clean_data = real_data.dropna().reset_index(drop=True)\n",
        "    noise = np.random.normal(loc=0.0, scale=noise_level, size=clean_data.shape)\n",
        "    synthetic = clean_data.values * (1 + noise)\n",
        "    synthetic_df = pd.DataFrame(synthetic, columns=clean_data.columns)\n",
        "    synthetic_df = synthetic_df.fillna(method='ffill').fillna(method='bfill')\n",
        "    return synthetic_df\n",
        "\n",
        "def scale_data(data):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled = scaler.fit_transform(data.values.reshape(-1, 1) if data.ndim == 2 and data.shape[1] == 1 else data.values)\n",
        "    return scaled, scaler\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def split_data(X, y, train_ratio=0.8):\n",
        "    split_idx = int(len(X) * train_ratio)\n",
        "    X_train = X[:split_idx]\n",
        "    X_test = X[split_idx:]\n",
        "    y_train = y[:split_idx]\n",
        "    y_test = y[split_idx:]\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    y_true_flat = y_true.flatten() if y_true.ndim > 1 else y_true\n",
        "    y_pred_flat = y_pred.flatten() if y_pred.ndim > 1 else y_pred\n",
        "\n",
        "    mse = mean_squared_error(y_true_flat, y_pred_flat)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true_flat, y_pred_flat)\n",
        "\n",
        "    if len(y_true_flat) > 1 and len(y_pred_flat) > 1:\n",
        "        y_true_diff = np.diff(y_true_flat)\n",
        "        y_pred_diff = np.diff(y_pred_flat)\n",
        "        correct_direction = np.sum(np.sign(y_true_diff) == np.sign(y_pred_diff))\n",
        "        directional_accuracy = correct_direction / len(y_true_diff) if len(y_true_diff) > 0 else np.nan\n",
        "    else:\n",
        "        directional_accuracy = np.nan\n",
        "\n",
        "    return {\n",
        "        \"MSE\": mse,\n",
        "        \"RMSE\": rmse,\n",
        "        \"R2\": r2,\n",
        "        \"Directional Accuracy\": directional_accuracy\n",
        "    }\n",
        "\n",
        "def compare_models(metric_real, metric_synthetic, metric_name=\"MSE\"):\n",
        "    print(f\"Comparing {metric_name} between Real-trained vs Synthetic-trained:\")\n",
        "    stat, p = None, None\n",
        "    if len(metric_real) > 1 and len(metric_synthetic) > 1 and len(metric_real) == len(metric_synthetic):\n",
        "        stat, p = ttest_rel(metric_real, metric_synthetic)\n",
        "        print(f\"Paired T-test: stat={stat:.4f}, p={p:.4f}\")\n",
        "    elif len(metric_real) == 1 and len(metric_synthetic) == 1:\n",
        "        print(f\"Single trial comparison - Real-trained {metric_name}: {metric_real[0]:.4f}, Synthetic-trained {metric_name}: {metric_synthetic[0]:.4f}\")\n",
        "    else:\n",
        "        print(\"Cannot perform statistical test: Mismatched or insufficient data for comparison.\")\n",
        "    return stat, p\n",
        "\n",
        "# =====================\n",
        "# Corrected Define LSTMModel\n",
        "# =====================\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=50, num_layers=2, dropout_rate=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# =====================\n",
        "# NEW: Train function with early stopping\n",
        "# =====================\n",
        "def train_model_with_early_stopping(model, X_full, y_full, model_save_path, epochs=100, patience=10):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    X_full_cpu = X_full.cpu().numpy() if X_full.is_cuda else X_full.numpy()\n",
        "    y_full_cpu = y_full.cpu().numpy() if y_full.is_cuda else y_full.numpy()\n",
        "\n",
        "    X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(X_full_cpu, y_full_cpu, test_size=0.2, random_state=42)\n",
        "\n",
        "    X_train, X_val = torch.tensor(X_train_np).float().to(device), torch.tensor(X_val_np).float().to(device)\n",
        "    y_train, y_val = torch.tensor(y_train_np).float().to(device), torch.tensor(y_val_np).float().to(device)\n",
        "\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    print(f\"Starting training with patience: {patience}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        outputs = model(X_train)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss_function(outputs, y_train)\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = loss_function(val_outputs, y_val)\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Validation loss improved. Model saved to {model_save_path}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == patience:\n",
        "                print(f\"Early stopping triggered after {epochs_no_improve} epochs without improvement.\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(model_save_path))\n",
        "    print(f'Final model loaded from {model_save_path} (best performing model during training).')\n",
        "\n",
        "# =====================\n",
        "# Main execution with trials (CORRECTED)\n",
        "# =====================\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "seq_length = 10\n",
        "\n",
        "# --- Load and prepare Real Data ---\n",
        "real_data = get_stock_data(\"AAPL\", \"2020-01-01\", \"2023-01-01\")\n",
        "scaled_real_data, real_scaler = scale_data(real_data)\n",
        "X_real, y_real = create_sequences(scaled_real_data, seq_length)\n",
        "X_train_real = torch.tensor(X_real, dtype=torch.float32).to(device)\n",
        "y_train_real = torch.tensor(y_real, dtype=torch.float32).to(device) # FIXED: Removed .unsqueeze(1)\n",
        "\n",
        "# --- Load and prepare Synthetic Data ---\n",
        "synthetic_data = generate_synthetic_data(real_data)\n",
        "scaled_synthetic_data, synthetic_scaler = scale_data(synthetic_data)\n",
        "X_synthetic, y_synthetic = create_sequences(scaled_synthetic_data, seq_length)\n",
        "X_train_synthetic = torch.tensor(X_synthetic, dtype=torch.float32).to(device)\n",
        "y_train_synthetic = torch.tensor(y_synthetic, dtype=torch.float32).to(device) # FIXED: Removed .unsqueeze(1)\n",
        "\n",
        "# --- Real Data Training with Trials and Early Stopping ---\n",
        "num_trials = 15\n",
        "epochs_per_trial = 50\n",
        "patience_value = 10\n",
        "model_real_path = 'lstm_model_real_best.pth'\n",
        "\n",
        "input_size = 1\n",
        "hidden_size = 50\n",
        "num_layers = 2\n",
        "dropout_rate = 0.2\n",
        "model_real = LSTMModel(input_size, hidden_size, num_layers, dropout_rate).to(device)\n",
        "\n",
        "for trial in range(1, num_trials + 1):\n",
        "    print(f\"\\n--- Starting REAL data training for Trial {trial} ---\")\n",
        "    if trial > 1 and os.path.exists(model_real_path):\n",
        "        model_real.load_state_dict(torch.load(model_real_path))\n",
        "        print(f\"Loaded model state from {model_real_path} to continue training.\")\n",
        "\n",
        "    train_model_with_early_stopping(\n",
        "        model=model_real,\n",
        "        X_full=X_train_real,\n",
        "        y_full=y_train_real,\n",
        "        model_save_path=model_real_path,\n",
        "        epochs=epochs_per_trial,\n",
        "        patience=patience_value\n",
        "    )\n",
        "    print(f\"--- Completed REAL data training for Trial {trial} ---\")\n",
        "\n",
        "# --- Synthetic Data Training with Trials and Early Stopping ---\n",
        "model_synthetic = LSTMModel(input_size, hidden_size, num_layers, dropout_rate).to(device)\n",
        "model_synthetic_path = 'lstm_model_synthetic_best.pth'\n",
        "\n",
        "for trial in range(1, num_trials + 1):\n",
        "    print(f\"\\n--- Starting SYNTHETIC data training for Trial {trial} ---\")\n",
        "    if trial > 1 and os.path.exists(model_synthetic_path):\n",
        "        model_synthetic.load_state_dict(torch.load(model_synthetic_path))\n",
        "        print(f\"Loaded model state from {model_synthetic_path} to continue training.\")\n",
        "\n",
        "    train_model_with_early_stopping(\n",
        "        model=model_synthetic,\n",
        "        X_full=X_train_synthetic,\n",
        "        y_full=y_train_synthetic,\n",
        "        model_save_path=model_synthetic_path,\n",
        "        epochs=epochs_per_trial,\n",
        "        patience=patience_value\n",
        "    )\n",
        "    print(f\"--- Completed SYNTHETIC data training for Trial {trial} ---\")\n",
        "\n",
        "# --- Final Evaluation and Comparison ---\n",
        "print(\"\\n--- Final Evaluation and Comparison of Best Models ---\")\n",
        "# Re-evaluate the best real-trained model on the test data\n",
        "model_real.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_real = X_real[int(len(X_real) * 0.8):]\n",
        "    y_test_real = y_real[int(len(y_real) * 0.8):]\n",
        "    X_test_real_tensor = torch.tensor(X_test_real, dtype=torch.float32).to(device)\n",
        "    y_test_real_tensor = torch.tensor(y_test_real, dtype=torch.float32).to(device) # No .unsqueeze(1) needed here either\n",
        "    real_predictions_scaled = model_real(X_test_real_tensor).cpu().numpy()\n",
        "    y_test_real_rescaled = real_scaler.inverse_transform(y_test_real_tensor.cpu().numpy())\n",
        "    real_predictions_rescaled = real_scaler.inverse_transform(real_predictions_scaled)\n",
        "    final_real_metrics = evaluate_model(y_test_real_rescaled, real_predictions_rescaled)\n",
        "    print(\"Final Real-trained Model Metrics:\", final_real_metrics)\n",
        "\n",
        "# Re-evaluate the best synthetic-trained model on the test data\n",
        "model_synthetic.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_synthetic = X_synthetic[int(len(X_synthetic) * 0.8):]\n",
        "    y_test_synthetic = y_synthetic[int(len(y_synthetic) * 0.8):]\n",
        "    X_test_synthetic_tensor = torch.tensor(X_test_synthetic, dtype=torch.float32).to(device)\n",
        "    y_test_synthetic_tensor = torch.tensor(y_test_synthetic, dtype=torch.float32).to(device) # No .unsqueeze(1) needed here either\n",
        "    synthetic_predictions_scaled = model_synthetic(X_test_synthetic_tensor).cpu().numpy()\n",
        "    y_test_synthetic_rescaled = synthetic_scaler.inverse_transform(y_test_synthetic_tensor.cpu().numpy())\n",
        "    synthetic_predictions_rescaled = synthetic_scaler.inverse_transform(synthetic_predictions_scaled)\n",
        "    final_synthetic_metrics = evaluate_model(y_test_synthetic_rescaled, synthetic_predictions_rescaled)\n",
        "    print(\"Final Synthetic-trained Model Metrics:\", final_synthetic_metrics)\n",
        "\n",
        "# Final comparison\n",
        "compare_models([final_real_metrics['MSE']], [final_synthetic_metrics['MSE']], metric_name=\"MSE\")"
      ],
      "metadata": {
        "id": "-4Op95HWAgx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bad24a5-b84c-4fc4-a50b-d8a187f57a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1610930636.py:6: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=start, end=end)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-1610930636.py:16: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  synthetic_df = synthetic_df.fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting REAL data training for Trial 1 ---\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.3705, Val Loss: 0.3234\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.3485, Val Loss: 0.3032\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.3271, Val Loss: 0.2836\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [4/50], Train Loss: 0.3064, Val Loss: 0.2642\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [5/50], Train Loss: 0.2858, Val Loss: 0.2451\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [6/50], Train Loss: 0.2654, Val Loss: 0.2262\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [7/50], Train Loss: 0.2450, Val Loss: 0.2073\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.2251, Val Loss: 0.1884\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [9/50], Train Loss: 0.2048, Val Loss: 0.1695\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [10/50], Train Loss: 0.1839, Val Loss: 0.1506\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [11/50], Train Loss: 0.1638, Val Loss: 0.1318\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [12/50], Train Loss: 0.1433, Val Loss: 0.1133\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [13/50], Train Loss: 0.1226, Val Loss: 0.0955\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.1030, Val Loss: 0.0791\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [15/50], Train Loss: 0.0840, Val Loss: 0.0651\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [16/50], Train Loss: 0.0676, Val Loss: 0.0551\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [17/50], Train Loss: 0.0541, Val Loss: 0.0511\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [18/50], Train Loss: 0.0478, Val Loss: 0.0548\n",
            "Epoch [19/50], Train Loss: 0.0479, Val Loss: 0.0657\n",
            "Epoch [20/50], Train Loss: 0.0560, Val Loss: 0.0779\n",
            "Epoch [21/50], Train Loss: 0.0664, Val Loss: 0.0842\n",
            "Epoch [22/50], Train Loss: 0.0717, Val Loss: 0.0829\n",
            "Epoch [23/50], Train Loss: 0.0706, Val Loss: 0.0762\n",
            "Epoch [24/50], Train Loss: 0.0647, Val Loss: 0.0675\n",
            "Epoch [25/50], Train Loss: 0.0577, Val Loss: 0.0594\n",
            "Epoch [26/50], Train Loss: 0.0511, Val Loss: 0.0533\n",
            "Epoch [27/50], Train Loss: 0.0459, Val Loss: 0.0495\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [28/50], Train Loss: 0.0436, Val Loss: 0.0475\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [29/50], Train Loss: 0.0432, Val Loss: 0.0469\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [30/50], Train Loss: 0.0433, Val Loss: 0.0470\n",
            "Epoch [31/50], Train Loss: 0.0448, Val Loss: 0.0475\n",
            "Epoch [32/50], Train Loss: 0.0458, Val Loss: 0.0478\n",
            "Epoch [33/50], Train Loss: 0.0470, Val Loss: 0.0479\n",
            "Epoch [34/50], Train Loss: 0.0472, Val Loss: 0.0476\n",
            "Epoch [35/50], Train Loss: 0.0471, Val Loss: 0.0470\n",
            "Epoch [36/50], Train Loss: 0.0459, Val Loss: 0.0460\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [37/50], Train Loss: 0.0451, Val Loss: 0.0448\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [38/50], Train Loss: 0.0436, Val Loss: 0.0435\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [39/50], Train Loss: 0.0420, Val Loss: 0.0423\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [40/50], Train Loss: 0.0401, Val Loss: 0.0412\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [41/50], Train Loss: 0.0385, Val Loss: 0.0404\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [42/50], Train Loss: 0.0370, Val Loss: 0.0401\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [43/50], Train Loss: 0.0363, Val Loss: 0.0400\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [44/50], Train Loss: 0.0357, Val Loss: 0.0402\n",
            "Epoch [45/50], Train Loss: 0.0352, Val Loss: 0.0405\n",
            "Epoch [46/50], Train Loss: 0.0354, Val Loss: 0.0405\n",
            "Epoch [47/50], Train Loss: 0.0351, Val Loss: 0.0401\n",
            "Epoch [48/50], Train Loss: 0.0349, Val Loss: 0.0392\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [49/50], Train Loss: 0.0335, Val Loss: 0.0378\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [50/50], Train Loss: 0.0331, Val Loss: 0.0361\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 1 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 2 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0314, Val Loss: 0.0320\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0294, Val Loss: 0.0305\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0285, Val Loss: 0.0285\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0262, Val Loss: 0.0273\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0242, Val Loss: 0.0261\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0227, Val Loss: 0.0241\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0217, Val Loss: 0.0214\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0193, Val Loss: 0.0188\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [9/50], Train Loss: 0.0171, Val Loss: 0.0166\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [10/50], Train Loss: 0.0154, Val Loss: 0.0145\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [11/50], Train Loss: 0.0136, Val Loss: 0.0124\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [12/50], Train Loss: 0.0115, Val Loss: 0.0106\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [13/50], Train Loss: 0.0098, Val Loss: 0.0090\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0084, Val Loss: 0.0073\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [15/50], Train Loss: 0.0072, Val Loss: 0.0056\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [16/50], Train Loss: 0.0058, Val Loss: 0.0044\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [17/50], Train Loss: 0.0049, Val Loss: 0.0036\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [18/50], Train Loss: 0.0045, Val Loss: 0.0030\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [19/50], Train Loss: 0.0039, Val Loss: 0.0029\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [20/50], Train Loss: 0.0039, Val Loss: 0.0028\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [21/50], Train Loss: 0.0044, Val Loss: 0.0031\n",
            "Epoch [22/50], Train Loss: 0.0047, Val Loss: 0.0034\n",
            "Epoch [23/50], Train Loss: 0.0051, Val Loss: 0.0035\n",
            "Epoch [24/50], Train Loss: 0.0053, Val Loss: 0.0037\n",
            "Epoch [25/50], Train Loss: 0.0056, Val Loss: 0.0036\n",
            "Epoch [26/50], Train Loss: 0.0053, Val Loss: 0.0036\n",
            "Epoch [27/50], Train Loss: 0.0051, Val Loss: 0.0033\n",
            "Epoch [28/50], Train Loss: 0.0049, Val Loss: 0.0030\n",
            "Epoch [29/50], Train Loss: 0.0046, Val Loss: 0.0028\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [30/50], Train Loss: 0.0042, Val Loss: 0.0026\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [31/50], Train Loss: 0.0041, Val Loss: 0.0024\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [32/50], Train Loss: 0.0037, Val Loss: 0.0024\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [33/50], Train Loss: 0.0036, Val Loss: 0.0023\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [34/50], Train Loss: 0.0034, Val Loss: 0.0024\n",
            "Epoch [35/50], Train Loss: 0.0034, Val Loss: 0.0025\n",
            "Epoch [36/50], Train Loss: 0.0036, Val Loss: 0.0025\n",
            "Epoch [37/50], Train Loss: 0.0036, Val Loss: 0.0026\n",
            "Epoch [38/50], Train Loss: 0.0034, Val Loss: 0.0026\n",
            "Epoch [39/50], Train Loss: 0.0035, Val Loss: 0.0027\n",
            "Epoch [40/50], Train Loss: 0.0035, Val Loss: 0.0027\n",
            "Epoch [41/50], Train Loss: 0.0034, Val Loss: 0.0027\n",
            "Epoch [42/50], Train Loss: 0.0036, Val Loss: 0.0027\n",
            "Epoch [43/50], Train Loss: 0.0034, Val Loss: 0.0026\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 2 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 3 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0033, Val Loss: 0.0173\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0192, Val Loss: 0.0030\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0041, Val Loss: 0.0049\n",
            "Epoch [4/50], Train Loss: 0.0065, Val Loss: 0.0099\n",
            "Epoch [5/50], Train Loss: 0.0118, Val Loss: 0.0096\n",
            "Epoch [6/50], Train Loss: 0.0110, Val Loss: 0.0061\n",
            "Epoch [7/50], Train Loss: 0.0075, Val Loss: 0.0030\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0040, Val Loss: 0.0026\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [9/50], Train Loss: 0.0035, Val Loss: 0.0047\n",
            "Epoch [10/50], Train Loss: 0.0054, Val Loss: 0.0065\n",
            "Epoch [11/50], Train Loss: 0.0075, Val Loss: 0.0059\n",
            "Epoch [12/50], Train Loss: 0.0072, Val Loss: 0.0040\n",
            "Epoch [13/50], Train Loss: 0.0048, Val Loss: 0.0026\n",
            "Epoch [14/50], Train Loss: 0.0035, Val Loss: 0.0026\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [15/50], Train Loss: 0.0037, Val Loss: 0.0034\n",
            "Epoch [16/50], Train Loss: 0.0043, Val Loss: 0.0042\n",
            "Epoch [17/50], Train Loss: 0.0053, Val Loss: 0.0042\n",
            "Epoch [18/50], Train Loss: 0.0053, Val Loss: 0.0035\n",
            "Epoch [19/50], Train Loss: 0.0046, Val Loss: 0.0027\n",
            "Epoch [20/50], Train Loss: 0.0041, Val Loss: 0.0023\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [21/50], Train Loss: 0.0033, Val Loss: 0.0025\n",
            "Epoch [22/50], Train Loss: 0.0034, Val Loss: 0.0030\n",
            "Epoch [23/50], Train Loss: 0.0040, Val Loss: 0.0033\n",
            "Epoch [24/50], Train Loss: 0.0043, Val Loss: 0.0031\n",
            "Epoch [25/50], Train Loss: 0.0040, Val Loss: 0.0025\n",
            "Epoch [26/50], Train Loss: 0.0033, Val Loss: 0.0021\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [27/50], Train Loss: 0.0031, Val Loss: 0.0020\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [28/50], Train Loss: 0.0031, Val Loss: 0.0023\n",
            "Epoch [29/50], Train Loss: 0.0035, Val Loss: 0.0025\n",
            "Epoch [30/50], Train Loss: 0.0034, Val Loss: 0.0024\n",
            "Epoch [31/50], Train Loss: 0.0036, Val Loss: 0.0021\n",
            "Epoch [32/50], Train Loss: 0.0032, Val Loss: 0.0019\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [33/50], Train Loss: 0.0029, Val Loss: 0.0018\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [34/50], Train Loss: 0.0029, Val Loss: 0.0020\n",
            "Epoch [35/50], Train Loss: 0.0033, Val Loss: 0.0022\n",
            "Epoch [36/50], Train Loss: 0.0032, Val Loss: 0.0021\n",
            "Epoch [37/50], Train Loss: 0.0034, Val Loss: 0.0019\n",
            "Epoch [38/50], Train Loss: 0.0031, Val Loss: 0.0017\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [39/50], Train Loss: 0.0029, Val Loss: 0.0017\n",
            "Epoch [40/50], Train Loss: 0.0028, Val Loss: 0.0018\n",
            "Epoch [41/50], Train Loss: 0.0031, Val Loss: 0.0019\n",
            "Epoch [42/50], Train Loss: 0.0029, Val Loss: 0.0018\n",
            "Epoch [43/50], Train Loss: 0.0030, Val Loss: 0.0017\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [44/50], Train Loss: 0.0027, Val Loss: 0.0016\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [45/50], Train Loss: 0.0029, Val Loss: 0.0016\n",
            "Epoch [46/50], Train Loss: 0.0027, Val Loss: 0.0017\n",
            "Epoch [47/50], Train Loss: 0.0026, Val Loss: 0.0017\n",
            "Epoch [48/50], Train Loss: 0.0027, Val Loss: 0.0017\n",
            "Epoch [49/50], Train Loss: 0.0027, Val Loss: 0.0016\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [50/50], Train Loss: 0.0028, Val Loss: 0.0015\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 3 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 4 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0026, Val Loss: 0.0096\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0105, Val Loss: 0.0016\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0026, Val Loss: 0.0045\n",
            "Epoch [4/50], Train Loss: 0.0061, Val Loss: 0.0062\n",
            "Epoch [5/50], Train Loss: 0.0075, Val Loss: 0.0042\n",
            "Epoch [6/50], Train Loss: 0.0053, Val Loss: 0.0019\n",
            "Epoch [7/50], Train Loss: 0.0029, Val Loss: 0.0018\n",
            "Epoch [8/50], Train Loss: 0.0029, Val Loss: 0.0035\n",
            "Epoch [9/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Epoch [10/50], Train Loss: 0.0052, Val Loss: 0.0031\n",
            "Epoch [11/50], Train Loss: 0.0040, Val Loss: 0.0018\n",
            "Epoch [12/50], Train Loss: 0.0028, Val Loss: 0.0015\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [13/50], Train Loss: 0.0025, Val Loss: 0.0022\n",
            "Epoch [14/50], Train Loss: 0.0034, Val Loss: 0.0026\n",
            "Epoch [15/50], Train Loss: 0.0040, Val Loss: 0.0024\n",
            "Epoch [16/50], Train Loss: 0.0035, Val Loss: 0.0018\n",
            "Epoch [17/50], Train Loss: 0.0027, Val Loss: 0.0014\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [18/50], Train Loss: 0.0026, Val Loss: 0.0015\n",
            "Epoch [19/50], Train Loss: 0.0025, Val Loss: 0.0020\n",
            "Epoch [20/50], Train Loss: 0.0029, Val Loss: 0.0022\n",
            "Epoch [21/50], Train Loss: 0.0031, Val Loss: 0.0019\n",
            "Epoch [22/50], Train Loss: 0.0028, Val Loss: 0.0015\n",
            "Epoch [23/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [24/50], Train Loss: 0.0024, Val Loss: 0.0014\n",
            "Epoch [25/50], Train Loss: 0.0023, Val Loss: 0.0016\n",
            "Epoch [26/50], Train Loss: 0.0026, Val Loss: 0.0016\n",
            "Epoch [27/50], Train Loss: 0.0028, Val Loss: 0.0014\n",
            "Epoch [28/50], Train Loss: 0.0022, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [29/50], Train Loss: 0.0023, Val Loss: 0.0014\n",
            "Epoch [30/50], Train Loss: 0.0024, Val Loss: 0.0015\n",
            "Epoch [31/50], Train Loss: 0.0026, Val Loss: 0.0016\n",
            "Epoch [32/50], Train Loss: 0.0028, Val Loss: 0.0014\n",
            "Epoch [33/50], Train Loss: 0.0025, Val Loss: 0.0013\n",
            "Epoch [34/50], Train Loss: 0.0024, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [35/50], Train Loss: 0.0022, Val Loss: 0.0013\n",
            "Epoch [36/50], Train Loss: 0.0023, Val Loss: 0.0014\n",
            "Epoch [37/50], Train Loss: 0.0024, Val Loss: 0.0013\n",
            "Epoch [38/50], Train Loss: 0.0022, Val Loss: 0.0013\n",
            "Epoch [39/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Epoch [40/50], Train Loss: 0.0024, Val Loss: 0.0014\n",
            "Epoch [41/50], Train Loss: 0.0021, Val Loss: 0.0014\n",
            "Epoch [42/50], Train Loss: 0.0021, Val Loss: 0.0014\n",
            "Epoch [43/50], Train Loss: 0.0022, Val Loss: 0.0013\n",
            "Epoch [44/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [45/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [46/50], Train Loss: 0.0024, Val Loss: 0.0013\n",
            "Epoch [47/50], Train Loss: 0.0024, Val Loss: 0.0013\n",
            "Epoch [48/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [49/50], Train Loss: 0.0022, Val Loss: 0.0013\n",
            "Epoch [50/50], Train Loss: 0.0022, Val Loss: 0.0013\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 4 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 5 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0023, Val Loss: 0.0122\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0132, Val Loss: 0.0020\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0026, Val Loss: 0.0029\n",
            "Epoch [4/50], Train Loss: 0.0040, Val Loss: 0.0062\n",
            "Epoch [5/50], Train Loss: 0.0073, Val Loss: 0.0060\n",
            "Epoch [6/50], Train Loss: 0.0073, Val Loss: 0.0036\n",
            "Epoch [7/50], Train Loss: 0.0048, Val Loss: 0.0016\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0026, Val Loss: 0.0017\n",
            "Epoch [9/50], Train Loss: 0.0027, Val Loss: 0.0033\n",
            "Epoch [10/50], Train Loss: 0.0040, Val Loss: 0.0044\n",
            "Epoch [11/50], Train Loss: 0.0051, Val Loss: 0.0038\n",
            "Epoch [12/50], Train Loss: 0.0044, Val Loss: 0.0024\n",
            "Epoch [13/50], Train Loss: 0.0032, Val Loss: 0.0015\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0024, Val Loss: 0.0015\n",
            "Epoch [15/50], Train Loss: 0.0025, Val Loss: 0.0021\n",
            "Epoch [16/50], Train Loss: 0.0033, Val Loss: 0.0025\n",
            "Epoch [17/50], Train Loss: 0.0038, Val Loss: 0.0023\n",
            "Epoch [18/50], Train Loss: 0.0032, Val Loss: 0.0018\n",
            "Epoch [19/50], Train Loss: 0.0027, Val Loss: 0.0014\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [20/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [21/50], Train Loss: 0.0024, Val Loss: 0.0017\n",
            "Epoch [22/50], Train Loss: 0.0027, Val Loss: 0.0021\n",
            "Epoch [23/50], Train Loss: 0.0028, Val Loss: 0.0021\n",
            "Epoch [24/50], Train Loss: 0.0029, Val Loss: 0.0018\n",
            "Epoch [25/50], Train Loss: 0.0026, Val Loss: 0.0014\n",
            "Epoch [26/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [27/50], Train Loss: 0.0021, Val Loss: 0.0015\n",
            "Epoch [28/50], Train Loss: 0.0024, Val Loss: 0.0016\n",
            "Epoch [29/50], Train Loss: 0.0027, Val Loss: 0.0016\n",
            "Epoch [30/50], Train Loss: 0.0026, Val Loss: 0.0015\n",
            "Epoch [31/50], Train Loss: 0.0025, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [32/50], Train Loss: 0.0022, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [33/50], Train Loss: 0.0022, Val Loss: 0.0014\n",
            "Epoch [34/50], Train Loss: 0.0024, Val Loss: 0.0016\n",
            "Epoch [35/50], Train Loss: 0.0024, Val Loss: 0.0016\n",
            "Epoch [36/50], Train Loss: 0.0025, Val Loss: 0.0015\n",
            "Epoch [37/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Epoch [38/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [39/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [40/50], Train Loss: 0.0023, Val Loss: 0.0014\n",
            "Epoch [41/50], Train Loss: 0.0022, Val Loss: 0.0014\n",
            "Epoch [42/50], Train Loss: 0.0022, Val Loss: 0.0013\n",
            "Epoch [43/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [44/50], Train Loss: 0.0022, Val Loss: 0.0013\n",
            "Epoch [45/50], Train Loss: 0.0021, Val Loss: 0.0014\n",
            "Epoch [46/50], Train Loss: 0.0022, Val Loss: 0.0014\n",
            "Epoch [47/50], Train Loss: 0.0023, Val Loss: 0.0014\n",
            "Epoch [48/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [49/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [50/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 5 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 6 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0022, Val Loss: 0.0112\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0116, Val Loss: 0.0020\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0029, Val Loss: 0.0029\n",
            "Epoch [4/50], Train Loss: 0.0039, Val Loss: 0.0061\n",
            "Epoch [5/50], Train Loss: 0.0075, Val Loss: 0.0057\n",
            "Epoch [6/50], Train Loss: 0.0068, Val Loss: 0.0034\n",
            "Epoch [7/50], Train Loss: 0.0046, Val Loss: 0.0016\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0024, Val Loss: 0.0017\n",
            "Epoch [9/50], Train Loss: 0.0023, Val Loss: 0.0033\n",
            "Epoch [10/50], Train Loss: 0.0040, Val Loss: 0.0043\n",
            "Epoch [11/50], Train Loss: 0.0047, Val Loss: 0.0037\n",
            "Epoch [12/50], Train Loss: 0.0044, Val Loss: 0.0024\n",
            "Epoch [13/50], Train Loss: 0.0031, Val Loss: 0.0015\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0021, Val Loss: 0.0015\n",
            "Epoch [15/50], Train Loss: 0.0024, Val Loss: 0.0021\n",
            "Epoch [16/50], Train Loss: 0.0032, Val Loss: 0.0025\n",
            "Epoch [17/50], Train Loss: 0.0036, Val Loss: 0.0023\n",
            "Epoch [18/50], Train Loss: 0.0032, Val Loss: 0.0018\n",
            "Epoch [19/50], Train Loss: 0.0028, Val Loss: 0.0014\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [20/50], Train Loss: 0.0022, Val Loss: 0.0014\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [21/50], Train Loss: 0.0021, Val Loss: 0.0017\n",
            "Epoch [22/50], Train Loss: 0.0024, Val Loss: 0.0021\n",
            "Epoch [23/50], Train Loss: 0.0029, Val Loss: 0.0021\n",
            "Epoch [24/50], Train Loss: 0.0028, Val Loss: 0.0018\n",
            "Epoch [25/50], Train Loss: 0.0026, Val Loss: 0.0014\n",
            "Epoch [26/50], Train Loss: 0.0024, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [27/50], Train Loss: 0.0021, Val Loss: 0.0015\n",
            "Epoch [28/50], Train Loss: 0.0023, Val Loss: 0.0017\n",
            "Epoch [29/50], Train Loss: 0.0026, Val Loss: 0.0017\n",
            "Epoch [30/50], Train Loss: 0.0024, Val Loss: 0.0015\n",
            "Epoch [31/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [32/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [33/50], Train Loss: 0.0021, Val Loss: 0.0014\n",
            "Epoch [34/50], Train Loss: 0.0023, Val Loss: 0.0016\n",
            "Epoch [35/50], Train Loss: 0.0023, Val Loss: 0.0016\n",
            "Epoch [36/50], Train Loss: 0.0024, Val Loss: 0.0014\n",
            "Epoch [37/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Epoch [38/50], Train Loss: 0.0020, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [39/50], Train Loss: 0.0020, Val Loss: 0.0013\n",
            "Epoch [40/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [41/50], Train Loss: 0.0022, Val Loss: 0.0013\n",
            "Epoch [42/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [43/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [44/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [45/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [46/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [47/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Epoch [48/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [49/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [50/50], Train Loss: 0.0022, Val Loss: 0.0013\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 6 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 7 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0020, Val Loss: 0.0096\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0104, Val Loss: 0.0017\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0025, Val Loss: 0.0027\n",
            "Epoch [4/50], Train Loss: 0.0038, Val Loss: 0.0054\n",
            "Epoch [5/50], Train Loss: 0.0065, Val Loss: 0.0049\n",
            "Epoch [6/50], Train Loss: 0.0061, Val Loss: 0.0028\n",
            "Epoch [7/50], Train Loss: 0.0037, Val Loss: 0.0014\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0023, Val Loss: 0.0018\n",
            "Epoch [9/50], Train Loss: 0.0024, Val Loss: 0.0032\n",
            "Epoch [10/50], Train Loss: 0.0038, Val Loss: 0.0037\n",
            "Epoch [11/50], Train Loss: 0.0042, Val Loss: 0.0029\n",
            "Epoch [12/50], Train Loss: 0.0035, Val Loss: 0.0017\n",
            "Epoch [13/50], Train Loss: 0.0025, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0022, Val Loss: 0.0016\n",
            "Epoch [15/50], Train Loss: 0.0025, Val Loss: 0.0022\n",
            "Epoch [16/50], Train Loss: 0.0031, Val Loss: 0.0023\n",
            "Epoch [17/50], Train Loss: 0.0033, Val Loss: 0.0020\n",
            "Epoch [18/50], Train Loss: 0.0028, Val Loss: 0.0015\n",
            "Epoch [19/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Epoch [20/50], Train Loss: 0.0021, Val Loss: 0.0015\n",
            "Epoch [21/50], Train Loss: 0.0023, Val Loss: 0.0019\n",
            "Epoch [22/50], Train Loss: 0.0026, Val Loss: 0.0020\n",
            "Epoch [23/50], Train Loss: 0.0027, Val Loss: 0.0018\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 7 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 8 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0020, Val Loss: 0.0097\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0109, Val Loss: 0.0018\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0026, Val Loss: 0.0027\n",
            "Epoch [4/50], Train Loss: 0.0036, Val Loss: 0.0054\n",
            "Epoch [5/50], Train Loss: 0.0067, Val Loss: 0.0051\n",
            "Epoch [6/50], Train Loss: 0.0064, Val Loss: 0.0030\n",
            "Epoch [7/50], Train Loss: 0.0039, Val Loss: 0.0015\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0024, Val Loss: 0.0017\n",
            "Epoch [9/50], Train Loss: 0.0023, Val Loss: 0.0031\n",
            "Epoch [10/50], Train Loss: 0.0038, Val Loss: 0.0038\n",
            "Epoch [11/50], Train Loss: 0.0044, Val Loss: 0.0032\n",
            "Epoch [12/50], Train Loss: 0.0039, Val Loss: 0.0021\n",
            "Epoch [13/50], Train Loss: 0.0027, Val Loss: 0.0014\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0021, Val Loss: 0.0015\n",
            "Epoch [15/50], Train Loss: 0.0023, Val Loss: 0.0020\n",
            "Epoch [16/50], Train Loss: 0.0029, Val Loss: 0.0024\n",
            "Epoch [17/50], Train Loss: 0.0033, Val Loss: 0.0022\n",
            "Epoch [18/50], Train Loss: 0.0033, Val Loss: 0.0017\n",
            "Epoch [19/50], Train Loss: 0.0026, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [20/50], Train Loss: 0.0021, Val Loss: 0.0014\n",
            "Epoch [21/50], Train Loss: 0.0022, Val Loss: 0.0017\n",
            "Epoch [22/50], Train Loss: 0.0025, Val Loss: 0.0021\n",
            "Epoch [23/50], Train Loss: 0.0028, Val Loss: 0.0020\n",
            "Epoch [24/50], Train Loss: 0.0026, Val Loss: 0.0017\n",
            "Epoch [25/50], Train Loss: 0.0024, Val Loss: 0.0014\n",
            "Epoch [26/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [27/50], Train Loss: 0.0020, Val Loss: 0.0015\n",
            "Epoch [28/50], Train Loss: 0.0023, Val Loss: 0.0016\n",
            "Epoch [29/50], Train Loss: 0.0024, Val Loss: 0.0016\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 8 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 9 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0021, Val Loss: 0.0084\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0094, Val Loss: 0.0017\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0025, Val Loss: 0.0025\n",
            "Epoch [4/50], Train Loss: 0.0033, Val Loss: 0.0047\n",
            "Epoch [5/50], Train Loss: 0.0060, Val Loss: 0.0042\n",
            "Epoch [6/50], Train Loss: 0.0053, Val Loss: 0.0024\n",
            "Epoch [7/50], Train Loss: 0.0034, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0021, Val Loss: 0.0019\n",
            "Epoch [9/50], Train Loss: 0.0026, Val Loss: 0.0030\n",
            "Epoch [10/50], Train Loss: 0.0037, Val Loss: 0.0033\n",
            "Epoch [11/50], Train Loss: 0.0040, Val Loss: 0.0025\n",
            "Epoch [12/50], Train Loss: 0.0032, Val Loss: 0.0015\n",
            "Epoch [13/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0020, Val Loss: 0.0017\n",
            "Epoch [15/50], Train Loss: 0.0024, Val Loss: 0.0021\n",
            "Epoch [16/50], Train Loss: 0.0030, Val Loss: 0.0021\n",
            "Epoch [17/50], Train Loss: 0.0030, Val Loss: 0.0017\n",
            "Epoch [18/50], Train Loss: 0.0024, Val Loss: 0.0014\n",
            "Epoch [19/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Epoch [20/50], Train Loss: 0.0021, Val Loss: 0.0016\n",
            "Epoch [21/50], Train Loss: 0.0023, Val Loss: 0.0019\n",
            "Epoch [22/50], Train Loss: 0.0026, Val Loss: 0.0018\n",
            "Epoch [23/50], Train Loss: 0.0025, Val Loss: 0.0016\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 9 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 10 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0021, Val Loss: 0.0085\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0091, Val Loss: 0.0018\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0024, Val Loss: 0.0025\n",
            "Epoch [4/50], Train Loss: 0.0034, Val Loss: 0.0047\n",
            "Epoch [5/50], Train Loss: 0.0056, Val Loss: 0.0042\n",
            "Epoch [6/50], Train Loss: 0.0052, Val Loss: 0.0024\n",
            "Epoch [7/50], Train Loss: 0.0034, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0022, Val Loss: 0.0019\n",
            "Epoch [9/50], Train Loss: 0.0027, Val Loss: 0.0031\n",
            "Epoch [10/50], Train Loss: 0.0037, Val Loss: 0.0033\n",
            "Epoch [11/50], Train Loss: 0.0039, Val Loss: 0.0025\n",
            "Epoch [12/50], Train Loss: 0.0030, Val Loss: 0.0016\n",
            "Epoch [13/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0021, Val Loss: 0.0017\n",
            "Epoch [15/50], Train Loss: 0.0026, Val Loss: 0.0020\n",
            "Epoch [16/50], Train Loss: 0.0030, Val Loss: 0.0020\n",
            "Epoch [17/50], Train Loss: 0.0029, Val Loss: 0.0017\n",
            "Epoch [18/50], Train Loss: 0.0026, Val Loss: 0.0013\n",
            "Epoch [19/50], Train Loss: 0.0020, Val Loss: 0.0013\n",
            "Epoch [20/50], Train Loss: 0.0020, Val Loss: 0.0016\n",
            "Epoch [21/50], Train Loss: 0.0023, Val Loss: 0.0018\n",
            "Epoch [22/50], Train Loss: 0.0025, Val Loss: 0.0018\n",
            "Epoch [23/50], Train Loss: 0.0024, Val Loss: 0.0015\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 10 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 11 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0020, Val Loss: 0.0083\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0090, Val Loss: 0.0017\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0025, Val Loss: 0.0025\n",
            "Epoch [4/50], Train Loss: 0.0036, Val Loss: 0.0048\n",
            "Epoch [5/50], Train Loss: 0.0058, Val Loss: 0.0044\n",
            "Epoch [6/50], Train Loss: 0.0055, Val Loss: 0.0025\n",
            "Epoch [7/50], Train Loss: 0.0035, Val Loss: 0.0014\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0023, Val Loss: 0.0018\n",
            "Epoch [9/50], Train Loss: 0.0025, Val Loss: 0.0030\n",
            "Epoch [10/50], Train Loss: 0.0037, Val Loss: 0.0034\n",
            "Epoch [11/50], Train Loss: 0.0039, Val Loss: 0.0027\n",
            "Epoch [12/50], Train Loss: 0.0034, Val Loss: 0.0017\n",
            "Epoch [13/50], Train Loss: 0.0024, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0021, Val Loss: 0.0016\n",
            "Epoch [15/50], Train Loss: 0.0024, Val Loss: 0.0021\n",
            "Epoch [16/50], Train Loss: 0.0032, Val Loss: 0.0021\n",
            "Epoch [17/50], Train Loss: 0.0028, Val Loss: 0.0018\n",
            "Epoch [18/50], Train Loss: 0.0027, Val Loss: 0.0014\n",
            "Epoch [19/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [20/50], Train Loss: 0.0020, Val Loss: 0.0015\n",
            "Epoch [21/50], Train Loss: 0.0022, Val Loss: 0.0018\n",
            "Epoch [22/50], Train Loss: 0.0026, Val Loss: 0.0019\n",
            "Epoch [23/50], Train Loss: 0.0025, Val Loss: 0.0016\n",
            "Epoch [24/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [25/50], Train Loss: 0.0020, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [26/50], Train Loss: 0.0020, Val Loss: 0.0014\n",
            "Epoch [27/50], Train Loss: 0.0022, Val Loss: 0.0015\n",
            "Epoch [28/50], Train Loss: 0.0024, Val Loss: 0.0015\n",
            "Epoch [29/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
            "Epoch [30/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [31/50], Train Loss: 0.0020, Val Loss: 0.0013\n",
            "Epoch [32/50], Train Loss: 0.0020, Val Loss: 0.0014\n",
            "Epoch [33/50], Train Loss: 0.0021, Val Loss: 0.0014\n",
            "Epoch [34/50], Train Loss: 0.0021, Val Loss: 0.0014\n",
            "Epoch [35/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [36/50], Train Loss: 0.0019, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [37/50], Train Loss: 0.0020, Val Loss: 0.0013\n",
            "Epoch [38/50], Train Loss: 0.0020, Val Loss: 0.0013\n",
            "Epoch [39/50], Train Loss: 0.0020, Val Loss: 0.0013\n",
            "Epoch [40/50], Train Loss: 0.0020, Val Loss: 0.0012\n",
            "Epoch [41/50], Train Loss: 0.0020, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [42/50], Train Loss: 0.0020, Val Loss: 0.0012\n",
            "Epoch [43/50], Train Loss: 0.0019, Val Loss: 0.0013\n",
            "Epoch [44/50], Train Loss: 0.0020, Val Loss: 0.0013\n",
            "Epoch [45/50], Train Loss: 0.0019, Val Loss: 0.0012\n",
            "Epoch [46/50], Train Loss: 0.0019, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [47/50], Train Loss: 0.0020, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [48/50], Train Loss: 0.0020, Val Loss: 0.0012\n",
            "Epoch [49/50], Train Loss: 0.0020, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [50/50], Train Loss: 0.0019, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 11 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 12 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0020, Val Loss: 0.0070\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0077, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0019, Val Loss: 0.0029\n",
            "Epoch [4/50], Train Loss: 0.0040, Val Loss: 0.0045\n",
            "Epoch [5/50], Train Loss: 0.0055, Val Loss: 0.0034\n",
            "Epoch [6/50], Train Loss: 0.0043, Val Loss: 0.0017\n",
            "Epoch [7/50], Train Loss: 0.0025, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0020, Val Loss: 0.0022\n",
            "Epoch [9/50], Train Loss: 0.0029, Val Loss: 0.0030\n",
            "Epoch [10/50], Train Loss: 0.0037, Val Loss: 0.0027\n",
            "Epoch [11/50], Train Loss: 0.0033, Val Loss: 0.0017\n",
            "Epoch [12/50], Train Loss: 0.0024, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [13/50], Train Loss: 0.0019, Val Loss: 0.0014\n",
            "Epoch [14/50], Train Loss: 0.0023, Val Loss: 0.0019\n",
            "Epoch [15/50], Train Loss: 0.0027, Val Loss: 0.0021\n",
            "Epoch [16/50], Train Loss: 0.0029, Val Loss: 0.0018\n",
            "Epoch [17/50], Train Loss: 0.0027, Val Loss: 0.0014\n",
            "Epoch [18/50], Train Loss: 0.0021, Val Loss: 0.0012\n",
            "Epoch [19/50], Train Loss: 0.0020, Val Loss: 0.0014\n",
            "Epoch [20/50], Train Loss: 0.0021, Val Loss: 0.0017\n",
            "Epoch [21/50], Train Loss: 0.0023, Val Loss: 0.0018\n",
            "Epoch [22/50], Train Loss: 0.0025, Val Loss: 0.0015\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 12 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 13 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0019, Val Loss: 0.0020\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0027, Val Loss: 0.0032\n",
            "Epoch [3/50], Train Loss: 0.0039, Val Loss: 0.0015\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0023, Val Loss: 0.0015\n",
            "Epoch [5/50], Train Loss: 0.0022, Val Loss: 0.0021\n",
            "Epoch [6/50], Train Loss: 0.0029, Val Loss: 0.0015\n",
            "Epoch [7/50], Train Loss: 0.0022, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0020, Val Loss: 0.0016\n",
            "Epoch [9/50], Train Loss: 0.0022, Val Loss: 0.0019\n",
            "Epoch [10/50], Train Loss: 0.0026, Val Loss: 0.0015\n",
            "Epoch [11/50], Train Loss: 0.0022, Val Loss: 0.0013\n",
            "Epoch [12/50], Train Loss: 0.0020, Val Loss: 0.0014\n",
            "Epoch [13/50], Train Loss: 0.0021, Val Loss: 0.0015\n",
            "Epoch [14/50], Train Loss: 0.0024, Val Loss: 0.0013\n",
            "Epoch [15/50], Train Loss: 0.0020, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [16/50], Train Loss: 0.0019, Val Loss: 0.0013\n",
            "Epoch [17/50], Train Loss: 0.0021, Val Loss: 0.0014\n",
            "Epoch [18/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Epoch [19/50], Train Loss: 0.0019, Val Loss: 0.0012\n",
            "Epoch [20/50], Train Loss: 0.0020, Val Loss: 0.0012\n",
            "Epoch [21/50], Train Loss: 0.0019, Val Loss: 0.0013\n",
            "Epoch [22/50], Train Loss: 0.0020, Val Loss: 0.0013\n",
            "Epoch [23/50], Train Loss: 0.0020, Val Loss: 0.0012\n",
            "Epoch [24/50], Train Loss: 0.0018, Val Loss: 0.0012\n",
            "Epoch [25/50], Train Loss: 0.0020, Val Loss: 0.0013\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 13 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 14 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0019, Val Loss: 0.0080\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0085, Val Loss: 0.0015\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0022, Val Loss: 0.0026\n",
            "Epoch [4/50], Train Loss: 0.0033, Val Loss: 0.0048\n",
            "Epoch [5/50], Train Loss: 0.0058, Val Loss: 0.0043\n",
            "Epoch [6/50], Train Loss: 0.0054, Val Loss: 0.0024\n",
            "Epoch [7/50], Train Loss: 0.0034, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0020, Val Loss: 0.0017\n",
            "Epoch [9/50], Train Loss: 0.0024, Val Loss: 0.0029\n",
            "Epoch [10/50], Train Loss: 0.0034, Val Loss: 0.0032\n",
            "Epoch [11/50], Train Loss: 0.0038, Val Loss: 0.0025\n",
            "Epoch [12/50], Train Loss: 0.0031, Val Loss: 0.0016\n",
            "Epoch [13/50], Train Loss: 0.0022, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0019, Val Loss: 0.0015\n",
            "Epoch [15/50], Train Loss: 0.0022, Val Loss: 0.0019\n",
            "Epoch [16/50], Train Loss: 0.0028, Val Loss: 0.0021\n",
            "Epoch [17/50], Train Loss: 0.0027, Val Loss: 0.0018\n",
            "Epoch [18/50], Train Loss: 0.0027, Val Loss: 0.0014\n",
            "Epoch [19/50], Train Loss: 0.0021, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [20/50], Train Loss: 0.0019, Val Loss: 0.0014\n",
            "Epoch [21/50], Train Loss: 0.0019, Val Loss: 0.0017\n",
            "Epoch [22/50], Train Loss: 0.0022, Val Loss: 0.0018\n",
            "Epoch [23/50], Train Loss: 0.0024, Val Loss: 0.0016\n",
            "Epoch [24/50], Train Loss: 0.0022, Val Loss: 0.0014\n",
            "Epoch [25/50], Train Loss: 0.0020, Val Loss: 0.0012\n",
            "Epoch [26/50], Train Loss: 0.0018, Val Loss: 0.0013\n",
            "Epoch [27/50], Train Loss: 0.0019, Val Loss: 0.0014\n",
            "Epoch [28/50], Train Loss: 0.0022, Val Loss: 0.0014\n",
            "Epoch [29/50], Train Loss: 0.0021, Val Loss: 0.0013\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 14 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 15 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0019, Val Loss: 0.0082\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0088, Val Loss: 0.0016\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0022, Val Loss: 0.0023\n",
            "Epoch [4/50], Train Loss: 0.0030, Val Loss: 0.0046\n",
            "Epoch [5/50], Train Loss: 0.0057, Val Loss: 0.0044\n",
            "Epoch [6/50], Train Loss: 0.0054, Val Loss: 0.0027\n",
            "Epoch [7/50], Train Loss: 0.0036, Val Loss: 0.0014\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0020, Val Loss: 0.0015\n",
            "Epoch [9/50], Train Loss: 0.0021, Val Loss: 0.0026\n",
            "Epoch [10/50], Train Loss: 0.0031, Val Loss: 0.0032\n",
            "Epoch [11/50], Train Loss: 0.0038, Val Loss: 0.0027\n",
            "Epoch [12/50], Train Loss: 0.0033, Val Loss: 0.0017\n",
            "Epoch [13/50], Train Loss: 0.0025, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0019, Val Loss: 0.0014\n",
            "Epoch [15/50], Train Loss: 0.0021, Val Loss: 0.0019\n",
            "Epoch [16/50], Train Loss: 0.0027, Val Loss: 0.0021\n",
            "Epoch [17/50], Train Loss: 0.0028, Val Loss: 0.0019\n",
            "Epoch [18/50], Train Loss: 0.0028, Val Loss: 0.0015\n",
            "Epoch [19/50], Train Loss: 0.0021, Val Loss: 0.0012\n",
            "Epoch [20/50], Train Loss: 0.0018, Val Loss: 0.0013\n",
            "Epoch [21/50], Train Loss: 0.0019, Val Loss: 0.0016\n",
            "Epoch [22/50], Train Loss: 0.0022, Val Loss: 0.0017\n",
            "Epoch [23/50], Train Loss: 0.0022, Val Loss: 0.0017\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 15 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 1 ---\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.5935, Val Loss: 0.5170\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.5617, Val Loss: 0.4883\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.5321, Val Loss: 0.4609\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [4/50], Train Loss: 0.5031, Val Loss: 0.4344\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [5/50], Train Loss: 0.4751, Val Loss: 0.4087\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [6/50], Train Loss: 0.4482, Val Loss: 0.3836\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [7/50], Train Loss: 0.4216, Val Loss: 0.3589\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.3953, Val Loss: 0.3345\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [9/50], Train Loss: 0.3690, Val Loss: 0.3101\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [10/50], Train Loss: 0.3436, Val Loss: 0.2857\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [11/50], Train Loss: 0.3174, Val Loss: 0.2610\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [12/50], Train Loss: 0.2909, Val Loss: 0.2359\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [13/50], Train Loss: 0.2645, Val Loss: 0.2105\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [14/50], Train Loss: 0.2369, Val Loss: 0.1847\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [15/50], Train Loss: 0.2092, Val Loss: 0.1587\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [16/50], Train Loss: 0.1803, Val Loss: 0.1328\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [17/50], Train Loss: 0.1520, Val Loss: 0.1075\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [18/50], Train Loss: 0.1237, Val Loss: 0.0837\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [19/50], Train Loss: 0.0971, Val Loss: 0.0628\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [20/50], Train Loss: 0.0727, Val Loss: 0.0469\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [21/50], Train Loss: 0.0530, Val Loss: 0.0389\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [22/50], Train Loss: 0.0398, Val Loss: 0.0414\n",
            "Epoch [23/50], Train Loss: 0.0378, Val Loss: 0.0549\n",
            "Epoch [24/50], Train Loss: 0.0472, Val Loss: 0.0734\n",
            "Epoch [25/50], Train Loss: 0.0627, Val Loss: 0.0866\n",
            "Epoch [26/50], Train Loss: 0.0741, Val Loss: 0.0895\n",
            "Epoch [27/50], Train Loss: 0.0761, Val Loss: 0.0835\n",
            "Epoch [28/50], Train Loss: 0.0709, Val Loss: 0.0728\n",
            "Epoch [29/50], Train Loss: 0.0615, Val Loss: 0.0613\n",
            "Epoch [30/50], Train Loss: 0.0518, Val Loss: 0.0514\n",
            "Epoch [31/50], Train Loss: 0.0448, Val Loss: 0.0444\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 1 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 2 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0404, Val Loss: 0.0420\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0386, Val Loss: 0.0420\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0383, Val Loss: 0.0380\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0351, Val Loss: 0.0351\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0342, Val Loss: 0.0337\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0342, Val Loss: 0.0324\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0329, Val Loss: 0.0312\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0306, Val Loss: 0.0307\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [9/50], Train Loss: 0.0299, Val Loss: 0.0305\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [10/50], Train Loss: 0.0282, Val Loss: 0.0294\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [11/50], Train Loss: 0.0272, Val Loss: 0.0271\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [12/50], Train Loss: 0.0255, Val Loss: 0.0244\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [13/50], Train Loss: 0.0239, Val Loss: 0.0222\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0223, Val Loss: 0.0204\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [15/50], Train Loss: 0.0207, Val Loss: 0.0187\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [16/50], Train Loss: 0.0189, Val Loss: 0.0173\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [17/50], Train Loss: 0.0171, Val Loss: 0.0162\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [18/50], Train Loss: 0.0156, Val Loss: 0.0149\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [19/50], Train Loss: 0.0149, Val Loss: 0.0128\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [20/50], Train Loss: 0.0130, Val Loss: 0.0106\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [21/50], Train Loss: 0.0106, Val Loss: 0.0090\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [22/50], Train Loss: 0.0103, Val Loss: 0.0077\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [23/50], Train Loss: 0.0091, Val Loss: 0.0068\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [24/50], Train Loss: 0.0078, Val Loss: 0.0064\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [25/50], Train Loss: 0.0074, Val Loss: 0.0054\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [26/50], Train Loss: 0.0066, Val Loss: 0.0048\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [27/50], Train Loss: 0.0064, Val Loss: 0.0047\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [28/50], Train Loss: 0.0061, Val Loss: 0.0048\n",
            "Epoch [29/50], Train Loss: 0.0069, Val Loss: 0.0053\n",
            "Epoch [30/50], Train Loss: 0.0071, Val Loss: 0.0054\n",
            "Epoch [31/50], Train Loss: 0.0066, Val Loss: 0.0056\n",
            "Epoch [32/50], Train Loss: 0.0075, Val Loss: 0.0058\n",
            "Epoch [33/50], Train Loss: 0.0076, Val Loss: 0.0060\n",
            "Epoch [34/50], Train Loss: 0.0080, Val Loss: 0.0059\n",
            "Epoch [35/50], Train Loss: 0.0080, Val Loss: 0.0056\n",
            "Epoch [36/50], Train Loss: 0.0075, Val Loss: 0.0054\n",
            "Epoch [37/50], Train Loss: 0.0071, Val Loss: 0.0052\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 2 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 3 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0066, Val Loss: 0.0317\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0312, Val Loss: 0.0065\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0077, Val Loss: 0.0079\n",
            "Epoch [4/50], Train Loss: 0.0105, Val Loss: 0.0152\n",
            "Epoch [5/50], Train Loss: 0.0191, Val Loss: 0.0156\n",
            "Epoch [6/50], Train Loss: 0.0191, Val Loss: 0.0109\n",
            "Epoch [7/50], Train Loss: 0.0134, Val Loss: 0.0060\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0084, Val Loss: 0.0050\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [9/50], Train Loss: 0.0063, Val Loss: 0.0084\n",
            "Epoch [10/50], Train Loss: 0.0095, Val Loss: 0.0118\n",
            "Epoch [11/50], Train Loss: 0.0118, Val Loss: 0.0115\n",
            "Epoch [12/50], Train Loss: 0.0120, Val Loss: 0.0084\n",
            "Epoch [13/50], Train Loss: 0.0088, Val Loss: 0.0056\n",
            "Epoch [14/50], Train Loss: 0.0066, Val Loss: 0.0049\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [15/50], Train Loss: 0.0062, Val Loss: 0.0056\n",
            "Epoch [16/50], Train Loss: 0.0076, Val Loss: 0.0066\n",
            "Epoch [17/50], Train Loss: 0.0090, Val Loss: 0.0069\n",
            "Epoch [18/50], Train Loss: 0.0092, Val Loss: 0.0062\n",
            "Epoch [19/50], Train Loss: 0.0088, Val Loss: 0.0053\n",
            "Epoch [20/50], Train Loss: 0.0075, Val Loss: 0.0049\n",
            "Epoch [21/50], Train Loss: 0.0064, Val Loss: 0.0054\n",
            "Epoch [22/50], Train Loss: 0.0067, Val Loss: 0.0065\n",
            "Epoch [23/50], Train Loss: 0.0072, Val Loss: 0.0071\n",
            "Epoch [24/50], Train Loss: 0.0081, Val Loss: 0.0069\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 3 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 4 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0066, Val Loss: 0.0282\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0275, Val Loss: 0.0066\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0071, Val Loss: 0.0074\n",
            "Epoch [4/50], Train Loss: 0.0103, Val Loss: 0.0135\n",
            "Epoch [5/50], Train Loss: 0.0172, Val Loss: 0.0137\n",
            "Epoch [6/50], Train Loss: 0.0177, Val Loss: 0.0095\n",
            "Epoch [7/50], Train Loss: 0.0122, Val Loss: 0.0056\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0074, Val Loss: 0.0053\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [9/50], Train Loss: 0.0061, Val Loss: 0.0087\n",
            "Epoch [10/50], Train Loss: 0.0092, Val Loss: 0.0116\n",
            "Epoch [11/50], Train Loss: 0.0119, Val Loss: 0.0108\n",
            "Epoch [12/50], Train Loss: 0.0113, Val Loss: 0.0077\n",
            "Epoch [13/50], Train Loss: 0.0082, Val Loss: 0.0054\n",
            "Epoch [14/50], Train Loss: 0.0064, Val Loss: 0.0050\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [15/50], Train Loss: 0.0068, Val Loss: 0.0059\n",
            "Epoch [16/50], Train Loss: 0.0082, Val Loss: 0.0067\n",
            "Epoch [17/50], Train Loss: 0.0085, Val Loss: 0.0066\n",
            "Epoch [18/50], Train Loss: 0.0089, Val Loss: 0.0058\n",
            "Epoch [19/50], Train Loss: 0.0078, Val Loss: 0.0050\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [20/50], Train Loss: 0.0068, Val Loss: 0.0049\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [21/50], Train Loss: 0.0061, Val Loss: 0.0058\n",
            "Epoch [22/50], Train Loss: 0.0065, Val Loss: 0.0069\n",
            "Epoch [23/50], Train Loss: 0.0077, Val Loss: 0.0071\n",
            "Epoch [24/50], Train Loss: 0.0076, Val Loss: 0.0063\n",
            "Epoch [25/50], Train Loss: 0.0071, Val Loss: 0.0052\n",
            "Epoch [26/50], Train Loss: 0.0062, Val Loss: 0.0047\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [27/50], Train Loss: 0.0062, Val Loss: 0.0047\n",
            "Epoch [28/50], Train Loss: 0.0065, Val Loss: 0.0049\n",
            "Epoch [29/50], Train Loss: 0.0067, Val Loss: 0.0050\n",
            "Epoch [30/50], Train Loss: 0.0070, Val Loss: 0.0047\n",
            "Epoch [31/50], Train Loss: 0.0065, Val Loss: 0.0045\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [32/50], Train Loss: 0.0060, Val Loss: 0.0046\n",
            "Epoch [33/50], Train Loss: 0.0057, Val Loss: 0.0049\n",
            "Epoch [34/50], Train Loss: 0.0057, Val Loss: 0.0053\n",
            "Epoch [35/50], Train Loss: 0.0066, Val Loss: 0.0052\n",
            "Epoch [36/50], Train Loss: 0.0065, Val Loss: 0.0049\n",
            "Epoch [37/50], Train Loss: 0.0060, Val Loss: 0.0045\n",
            "Epoch [38/50], Train Loss: 0.0056, Val Loss: 0.0044\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [39/50], Train Loss: 0.0061, Val Loss: 0.0044\n",
            "Epoch [40/50], Train Loss: 0.0059, Val Loss: 0.0044\n",
            "Epoch [41/50], Train Loss: 0.0065, Val Loss: 0.0044\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [42/50], Train Loss: 0.0058, Val Loss: 0.0043\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [43/50], Train Loss: 0.0058, Val Loss: 0.0044\n",
            "Epoch [44/50], Train Loss: 0.0058, Val Loss: 0.0045\n",
            "Epoch [45/50], Train Loss: 0.0060, Val Loss: 0.0046\n",
            "Epoch [46/50], Train Loss: 0.0059, Val Loss: 0.0046\n",
            "Epoch [47/50], Train Loss: 0.0055, Val Loss: 0.0045\n",
            "Epoch [48/50], Train Loss: 0.0056, Val Loss: 0.0044\n",
            "Epoch [49/50], Train Loss: 0.0058, Val Loss: 0.0043\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [50/50], Train Loss: 0.0057, Val Loss: 0.0043\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 4 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 5 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0061, Val Loss: 0.0248\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0250, Val Loss: 0.0055\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0061, Val Loss: 0.0071\n",
            "Epoch [4/50], Train Loss: 0.0093, Val Loss: 0.0127\n",
            "Epoch [5/50], Train Loss: 0.0163, Val Loss: 0.0123\n",
            "Epoch [6/50], Train Loss: 0.0157, Val Loss: 0.0082\n",
            "Epoch [7/50], Train Loss: 0.0108, Val Loss: 0.0047\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0065, Val Loss: 0.0050\n",
            "Epoch [9/50], Train Loss: 0.0057, Val Loss: 0.0083\n",
            "Epoch [10/50], Train Loss: 0.0083, Val Loss: 0.0107\n",
            "Epoch [11/50], Train Loss: 0.0108, Val Loss: 0.0098\n",
            "Epoch [12/50], Train Loss: 0.0100, Val Loss: 0.0070\n",
            "Epoch [13/50], Train Loss: 0.0075, Val Loss: 0.0049\n",
            "Epoch [14/50], Train Loss: 0.0060, Val Loss: 0.0045\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [15/50], Train Loss: 0.0059, Val Loss: 0.0052\n",
            "Epoch [16/50], Train Loss: 0.0075, Val Loss: 0.0059\n",
            "Epoch [17/50], Train Loss: 0.0082, Val Loss: 0.0058\n",
            "Epoch [18/50], Train Loss: 0.0082, Val Loss: 0.0051\n",
            "Epoch [19/50], Train Loss: 0.0070, Val Loss: 0.0045\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [20/50], Train Loss: 0.0063, Val Loss: 0.0045\n",
            "Epoch [21/50], Train Loss: 0.0059, Val Loss: 0.0053\n",
            "Epoch [22/50], Train Loss: 0.0062, Val Loss: 0.0062\n",
            "Epoch [23/50], Train Loss: 0.0069, Val Loss: 0.0064\n",
            "Epoch [24/50], Train Loss: 0.0073, Val Loss: 0.0057\n",
            "Epoch [25/50], Train Loss: 0.0065, Val Loss: 0.0048\n",
            "Epoch [26/50], Train Loss: 0.0059, Val Loss: 0.0043\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [27/50], Train Loss: 0.0057, Val Loss: 0.0043\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [28/50], Train Loss: 0.0059, Val Loss: 0.0045\n",
            "Epoch [29/50], Train Loss: 0.0062, Val Loss: 0.0045\n",
            "Epoch [30/50], Train Loss: 0.0065, Val Loss: 0.0043\n",
            "Epoch [31/50], Train Loss: 0.0061, Val Loss: 0.0042\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [32/50], Train Loss: 0.0056, Val Loss: 0.0042\n",
            "Epoch [33/50], Train Loss: 0.0055, Val Loss: 0.0045\n",
            "Epoch [34/50], Train Loss: 0.0055, Val Loss: 0.0048\n",
            "Epoch [35/50], Train Loss: 0.0057, Val Loss: 0.0048\n",
            "Epoch [36/50], Train Loss: 0.0061, Val Loss: 0.0046\n",
            "Epoch [37/50], Train Loss: 0.0058, Val Loss: 0.0043\n",
            "Epoch [38/50], Train Loss: 0.0056, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [39/50], Train Loss: 0.0058, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [40/50], Train Loss: 0.0057, Val Loss: 0.0041\n",
            "Epoch [41/50], Train Loss: 0.0060, Val Loss: 0.0041\n",
            "Epoch [42/50], Train Loss: 0.0054, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [43/50], Train Loss: 0.0055, Val Loss: 0.0041\n",
            "Epoch [44/50], Train Loss: 0.0052, Val Loss: 0.0042\n",
            "Epoch [45/50], Train Loss: 0.0051, Val Loss: 0.0043\n",
            "Epoch [46/50], Train Loss: 0.0055, Val Loss: 0.0044\n",
            "Epoch [47/50], Train Loss: 0.0053, Val Loss: 0.0043\n",
            "Epoch [48/50], Train Loss: 0.0055, Val Loss: 0.0042\n",
            "Epoch [49/50], Train Loss: 0.0052, Val Loss: 0.0041\n",
            "Epoch [50/50], Train Loss: 0.0056, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 5 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 6 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0052, Val Loss: 0.0225\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0223, Val Loss: 0.0054\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0060, Val Loss: 0.0061\n",
            "Epoch [4/50], Train Loss: 0.0085, Val Loss: 0.0110\n",
            "Epoch [5/50], Train Loss: 0.0137, Val Loss: 0.0107\n",
            "Epoch [6/50], Train Loss: 0.0134, Val Loss: 0.0071\n",
            "Epoch [7/50], Train Loss: 0.0096, Val Loss: 0.0043\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0061, Val Loss: 0.0047\n",
            "Epoch [9/50], Train Loss: 0.0054, Val Loss: 0.0076\n",
            "Epoch [10/50], Train Loss: 0.0079, Val Loss: 0.0096\n",
            "Epoch [11/50], Train Loss: 0.0098, Val Loss: 0.0087\n",
            "Epoch [12/50], Train Loss: 0.0090, Val Loss: 0.0063\n",
            "Epoch [13/50], Train Loss: 0.0067, Val Loss: 0.0045\n",
            "Epoch [14/50], Train Loss: 0.0055, Val Loss: 0.0042\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [15/50], Train Loss: 0.0055, Val Loss: 0.0048\n",
            "Epoch [16/50], Train Loss: 0.0067, Val Loss: 0.0054\n",
            "Epoch [17/50], Train Loss: 0.0075, Val Loss: 0.0052\n",
            "Epoch [18/50], Train Loss: 0.0071, Val Loss: 0.0046\n",
            "Epoch [19/50], Train Loss: 0.0065, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [20/50], Train Loss: 0.0055, Val Loss: 0.0042\n",
            "Epoch [21/50], Train Loss: 0.0055, Val Loss: 0.0049\n",
            "Epoch [22/50], Train Loss: 0.0058, Val Loss: 0.0056\n",
            "Epoch [23/50], Train Loss: 0.0062, Val Loss: 0.0058\n",
            "Epoch [24/50], Train Loss: 0.0063, Val Loss: 0.0052\n",
            "Epoch [25/50], Train Loss: 0.0061, Val Loss: 0.0045\n",
            "Epoch [26/50], Train Loss: 0.0055, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [27/50], Train Loss: 0.0054, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [28/50], Train Loss: 0.0051, Val Loss: 0.0041\n",
            "Epoch [29/50], Train Loss: 0.0056, Val Loss: 0.0041\n",
            "Epoch [30/50], Train Loss: 0.0056, Val Loss: 0.0040\n",
            "Epoch [31/50], Train Loss: 0.0055, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [32/50], Train Loss: 0.0054, Val Loss: 0.0040\n",
            "Epoch [33/50], Train Loss: 0.0052, Val Loss: 0.0042\n",
            "Epoch [34/50], Train Loss: 0.0054, Val Loss: 0.0044\n",
            "Epoch [35/50], Train Loss: 0.0054, Val Loss: 0.0045\n",
            "Epoch [36/50], Train Loss: 0.0053, Val Loss: 0.0044\n",
            "Epoch [37/50], Train Loss: 0.0053, Val Loss: 0.0041\n",
            "Epoch [38/50], Train Loss: 0.0053, Val Loss: 0.0040\n",
            "Epoch [39/50], Train Loss: 0.0053, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [40/50], Train Loss: 0.0048, Val Loss: 0.0039\n",
            "Epoch [41/50], Train Loss: 0.0052, Val Loss: 0.0039\n",
            "Epoch [42/50], Train Loss: 0.0052, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [43/50], Train Loss: 0.0053, Val Loss: 0.0039\n",
            "Epoch [44/50], Train Loss: 0.0051, Val Loss: 0.0040\n",
            "Epoch [45/50], Train Loss: 0.0051, Val Loss: 0.0041\n",
            "Epoch [46/50], Train Loss: 0.0053, Val Loss: 0.0042\n",
            "Epoch [47/50], Train Loss: 0.0049, Val Loss: 0.0042\n",
            "Epoch [48/50], Train Loss: 0.0052, Val Loss: 0.0041\n",
            "Epoch [49/50], Train Loss: 0.0052, Val Loss: 0.0040\n",
            "Epoch [50/50], Train Loss: 0.0050, Val Loss: 0.0039\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 6 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 7 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0051, Val Loss: 0.0195\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0193, Val Loss: 0.0052\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0058, Val Loss: 0.0056\n",
            "Epoch [4/50], Train Loss: 0.0077, Val Loss: 0.0095\n",
            "Epoch [5/50], Train Loss: 0.0129, Val Loss: 0.0089\n",
            "Epoch [6/50], Train Loss: 0.0118, Val Loss: 0.0059\n",
            "Epoch [7/50], Train Loss: 0.0079, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0055, Val Loss: 0.0049\n",
            "Epoch [9/50], Train Loss: 0.0057, Val Loss: 0.0075\n",
            "Epoch [10/50], Train Loss: 0.0078, Val Loss: 0.0087\n",
            "Epoch [11/50], Train Loss: 0.0090, Val Loss: 0.0075\n",
            "Epoch [12/50], Train Loss: 0.0077, Val Loss: 0.0054\n",
            "Epoch [13/50], Train Loss: 0.0060, Val Loss: 0.0041\n",
            "Epoch [14/50], Train Loss: 0.0053, Val Loss: 0.0041\n",
            "Epoch [15/50], Train Loss: 0.0057, Val Loss: 0.0047\n",
            "Epoch [16/50], Train Loss: 0.0064, Val Loss: 0.0050\n",
            "Epoch [17/50], Train Loss: 0.0072, Val Loss: 0.0047\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 7 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 8 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0054, Val Loss: 0.0167\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0164, Val Loss: 0.0050\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0061, Val Loss: 0.0053\n",
            "Epoch [4/50], Train Loss: 0.0073, Val Loss: 0.0083\n",
            "Epoch [5/50], Train Loss: 0.0107, Val Loss: 0.0075\n",
            "Epoch [6/50], Train Loss: 0.0101, Val Loss: 0.0049\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0067, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0053, Val Loss: 0.0057\n",
            "Epoch [9/50], Train Loss: 0.0061, Val Loss: 0.0078\n",
            "Epoch [10/50], Train Loss: 0.0078, Val Loss: 0.0077\n",
            "Epoch [11/50], Train Loss: 0.0080, Val Loss: 0.0059\n",
            "Epoch [12/50], Train Loss: 0.0062, Val Loss: 0.0043\n",
            "Epoch [13/50], Train Loss: 0.0052, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0054, Val Loss: 0.0045\n",
            "Epoch [15/50], Train Loss: 0.0064, Val Loss: 0.0049\n",
            "Epoch [16/50], Train Loss: 0.0069, Val Loss: 0.0046\n",
            "Epoch [17/50], Train Loss: 0.0064, Val Loss: 0.0041\n",
            "Epoch [18/50], Train Loss: 0.0057, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [19/50], Train Loss: 0.0052, Val Loss: 0.0044\n",
            "Epoch [20/50], Train Loss: 0.0053, Val Loss: 0.0051\n",
            "Epoch [21/50], Train Loss: 0.0059, Val Loss: 0.0054\n",
            "Epoch [22/50], Train Loss: 0.0058, Val Loss: 0.0050\n",
            "Epoch [23/50], Train Loss: 0.0057, Val Loss: 0.0043\n",
            "Epoch [24/50], Train Loss: 0.0049, Val Loss: 0.0039\n",
            "Epoch [25/50], Train Loss: 0.0049, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [26/50], Train Loss: 0.0052, Val Loss: 0.0040\n",
            "Epoch [27/50], Train Loss: 0.0055, Val Loss: 0.0040\n",
            "Epoch [28/50], Train Loss: 0.0054, Val Loss: 0.0039\n",
            "Epoch [29/50], Train Loss: 0.0053, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [30/50], Train Loss: 0.0051, Val Loss: 0.0040\n",
            "Epoch [31/50], Train Loss: 0.0051, Val Loss: 0.0043\n",
            "Epoch [32/50], Train Loss: 0.0051, Val Loss: 0.0045\n",
            "Epoch [33/50], Train Loss: 0.0051, Val Loss: 0.0044\n",
            "Epoch [34/50], Train Loss: 0.0052, Val Loss: 0.0042\n",
            "Epoch [35/50], Train Loss: 0.0050, Val Loss: 0.0040\n",
            "Epoch [36/50], Train Loss: 0.0049, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [37/50], Train Loss: 0.0050, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [38/50], Train Loss: 0.0051, Val Loss: 0.0039\n",
            "Epoch [39/50], Train Loss: 0.0052, Val Loss: 0.0038\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [40/50], Train Loss: 0.0050, Val Loss: 0.0039\n",
            "Epoch [41/50], Train Loss: 0.0049, Val Loss: 0.0040\n",
            "Epoch [42/50], Train Loss: 0.0050, Val Loss: 0.0041\n",
            "Epoch [43/50], Train Loss: 0.0049, Val Loss: 0.0042\n",
            "Epoch [44/50], Train Loss: 0.0050, Val Loss: 0.0041\n",
            "Epoch [45/50], Train Loss: 0.0049, Val Loss: 0.0040\n",
            "Epoch [46/50], Train Loss: 0.0050, Val Loss: 0.0039\n",
            "Epoch [47/50], Train Loss: 0.0050, Val Loss: 0.0039\n",
            "Epoch [48/50], Train Loss: 0.0050, Val Loss: 0.0038\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [49/50], Train Loss: 0.0051, Val Loss: 0.0039\n",
            "Epoch [50/50], Train Loss: 0.0049, Val Loss: 0.0039\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 8 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 9 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0051, Val Loss: 0.0175\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0172, Val Loss: 0.0050\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0057, Val Loss: 0.0054\n",
            "Epoch [4/50], Train Loss: 0.0075, Val Loss: 0.0087\n",
            "Epoch [5/50], Train Loss: 0.0113, Val Loss: 0.0081\n",
            "Epoch [6/50], Train Loss: 0.0109, Val Loss: 0.0054\n",
            "Epoch [7/50], Train Loss: 0.0076, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0053, Val Loss: 0.0050\n",
            "Epoch [9/50], Train Loss: 0.0058, Val Loss: 0.0074\n",
            "Epoch [10/50], Train Loss: 0.0076, Val Loss: 0.0083\n",
            "Epoch [11/50], Train Loss: 0.0083, Val Loss: 0.0071\n",
            "Epoch [12/50], Train Loss: 0.0072, Val Loss: 0.0052\n",
            "Epoch [13/50], Train Loss: 0.0058, Val Loss: 0.0040\n",
            "Epoch [14/50], Train Loss: 0.0051, Val Loss: 0.0040\n",
            "Epoch [15/50], Train Loss: 0.0056, Val Loss: 0.0046\n",
            "Epoch [16/50], Train Loss: 0.0062, Val Loss: 0.0048\n",
            "Epoch [17/50], Train Loss: 0.0068, Val Loss: 0.0045\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 9 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 10 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0051, Val Loss: 0.0155\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0152, Val Loss: 0.0049\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0058, Val Loss: 0.0051\n",
            "Epoch [4/50], Train Loss: 0.0072, Val Loss: 0.0079\n",
            "Epoch [5/50], Train Loss: 0.0107, Val Loss: 0.0070\n",
            "Epoch [6/50], Train Loss: 0.0095, Val Loss: 0.0047\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0067, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0050, Val Loss: 0.0055\n",
            "Epoch [9/50], Train Loss: 0.0060, Val Loss: 0.0073\n",
            "Epoch [10/50], Train Loss: 0.0077, Val Loss: 0.0073\n",
            "Epoch [11/50], Train Loss: 0.0075, Val Loss: 0.0057\n",
            "Epoch [12/50], Train Loss: 0.0063, Val Loss: 0.0043\n",
            "Epoch [13/50], Train Loss: 0.0049, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0052, Val Loss: 0.0043\n",
            "Epoch [15/50], Train Loss: 0.0060, Val Loss: 0.0046\n",
            "Epoch [16/50], Train Loss: 0.0065, Val Loss: 0.0045\n",
            "Epoch [17/50], Train Loss: 0.0062, Val Loss: 0.0040\n",
            "Epoch [18/50], Train Loss: 0.0053, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [19/50], Train Loss: 0.0047, Val Loss: 0.0043\n",
            "Epoch [20/50], Train Loss: 0.0051, Val Loss: 0.0049\n",
            "Epoch [21/50], Train Loss: 0.0056, Val Loss: 0.0052\n",
            "Epoch [22/50], Train Loss: 0.0057, Val Loss: 0.0048\n",
            "Epoch [23/50], Train Loss: 0.0055, Val Loss: 0.0043\n",
            "Epoch [24/50], Train Loss: 0.0052, Val Loss: 0.0039\n",
            "Epoch [25/50], Train Loss: 0.0047, Val Loss: 0.0039\n",
            "Epoch [26/50], Train Loss: 0.0051, Val Loss: 0.0040\n",
            "Epoch [27/50], Train Loss: 0.0053, Val Loss: 0.0040\n",
            "Epoch [28/50], Train Loss: 0.0053, Val Loss: 0.0039\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 10 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 11 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0049, Val Loss: 0.0166\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0160, Val Loss: 0.0049\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0054, Val Loss: 0.0053\n",
            "Epoch [4/50], Train Loss: 0.0073, Val Loss: 0.0083\n",
            "Epoch [5/50], Train Loss: 0.0107, Val Loss: 0.0076\n",
            "Epoch [6/50], Train Loss: 0.0098, Val Loss: 0.0051\n",
            "Epoch [7/50], Train Loss: 0.0069, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0050, Val Loss: 0.0051\n",
            "Epoch [9/50], Train Loss: 0.0055, Val Loss: 0.0073\n",
            "Epoch [10/50], Train Loss: 0.0074, Val Loss: 0.0079\n",
            "Epoch [11/50], Train Loss: 0.0082, Val Loss: 0.0066\n",
            "Epoch [12/50], Train Loss: 0.0071, Val Loss: 0.0048\n",
            "Epoch [13/50], Train Loss: 0.0054, Val Loss: 0.0039\n",
            "Epoch [14/50], Train Loss: 0.0050, Val Loss: 0.0041\n",
            "Epoch [15/50], Train Loss: 0.0056, Val Loss: 0.0046\n",
            "Epoch [16/50], Train Loss: 0.0064, Val Loss: 0.0048\n",
            "Epoch [17/50], Train Loss: 0.0067, Val Loss: 0.0044\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 11 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 12 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0048, Val Loss: 0.0154\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0148, Val Loss: 0.0049\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0055, Val Loss: 0.0052\n",
            "Epoch [4/50], Train Loss: 0.0070, Val Loss: 0.0079\n",
            "Epoch [5/50], Train Loss: 0.0104, Val Loss: 0.0071\n",
            "Epoch [6/50], Train Loss: 0.0094, Val Loss: 0.0048\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0067, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0050, Val Loss: 0.0054\n",
            "Epoch [9/50], Train Loss: 0.0061, Val Loss: 0.0073\n",
            "Epoch [10/50], Train Loss: 0.0073, Val Loss: 0.0076\n",
            "Epoch [11/50], Train Loss: 0.0075, Val Loss: 0.0061\n",
            "Epoch [12/50], Train Loss: 0.0065, Val Loss: 0.0045\n",
            "Epoch [13/50], Train Loss: 0.0051, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0051, Val Loss: 0.0042\n",
            "Epoch [15/50], Train Loss: 0.0056, Val Loss: 0.0046\n",
            "Epoch [16/50], Train Loss: 0.0065, Val Loss: 0.0046\n",
            "Epoch [17/50], Train Loss: 0.0060, Val Loss: 0.0042\n",
            "Epoch [18/50], Train Loss: 0.0057, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [19/50], Train Loss: 0.0050, Val Loss: 0.0041\n",
            "Epoch [20/50], Train Loss: 0.0049, Val Loss: 0.0047\n",
            "Epoch [21/50], Train Loss: 0.0053, Val Loss: 0.0052\n",
            "Epoch [22/50], Train Loss: 0.0057, Val Loss: 0.0051\n",
            "Epoch [23/50], Train Loss: 0.0057, Val Loss: 0.0046\n",
            "Epoch [24/50], Train Loss: 0.0055, Val Loss: 0.0041\n",
            "Epoch [25/50], Train Loss: 0.0051, Val Loss: 0.0039\n",
            "Epoch [26/50], Train Loss: 0.0051, Val Loss: 0.0040\n",
            "Epoch [27/50], Train Loss: 0.0054, Val Loss: 0.0040\n",
            "Epoch [28/50], Train Loss: 0.0053, Val Loss: 0.0040\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 12 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 13 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0051, Val Loss: 0.0144\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0142, Val Loss: 0.0048\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0055, Val Loss: 0.0051\n",
            "Epoch [4/50], Train Loss: 0.0067, Val Loss: 0.0076\n",
            "Epoch [5/50], Train Loss: 0.0100, Val Loss: 0.0068\n",
            "Epoch [6/50], Train Loss: 0.0090, Val Loss: 0.0046\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0064, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0049, Val Loss: 0.0054\n",
            "Epoch [9/50], Train Loss: 0.0060, Val Loss: 0.0072\n",
            "Epoch [10/50], Train Loss: 0.0071, Val Loss: 0.0073\n",
            "Epoch [11/50], Train Loss: 0.0074, Val Loss: 0.0058\n",
            "Epoch [12/50], Train Loss: 0.0062, Val Loss: 0.0044\n",
            "Epoch [13/50], Train Loss: 0.0049, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0048, Val Loss: 0.0042\n",
            "Epoch [15/50], Train Loss: 0.0057, Val Loss: 0.0046\n",
            "Epoch [16/50], Train Loss: 0.0062, Val Loss: 0.0045\n",
            "Epoch [17/50], Train Loss: 0.0062, Val Loss: 0.0041\n",
            "Epoch [18/50], Train Loss: 0.0056, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [19/50], Train Loss: 0.0050, Val Loss: 0.0042\n",
            "Epoch [20/50], Train Loss: 0.0047, Val Loss: 0.0048\n",
            "Epoch [21/50], Train Loss: 0.0055, Val Loss: 0.0052\n",
            "Epoch [22/50], Train Loss: 0.0058, Val Loss: 0.0050\n",
            "Epoch [23/50], Train Loss: 0.0055, Val Loss: 0.0045\n",
            "Epoch [24/50], Train Loss: 0.0053, Val Loss: 0.0040\n",
            "Epoch [25/50], Train Loss: 0.0050, Val Loss: 0.0039\n",
            "Epoch [26/50], Train Loss: 0.0050, Val Loss: 0.0040\n",
            "Epoch [27/50], Train Loss: 0.0051, Val Loss: 0.0040\n",
            "Epoch [28/50], Train Loss: 0.0051, Val Loss: 0.0040\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 13 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 14 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0052, Val Loss: 0.0146\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0142, Val Loss: 0.0049\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0052, Val Loss: 0.0051\n",
            "Epoch [4/50], Train Loss: 0.0069, Val Loss: 0.0076\n",
            "Epoch [5/50], Train Loss: 0.0098, Val Loss: 0.0069\n",
            "Epoch [6/50], Train Loss: 0.0091, Val Loss: 0.0047\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0064, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0051, Val Loss: 0.0053\n",
            "Epoch [9/50], Train Loss: 0.0058, Val Loss: 0.0070\n",
            "Epoch [10/50], Train Loss: 0.0070, Val Loss: 0.0073\n",
            "Epoch [11/50], Train Loss: 0.0073, Val Loss: 0.0059\n",
            "Epoch [12/50], Train Loss: 0.0064, Val Loss: 0.0045\n",
            "Epoch [13/50], Train Loss: 0.0054, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0051, Val Loss: 0.0042\n",
            "Epoch [15/50], Train Loss: 0.0058, Val Loss: 0.0045\n",
            "Epoch [16/50], Train Loss: 0.0063, Val Loss: 0.0045\n",
            "Epoch [17/50], Train Loss: 0.0062, Val Loss: 0.0041\n",
            "Epoch [18/50], Train Loss: 0.0056, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [19/50], Train Loss: 0.0050, Val Loss: 0.0041\n",
            "Epoch [20/50], Train Loss: 0.0050, Val Loss: 0.0047\n",
            "Epoch [21/50], Train Loss: 0.0055, Val Loss: 0.0052\n",
            "Epoch [22/50], Train Loss: 0.0057, Val Loss: 0.0052\n",
            "Epoch [23/50], Train Loss: 0.0056, Val Loss: 0.0046\n",
            "Epoch [24/50], Train Loss: 0.0054, Val Loss: 0.0041\n",
            "Epoch [25/50], Train Loss: 0.0048, Val Loss: 0.0039\n",
            "Epoch [26/50], Train Loss: 0.0051, Val Loss: 0.0040\n",
            "Epoch [27/50], Train Loss: 0.0052, Val Loss: 0.0041\n",
            "Epoch [28/50], Train Loss: 0.0055, Val Loss: 0.0040\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 14 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 15 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0052, Val Loss: 0.0137\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0132, Val Loss: 0.0048\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0053, Val Loss: 0.0050\n",
            "Epoch [4/50], Train Loss: 0.0066, Val Loss: 0.0073\n",
            "Epoch [5/50], Train Loss: 0.0099, Val Loss: 0.0065\n",
            "Epoch [6/50], Train Loss: 0.0087, Val Loss: 0.0045\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0062, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0051, Val Loss: 0.0054\n",
            "Epoch [9/50], Train Loss: 0.0060, Val Loss: 0.0071\n",
            "Epoch [10/50], Train Loss: 0.0073, Val Loss: 0.0070\n",
            "Epoch [11/50], Train Loss: 0.0071, Val Loss: 0.0056\n",
            "Epoch [12/50], Train Loss: 0.0060, Val Loss: 0.0043\n",
            "Epoch [13/50], Train Loss: 0.0050, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0052, Val Loss: 0.0042\n",
            "Epoch [15/50], Train Loss: 0.0057, Val Loss: 0.0045\n",
            "Epoch [16/50], Train Loss: 0.0063, Val Loss: 0.0044\n",
            "Epoch [17/50], Train Loss: 0.0060, Val Loss: 0.0040\n",
            "Epoch [18/50], Train Loss: 0.0054, Val Loss: 0.0039\n",
            "Epoch [19/50], Train Loss: 0.0050, Val Loss: 0.0043\n",
            "Epoch [20/50], Train Loss: 0.0049, Val Loss: 0.0048\n",
            "Epoch [21/50], Train Loss: 0.0052, Val Loss: 0.0051\n",
            "Epoch [22/50], Train Loss: 0.0060, Val Loss: 0.0049\n",
            "Epoch [23/50], Train Loss: 0.0052, Val Loss: 0.0043\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 15 ---\n",
            "\n",
            "--- Final Evaluation and Comparison of Best Models ---\n",
            "Final Real-trained Model Metrics: {'MSE': 32.009849548339844, 'RMSE': 5.657724767814342, 'R2': 0.7092070579528809, 'Directional Accuracy': 0.4966442953020134}\n",
            "Final Synthetic-trained Model Metrics: {'MSE': 107.404541015625, 'RMSE': 10.363616213254184, 'R2': 0.3665919303894043, 'Directional Accuracy': 0.4228187919463087}\n",
            "Comparing MSE between Real-trained vs Synthetic-trained:\n",
            "Single trial comparison - Real-trained MSE: 32.0098, Synthetic-trained MSE: 107.4045\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Example scaling\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(real_data.values.reshape(-1, 1))  # reshape if 1D\n",
        "\n",
        "# Use scaled_data in your training pipeline\n"
      ],
      "metadata": {
        "id": "hPPcuqxtAkXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model weights\n",
        "model = LSTMModel()  # Make sure this matches your LSTM class definition\n",
        "# Corrected filename to load the synthetic model\n",
        "model.load_state_dict(torch.load('lstm_model_synthetic_best.pth'))\n",
        "\n",
        "# Ensure the model is on the same device as the data before training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Model has already been trained and saved, no need to call train_lstm_model again here.\n",
        "\n",
        "print(\"Synthetic model loaded successfully.\")"
      ],
      "metadata": {
        "id": "_eFx0WgiAl45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa286bf0-1a9f-4137-89e8-46b4bfe6ffdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_stock_data(ticker, start, end):\n",
        "    df = yf.download(ticker, start=start, end=end)\n",
        "    df = df[['Close']].dropna()\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "def generate_synthetic_data(real_data, noise_level=0.05):\n",
        "    clean_data = real_data.dropna().reset_index(drop=True)\n",
        "    noise = np.random.normal(loc=0.0, scale=noise_level, size=clean_data.shape)\n",
        "    synthetic = clean_data.values * (1 + noise)\n",
        "    synthetic_df = pd.DataFrame(synthetic, columns=clean_data.columns)\n",
        "    # Forward and backward fill in case NaNs remain\n",
        "    synthetic_df = synthetic_df.fillna(method='ffill').fillna(method='bfill')\n",
        "    return synthetic_df\n",
        "\n",
        "def scale_data(data):\n",
        "    scaler = MinMaxScaler()\n",
        "    # Ensure data.values is 2D for fit_transform if it's not already\n",
        "    scaled = scaler.fit_transform(data.values.reshape(-1, 1) if data.ndim == 2 and data.shape[1] == 1 else data.values)\n",
        "    return scaled, scaler\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def split_data(X, y, train_ratio=0.8):\n",
        "    split_idx = int(len(X) * train_ratio)\n",
        "    X_train = X[:split_idx]\n",
        "    X_test = X[split_idx:]\n",
        "    y_train = y[:split_idx]\n",
        "    y_test = y[split_idx:]\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "def train_lstm_model(data, model=None, seq_length=10, epochs=20, batch_size=32, lr=0.001):\n",
        "    scaled_data, scaler = scale_data(data)\n",
        "    X, y = create_sequences(scaled_data, seq_length)\n",
        "\n",
        "    X_train, y_train, X_test, y_test = split_data(X, y)\n",
        "\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "\n",
        "    # ADJUSTED LINE: Ensure y_train is (num_samples, 1)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "    # ADJUSTED LINE: Ensure y_test is (num_samples, 1) for consistent handling in evaluation\n",
        "    y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if model is None:\n",
        "        model = LSTMModel(input_size=X_train.shape[-1] if X_train.ndim > 2 else 1)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    print(f\"Starting training for {epochs} epochs...\")\n",
        "\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        permutation = torch.randperm(X_train.size(0))\n",
        "        epoch_losses = []\n",
        "        for i in range(0, X_train.size(0), batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_x, batch_y = X_train[indices].to(device), y_train[indices].to(device)\n",
        "\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_epoch_loss = np.mean(epoch_losses)\n",
        "        train_losses.append(avg_epoch_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    model.eval()\n",
        "    print(\"Starting evaluation...\")\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X_test.to(device)).cpu().numpy()\n",
        "        y_test_np = y_test.cpu().numpy() # Convert to numpy for evaluation\n",
        "\n",
        "        predictions_rescaled = scaler.inverse_transform(predictions)\n",
        "        y_test_rescaled = scaler.inverse_transform(y_test_np) # Use the correctly shaped numpy array\n",
        "\n",
        "    metrics = evaluate_model(y_test_rescaled, predictions_rescaled)\n",
        "    print(\"Evaluation finished.\")\n",
        "    print(\"Evaluation Metrics:\", metrics)\n",
        "\n",
        "    return metrics, model, scaler, train_losses\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    y_true_flat = y_true.flatten() if y_true.ndim > 1 else y_true\n",
        "    y_pred_flat = y_pred.flatten() if y_pred.ndim > 1 else y_pred\n",
        "\n",
        "    mse = mean_squared_error(y_true_flat, y_pred_flat)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true_flat, y_pred_flat)\n",
        "\n",
        "    if len(y_true_flat) > 1 and len(y_pred_flat) > 1:\n",
        "        y_true_diff = np.diff(y_true_flat)\n",
        "        y_pred_diff = np.diff(y_pred_flat)\n",
        "\n",
        "        correct_direction = np.sum(np.sign(y_true_diff) == np.sign(y_pred_diff))\n",
        "        directional_accuracy = correct_direction / len(y_true_diff) if len(y_true_diff) > 0 else np.nan\n",
        "    else:\n",
        "        directional_accuracy = np.nan\n",
        "\n",
        "    return {\n",
        "        \"MSE\": mse,\n",
        "        \"RMSE\": rmse,\n",
        "        \"R2\": r2,\n",
        "        \"Directional Accuracy\": directional_accuracy\n",
        "    }\n",
        "\n",
        "def compare_models(metric_real, metric_synthetic, metric_name=\"MSE\"):\n",
        "    print(f\"Comparing {metric_name} between Real-trained vs Synthetic-trained:\")\n",
        "    stat, p = None, None\n",
        "    if len(metric_real) > 1 and len(metric_synthetic) > 1 and len(metric_real) == len(metric_synthetic):\n",
        "        stat, p = ttest_rel(metric_real, metric_synthetic)\n",
        "        print(f\"Paired T-test: stat={stat:.4f}, p={p:.4f}\")\n",
        "    elif len(metric_real) == 1 and len(metric_synthetic) == 1:\n",
        "        print(f\"Single trial comparison - Real-trained {metric_name}: {metric_real[0]:.4f}, Synthetic-trained {metric_name}: {metric_synthetic[0]:.4f}\")\n",
        "    else:\n",
        "        print(\"Cannot perform statistical test: Mismatched or insufficient data for comparison.\")\n",
        "    return stat, p\n",
        "\n",
        "# Load real data\n",
        "real_data = get_stock_data(\"AAPL\", \"2020-01-01\", \"2023-01-01\")\n",
        "\n",
        "# Generate synthetic data\n",
        "synthetic_data = generate_synthetic_data(real_data)\n",
        "\n",
        "# Train on real data\n",
        "real_metrics, real_model, real_scaler, real_train_losses = train_lstm_model(real_data)\n",
        "torch.save(real_model.state_dict(), 'lstm_model_real.pth')\n",
        "print(\"Real data model trained and saved.\")\n",
        "\n",
        "# Train on synthetic data\n",
        "synthetic_metrics, synthetic_model, synthetic_scaler, synthetic_train_losses = train_lstm_model(synthetic_data)\n",
        "torch.save(synthetic_model.state_dict(), 'lstm_model_synthetic.pth')\n",
        "print(\"Synthetic data model trained and saved.\")\n",
        "\n",
        "# Perform statistical comparison on MSE\n",
        "# Use the metrics dictionaries returned by train_lstm_model\n",
        "compare_models([real_metrics['MSE']], [synthetic_metrics['MSE']], metric_name=\"MSE\")"
      ],
      "metadata": {
        "id": "di7lcMaLAqg3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "107caf7c-04a4-4d2e-d501-9c5b11b2f41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-268532145.py:2: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=start, end=end)\n",
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-268532145.py:13: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  synthetic_df = synthetic_df.fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 20 epochs...\n",
            "Epoch 1/20, Avg Loss: 0.1624\n",
            "Epoch 2/20, Avg Loss: 0.0587\n",
            "Epoch 3/20, Avg Loss: 0.0352\n",
            "Epoch 4/20, Avg Loss: 0.0127\n",
            "Epoch 5/20, Avg Loss: 0.0054\n",
            "Epoch 6/20, Avg Loss: 0.0037\n",
            "Epoch 7/20, Avg Loss: 0.0031\n",
            "Epoch 8/20, Avg Loss: 0.0026\n",
            "Epoch 9/20, Avg Loss: 0.0025\n",
            "Epoch 10/20, Avg Loss: 0.0022\n",
            "Epoch 11/20, Avg Loss: 0.0022\n",
            "Epoch 12/20, Avg Loss: 0.0023\n",
            "Epoch 13/20, Avg Loss: 0.0022\n",
            "Epoch 14/20, Avg Loss: 0.0023\n",
            "Epoch 15/20, Avg Loss: 0.0020\n",
            "Epoch 16/20, Avg Loss: 0.0019\n",
            "Epoch 17/20, Avg Loss: 0.0020\n",
            "Epoch 18/20, Avg Loss: 0.0020\n",
            "Epoch 19/20, Avg Loss: 0.0019\n",
            "Epoch 20/20, Avg Loss: 0.0020\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 44.22727966308594, 'RMSE': 6.650359363454425, 'R2': 0.5982179641723633, 'Directional Accuracy': 0.5033557046979866}\n",
            "Real data model trained and saved.\n",
            "Starting training for 20 epochs...\n",
            "Epoch 1/20, Avg Loss: 0.1642\n",
            "Epoch 2/20, Avg Loss: 0.0484\n",
            "Epoch 3/20, Avg Loss: 0.0346\n",
            "Epoch 4/20, Avg Loss: 0.0225\n",
            "Epoch 5/20, Avg Loss: 0.0093\n",
            "Epoch 6/20, Avg Loss: 0.0068\n",
            "Epoch 7/20, Avg Loss: 0.0057\n",
            "Epoch 8/20, Avg Loss: 0.0054\n",
            "Epoch 9/20, Avg Loss: 0.0050\n",
            "Epoch 10/20, Avg Loss: 0.0046\n",
            "Epoch 11/20, Avg Loss: 0.0045\n",
            "Epoch 12/20, Avg Loss: 0.0054\n",
            "Epoch 13/20, Avg Loss: 0.0052\n",
            "Epoch 14/20, Avg Loss: 0.0047\n",
            "Epoch 15/20, Avg Loss: 0.0039\n",
            "Epoch 16/20, Avg Loss: 0.0040\n",
            "Epoch 17/20, Avg Loss: 0.0041\n",
            "Epoch 18/20, Avg Loss: 0.0042\n",
            "Epoch 19/20, Avg Loss: 0.0043\n",
            "Epoch 20/20, Avg Loss: 0.0041\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 93.20028686523438, 'RMSE': 9.654029566208836, 'R2': 0.41659247875213623, 'Directional Accuracy': 0.47651006711409394}\n",
            "Synthetic data model trained and saved.\n",
            "Comparing MSE between Real-trained vs Synthetic-trained:\n",
            "Single trial comparison - Real-trained MSE: 44.2273, Synthetic-trained MSE: 93.2003\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "id": "7nT0wVVBASvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b5c002-4a57-4e63-bc62-963e9981a831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained models\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate the models\n",
        "real_model_wgan = LSTMModel()\n",
        "synthetic_model_wgan = LSTMModel()\n",
        "\n",
        "# Load the state dictionaries\n",
        "real_model_wgan.load_state_dict(torch.load('lstm_model_real.pth'))\n",
        "synthetic_model_wgan.load_state_dict(torch.load('lstm_model_synthetic.pth'))\n",
        "\n",
        "# Move models to the device\n",
        "real_model_wgan.to(device)\n",
        "synthetic_model_wgan.to(device)\n",
        "\n",
        "# Set models to evaluation mode\n",
        "real_model_wgan.eval()\n",
        "synthetic_model_wgan.eval()\n",
        "\n",
        "# Preprocess the real data to get the test set for evaluation\n",
        "# Assuming real_data is already loaded from previous steps\n",
        "scaled_real_data_eval, real_scaler_eval = scale_data(real_data)\n",
        "X_real_eval, y_real_eval = create_sequences(scaled_real_data_eval, seq_length=10) # Use the same seq_length as training\n",
        "_, _, X_real_test_eval, y_real_test_eval = split_data(X_real_eval, y_real_eval)\n",
        "\n",
        "# Convert test data to tensors\n",
        "X_real_test_eval_tensor = torch.tensor(X_real_test_eval, dtype=torch.float32).to(device)\n",
        "y_real_test_eval_numpy = y_real_test_eval # Keep y_real_test_eval as numpy for inverse scaling comparison\n",
        "\n",
        "# Evaluate the real-trained model on the real test set\n",
        "with torch.no_grad():\n",
        "    real_predictions_scaled_wgan = real_model_wgan(X_real_test_eval_tensor).cpu().numpy()\n",
        "real_predictions_rescaled_wgan = real_scaler_eval.inverse_transform(real_predictions_scaled_wgan)\n",
        "real_metrics_final = evaluate_model(y_real_test_eval_numpy, real_predictions_rescaled_wgan)\n",
        "\n",
        "# Evaluate the WGAN-synthetic-trained model on the real test set\n",
        "with torch.no_grad():\n",
        "    synthetic_predictions_scaled_wgan = synthetic_model_wgan(X_real_test_eval_tensor).cpu().numpy()\n",
        "# Use the scaler from the real data, not the synthetic data, for evaluating on real data\n",
        "synthetic_predictions_rescaled_wgan = real_scaler_eval.inverse_transform(synthetic_predictions_scaled_wgan)\n",
        "synthetic_metrics_final = evaluate_model(y_real_test_eval_numpy, synthetic_predictions_rescaled_wgan)\n",
        "\n",
        "# Print final evaluation metrics\n",
        "print(\"\\n--- Final Evaluation Metrics on Real Test Set ---\")\n",
        "print(\"Real-trained Model Performance:\", real_metrics_final)\n",
        "print(\"WGAN-Synthetic-trained Model Performance:\", synthetic_metrics_final)\n",
        "\n",
        "# Perform statistical comparison on MSE\n",
        "compare_models([real_metrics_final['MSE']], [synthetic_metrics_final['MSE']], metric_name=\"MSE\")"
      ],
      "metadata": {
        "id": "_FUlSKO2AtY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98f7dfb-14ba-4e50-98f2-c2802a77edbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation Metrics on Real Test Set ---\n",
            "Real-trained Model Performance: {'MSE': 22037.07944433775, 'RMSE': 148.4489119001475, 'R2': -3084473.0948626404, 'Directional Accuracy': 0.5033557046979866}\n",
            "WGAN-Synthetic-trained Model Performance: {'MSE': 21416.718651427305, 'RMSE': 146.34452040109772, 'R2': -2997642.768728277, 'Directional Accuracy': 0.5100671140939598}\n",
            "Comparing MSE between Real-trained vs Synthetic-trained:\n",
            "Single trial comparison - Real-trained MSE: 22037.0794, Synthetic-trained MSE: 21416.7187\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from scipy.stats import ttest_rel\n",
        "import yfinance as yf\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "# Define necessary helper functions (assuming they are not already defined correctly)\n",
        "def get_stock_data(ticker, start, end):\n",
        "    df = yf.download(ticker, start=start, end=end)\n",
        "    df = df[['Close']].dropna()\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]\n",
        "    df.columns = ['Close']\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "def scale_data(data):\n",
        "    scaler = MinMaxScaler()\n",
        "    transformed_data = None\n",
        "\n",
        "    if isinstance(data, pd.Series):\n",
        "        transformed_data = data.values.reshape(-1, 1)\n",
        "    elif isinstance(data, np.ndarray) and data.ndim == 1:\n",
        "        transformed_data = data.reshape(-1, 1)\n",
        "    elif isinstance(data, pd.DataFrame):\n",
        "        if 'Close' in data.columns and data.shape[1] > 1:\n",
        "            transformed_data = data[['Close']].values\n",
        "        else:\n",
        "            transformed_data = data.values\n",
        "    elif isinstance(data, np.ndarray) and data.ndim == 2:\n",
        "        transformed_data = data\n",
        "    else:\n",
        "        transformed_data = np.array(data).reshape(-1, 1)\n",
        "\n",
        "    scaled = scaler.fit_transform(transformed_data)\n",
        "    return scaled, scaler\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def split_data(X, y, train_ratio=0.8):\n",
        "    split_idx = int(len(X) * train_ratio)\n",
        "    X_train = X[:split_idx]\n",
        "    X_test = X[split_idx:]\n",
        "    y_train = y[:split_idx]\n",
        "    y_test = y[split_idx:]\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=50, num_layers=2, output_size=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# MODIFIED: Removed .unsqueeze(1) for y_train and y_test\n",
        "def train_lstm_model(data, model=None, seq_length=10, epochs=20, batch_size=32, lr=0.001, device=None):\n",
        "    scaled_data, scaler = scale_data(data)\n",
        "    X, y = create_sequences(scaled_data, seq_length)\n",
        "\n",
        "    X_train, y_train, X_test, y_test = split_data(X, y)\n",
        "\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32) # Removed .unsqueeze(1)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.float32) # Removed .unsqueeze(1)\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if model is None:\n",
        "        model = LSTMModel(input_size=X_train.shape[-1] if X_train.ndim > 2 else 1)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    print(f\"Starting training for {epochs} epochs...\")\n",
        "\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        permutation = torch.randperm(X_train.size(0))\n",
        "        epoch_losses = []\n",
        "        for i in range(0, X_train.size(0), batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_x, batch_y = X_train[indices].to(device), y_train[indices].to(device)\n",
        "\n",
        "            # Ensure batch_y has the correct shape for loss calculation\n",
        "            # If batch_y is 1D (e.g., [32]), make it 2D (e.g., [32, 1])\n",
        "            if batch_y.ndim == 1:\n",
        "                batch_y = batch_y.unsqueeze(1)\n",
        "\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_epoch_loss = np.mean(epoch_losses)\n",
        "        train_losses.append(avg_epoch_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    model.eval()\n",
        "    print(\"Starting evaluation...\")\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X_test.to(device)).cpu().numpy()\n",
        "        y_test = y_test.numpy()\n",
        "\n",
        "        predictions_rescaled = scaler.inverse_transform(predictions)\n",
        "        y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    metrics = evaluate_model(y_test_rescaled, predictions_rescaled)\n",
        "    print(\"Evaluation finished.\")\n",
        "    print(\"Evaluation Metrics:\", metrics)\n",
        "\n",
        "    return metrics, model, scaler, train_losses\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    y_true_flat = y_true.flatten() if y_true.ndim > 1 else y_true\n",
        "    y_pred_flat = y_pred.flatten() if y_pred.ndim > 1 else y_pred\n",
        "\n",
        "    mse = mean_squared_error(y_true_flat, y_pred_flat)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true_flat, y_pred_flat)\n",
        "\n",
        "    if len(y_true_flat) > 1 and len(y_pred_flat) > 1:\n",
        "        y_true_diff = np.diff(y_true_flat)\n",
        "        y_pred_diff = np.diff(y_pred_flat)\n",
        "\n",
        "        correct_direction = np.sum(np.sign(y_true_diff) == np.sign(y_pred_diff))\n",
        "        directional_accuracy = correct_direction / len(y_true_diff) if len(y_true_diff) > 0 else np.nan\n",
        "    else:\n",
        "        directional_accuracy = np.nan\n",
        "\n",
        "    return {\n",
        "        \"MSE\": mse,\n",
        "        \"RMSE\": rmse,\n",
        "        \"R2\": r2,\n",
        "        \"Directional Accuracy\": directional_accuracy\n",
        "    }\n",
        "\n",
        "def compare_models(metric_real, metric_synthetic, metric_name=\"MSE\"):\n",
        "    print(f\"Comparing {metric_name} between Real-trained vs Synthetic-trained:\")\n",
        "    stat, p = None, None\n",
        "    if len(metric_real) > 1 and len(metric_synthetic) > 1 and len(metric_real) == len(metric_synthetic):\n",
        "        stat, p = ttest_rel(metric_real, metric_synthetic)\n",
        "        print(f\"Paired T-test: stat={stat:.4f}, p={p:.4f}\")\n",
        "    elif len(metric_real) == 1 and len(metric_synthetic) == 1:\n",
        "        print(f\"Single trial comparison - Real-trained {metric_name}: {metric_real[0]:.4f}, Synthetic-trained {metric_name}: {metric_synthetic[0]:.4f}\")\n",
        "    else:\n",
        "        print(\"Cannot perform statistical test: Mismatched or insufficient data for comparison.\")\n",
        "    return stat, p\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, seq_length, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.fc = nn.Linear(latent_dim, 128 * seq_length)\n",
        "        self.lstm = nn.LSTM(128, 256, 2, batch_first=True)\n",
        "        self.fc_out = nn.Linear(256, output_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.fc(z)\n",
        "        out = out.view(-1, self.seq_length, 128)\n",
        "        out, _ = self.lstm(out)\n",
        "        out = self.fc_out(out)\n",
        "        return torch.tanh(out)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, seq_length):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, 256, 2, batch_first=True)\n",
        "        self.fc = nn.Linear(256 * seq_length, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "    elif classname.find('LSTM') != -1:\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                torch.nn.init.normal_(param.data, 0.0, 0.02)\n",
        "            elif 'weight_hh' in name:\n",
        "                torch.nn.init.normal_(param.data, 0.0, 0.02)\n",
        "            elif 'bias' in name:\n",
        "                param.data.fill_(0)\n",
        "\n",
        "def train_wgan(generator, discriminator, real_data, latent_dim, device,\n",
        "               epochs=100, batch_size=64, lr_g=0.0001, lr_d=0.0001,\n",
        "               critic_iterations=5, lambda_gp=10):\n",
        "    optimizer_g = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.9))\n",
        "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.9))\n",
        "\n",
        "    generator.to(device)\n",
        "    discriminator.to(device)\n",
        "\n",
        "    print(\"Starting WGAN training...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        permutation = torch.randperm(real_data.size(0))\n",
        "        for i in range(0, real_data.size(0), batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            real_batch = real_data[indices].to(device)\n",
        "\n",
        "            for _ in range(critic_iterations):\n",
        "                discriminator.zero_grad()\n",
        "\n",
        "                with cudnn.flags(enabled=False):\n",
        "                    d_real = discriminator(real_batch)\n",
        "                    z = torch.randn(real_batch.size(0), latent_dim).to(device)\n",
        "                    fake_batch = generator(z).detach()\n",
        "                    d_fake = discriminator(fake_batch)\n",
        "                    loss_d_wasserstein = d_fake.mean() - d_real.mean()\n",
        "\n",
        "                    alpha = torch.rand(real_batch.size(0), 1, 1).to(device)\n",
        "                    interpolates = (alpha * real_batch + ((1 - alpha) * fake_batch)).requires_grad_(True)\n",
        "                    d_interpolates = discriminator(interpolates)\n",
        "\n",
        "                gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates,\n",
        "                                                 grad_outputs=torch.ones_like(d_interpolates),\n",
        "                                                 create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "                gradients = gradients.view(gradients.size(0), -1)\n",
        "                gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp\n",
        "                loss_d = loss_d_wasserstein + gradient_penalty\n",
        "                loss_d.backward()\n",
        "                optimizer_d.step()\n",
        "\n",
        "            if i % critic_iterations == 0:\n",
        "                generator.zero_grad()\n",
        "                z = torch.randn(batch_size, latent_dim).to(device)\n",
        "                fake_batch = generator(z)\n",
        "                d_fake = discriminator(fake_batch)\n",
        "                loss_g = -d_fake.mean()\n",
        "                loss_g.backward()\n",
        "                optimizer_g.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], D_loss: {loss_d.item():.4f}, G_loss: {loss_g.item():.4f}\")\n",
        "\n",
        "    print(\"WGAN training finished.\")\n",
        "    return generator, discriminator\n",
        "\n",
        "def generate_wgan_synthetic_data(generator, num_samples, latent_dim, seq_length, output_dim, device, scaler):\n",
        "    generator.eval()\n",
        "    synthetic_data_list = []\n",
        "    with torch.no_grad():\n",
        "        num_batches = (num_samples + 64 - 1) // 64\n",
        "        for _ in range(num_batches):\n",
        "            z = torch.randn(min(num_samples, 64), latent_dim).to(device)\n",
        "            fake_sequences_scaled = generator(z).cpu().numpy()\n",
        "            synthetic_data_list.append(fake_sequences_scaled.reshape(-1, output_dim))\n",
        "\n",
        "    synthetic_data_scaled = np.concatenate(synthetic_data_list, axis=0)[:num_samples]\n",
        "    synthetic_data_original_scale = scaler.inverse_transform(synthetic_data_scaled)\n",
        "    return synthetic_data_original_scale\n",
        "\n",
        "def smote_time_series(data, n_synthetic_samples, seq_length, k_neighbors=5):\n",
        "    num_sequences, seq_length, num_features = data.shape\n",
        "    if num_sequences < k_neighbors:\n",
        "        print(f\"Warning: Number of sequences ({num_sequences}) is less than k_neighbors ({k_neighbors}). Reducing k_neighbors.\")\n",
        "        k_neighbors = max(1, num_sequences - 1)\n",
        "        if k_neighbors == 0:\n",
        "            print(\"Error: Not enough data to generate synthetic samples with k_neighbors=1.\")\n",
        "            return np.array([])\n",
        "\n",
        "    data_flat = data.reshape(num_sequences, -1)\n",
        "    nn = NearestNeighbors(n_neighbors=k_neighbors + 1, algorithm='auto').fit(data_flat)\n",
        "    distances, indices = nn.kneighbors(data_flat)\n",
        "\n",
        "    synthetic_data_flat = []\n",
        "\n",
        "    for i in range(n_synthetic_samples):\n",
        "        anchor_idx = np.random.randint(0, num_sequences)\n",
        "        anchor_sequence_flat = data_flat[anchor_idx]\n",
        "\n",
        "        if k_neighbors < 1:\n",
        "            neighbor_sequence_flat = anchor_sequence_flat\n",
        "            alpha = 0.0\n",
        "        else:\n",
        "            neighbor_idx = indices[anchor_idx, np.random.randint(1, k_neighbors + 1)]\n",
        "            neighbor_sequence_flat = data_flat[neighbor_idx]\n",
        "            alpha = np.random.rand()\n",
        "\n",
        "        synthetic_sequence_flat = anchor_sequence_flat + alpha * (neighbor_sequence_flat - anchor_sequence_flat)\n",
        "        synthetic_data_flat.append(synthetic_sequence_flat)\n",
        "\n",
        "    synthetic_data_flat = np.array(synthetic_data_flat)\n",
        "    synthetic_data = synthetic_data_flat.reshape(n_synthetic_samples, seq_length, num_features)\n",
        "\n",
        "    return synthetic_data\n",
        "\n",
        "# Rest of your main execution block for WGAN and SMOTE-TS would go here,\n",
        "# using the corrected train_lstm_model function."
      ],
      "metadata": {
        "id": "Vq1ZaBWFAwc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Import Required Libraries\n",
        "# =====================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from scipy.stats import ttest_rel\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Helper Functions\n",
        "# =====================\n",
        "def get_stock_data(ticker, start, end):\n",
        "    df = yf.download(ticker, start=start, end=end)\n",
        "    df = df[['Close']].dropna()\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "def generate_synthetic_data(real_data, noise_level=0.05):\n",
        "    clean_data = real_data.dropna().reset_index(drop=True)\n",
        "    noise = np.random.normal(loc=0.0, scale=noise_level, size=clean_data.shape)\n",
        "    synthetic = clean_data.values * (1 + noise)\n",
        "    synthetic_df = pd.DataFrame(synthetic, columns=clean_data.columns)\n",
        "    synthetic_df = synthetic_df.fillna(method='ffill').fillna(method='bfill')\n",
        "    return synthetic_df\n",
        "\n",
        "def scale_data(data):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled = scaler.fit_transform(data.values.reshape(-1, 1) if data.ndim == 2 and data.shape[1] == 1 else data.values)\n",
        "    return scaled, scaler\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def split_data(X, y, train_ratio=0.8):\n",
        "    split_idx = int(len(X) * train_ratio)\n",
        "    X_train = X[:split_idx]\n",
        "    X_test = X[split_idx:]\n",
        "    y_train = y[:split_idx]\n",
        "    y_test = y[split_idx:]\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    y_true_flat = y_true.flatten() if y_true.ndim > 1 else y_true\n",
        "    y_pred_flat = y_pred.flatten() if y_pred.ndim > 1 else y_pred\n",
        "    mse = mean_squared_error(y_true_flat, y_pred_flat)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true_flat, y_pred_flat)\n",
        "    if len(y_true_flat) > 1 and len(y_pred_flat) > 1:\n",
        "        y_true_diff = np.diff(y_true_flat)\n",
        "        y_pred_diff = np.diff(y_pred_flat)\n",
        "        correct_direction = np.sum(np.sign(y_true_diff) == np.sign(y_pred_diff))\n",
        "        directional_accuracy = correct_direction / len(y_true_diff) if len(y_true_diff) > 0 else np.nan\n",
        "    else:\n",
        "        directional_accuracy = np.nan\n",
        "    return {\n",
        "        \"MSE\": mse,\n",
        "        \"RMSE\": rmse,\n",
        "        \"R2\": r2,\n",
        "        \"Directional Accuracy\": directional_accuracy\n",
        "    }\n",
        "\n",
        "def compare_models(metric_real, metric_synthetic, metric_name=\"MSE\"):\n",
        "    print(f\"Comparing {metric_name} between Real-trained vs Synthetic-trained:\")\n",
        "    stat, p = None, None\n",
        "    if len(metric_real) > 1 and len(metric_synthetic) > 1 and len(metric_real) == len(metric_synthetic):\n",
        "        stat, p = ttest_rel(metric_real, metric_synthetic)\n",
        "        print(f\"Paired T-test: stat={stat:.4f}, p={p:.4f}\")\n",
        "    elif len(metric_real) == 1 and len(metric_synthetic) == 1:\n",
        "        print(f\"Single trial comparison - Real-trained {metric_name}: {metric_real[0]:.4f}, Synthetic-trained {metric_name}: {metric_synthetic[0]:.4f}\")\n",
        "    else:\n",
        "        print(\"Cannot perform statistical test: Mismatched or insufficient data for comparison.\")\n",
        "    return stat, p\n",
        "\n",
        "\n",
        "#LSTMModel\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# =====================\n",
        "# Train function with early stopping\n",
        "# =====================\n",
        "def train_model_with_early_stopping(model, X_full, y_full, model_save_path, epochs=100, patience=10):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Use TensorDataset and DataLoader for handling the split and batching\n",
        "    dataset = TensorDataset(X_full, y_full)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    print(f\"Starting training with patience: {patience}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            outputs = model(X_batch)\n",
        "            optimizer.zero_grad()\n",
        "            train_loss = loss_function(outputs, y_batch)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += train_loss.item()\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch_val, y_batch_val in val_loader:\n",
        "                val_outputs = model(X_batch_val)\n",
        "                val_loss = loss_function(val_outputs, y_batch_val)\n",
        "                total_val_loss += val_loss.item()\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Validation loss improved. Model saved to {model_save_path}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == patience:\n",
        "                print(f\"Early stopping triggered after {epochs_no_improve} epochs without improvement.\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(model_save_path))\n",
        "    print(f'Final model loaded from {model_save_path} (best performing model during training).')\n",
        "\n",
        "# =====================\n",
        "# Main execution with trials\n",
        "# =====================\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "seq_length = 10\n",
        "\n",
        "# --- Load and prepare Real Data ---\n",
        "real_data = get_stock_data(\"AAPL\", \"2020-01-01\", \"2023-01-01\")\n",
        "scaled_real_data, real_scaler = scale_data(real_data)\n",
        "X_real, y_real = create_sequences(scaled_real_data, seq_length)\n",
        "X_train_real = torch.tensor(X_real, dtype=torch.float32).to(device)\n",
        "y_train_real = torch.tensor(y_real, dtype=torch.float32).to(device)\n",
        "\n",
        "# --- Load and prepare Synthetic Data ---\n",
        "synthetic_data = generate_synthetic_data(real_data)\n",
        "scaled_synthetic_data, synthetic_scaler = scale_data(synthetic_data)\n",
        "X_synthetic, y_synthetic = create_sequences(scaled_synthetic_data, seq_length)\n",
        "X_train_synthetic = torch.tensor(X_synthetic, dtype=torch.float32).to(device)\n",
        "y_train_synthetic = torch.tensor(y_synthetic, dtype=torch.float32).to(device)\n",
        "\n",
        "# --- Real Data Training with Trials and Early Stopping ---\n",
        "num_trials = 15\n",
        "epochs_per_trial = 50\n",
        "patience_value = 10\n",
        "model_real_path = 'lstm_model_real_best.pth'\n",
        "\n",
        "input_size = 1\n",
        "hidden_size = 50\n",
        "num_layers = 2\n",
        "dropout_rate = 0.2\n",
        "model_real = LSTMModel(input_size, hidden_size, num_layers, dropout_rate).to(device)\n",
        "\n",
        "for trial in range(1, num_trials + 1):\n",
        "    print(f\"\\n--- Starting REAL data training for Trial {trial} ---\")\n",
        "    if trial > 1 and os.path.exists(model_real_path):\n",
        "        model_real.load_state_dict(torch.load(model_real_path))\n",
        "        print(f\"Loaded model state from {model_real_path} to continue training.\")\n",
        "\n",
        "    train_model_with_early_stopping(\n",
        "        model=model_real,\n",
        "        X_full=X_train_real,\n",
        "        y_full=y_train_real,\n",
        "        model_save_path=model_real_path,\n",
        "        epochs=epochs_per_trial,\n",
        "        patience=patience_value\n",
        "    )\n",
        "    print(f\"--- Completed REAL data training for Trial {trial} ---\")\n",
        "\n",
        "# --- Synthetic Data Training with Trials and Early Stopping ---\n",
        "model_synthetic = LSTMModel(input_size, hidden_size, num_layers, dropout_rate).to(device)\n",
        "model_synthetic_path = 'lstm_model_synthetic_best.pth'\n",
        "\n",
        "for trial in range(1, num_trials + 1):\n",
        "    print(f\"\\n--- Starting SYNTHETIC data training for Trial {trial} ---\")\n",
        "    if trial > 1 and os.path.exists(model_synthetic_path):\n",
        "        model_synthetic.load_state_dict(torch.load(model_synthetic_path))\n",
        "        print(f\"Loaded model state from {model_synthetic_path} to continue training.\")\n",
        "\n",
        "    train_model_with_early_stopping(\n",
        "        model=model_synthetic,\n",
        "        X_full=X_train_synthetic,\n",
        "        y_full=y_train_synthetic,\n",
        "        model_save_path=model_synthetic_path,\n",
        "        epochs=epochs_per_trial,\n",
        "        patience=patience_value\n",
        "    )\n",
        "    print(f\"--- Completed SYNTHETIC data training for Trial {trial} ---\")\n",
        "\n",
        "# --- Final Evaluation and Comparison ---\n",
        "print(\"\\n--- Final Evaluation and Comparison of Best Models ---\")\n",
        "# Re-evaluate the best real-trained model on the test data\n",
        "model_real.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_real = X_real[int(len(X_real) * 0.8):]\n",
        "    y_test_real = y_real[int(len(y_real) * 0.8):]\n",
        "    X_test_real_tensor = torch.tensor(X_test_real, dtype=torch.float32).to(device)\n",
        "    y_test_real_tensor = torch.tensor(y_test_real, dtype=torch.float32).to(device)\n",
        "    real_predictions_scaled = model_real(X_test_real_tensor).cpu().numpy()\n",
        "    y_test_real_rescaled = real_scaler.inverse_transform(y_test_real_tensor.cpu().numpy().reshape(-1, 1))\n",
        "    real_predictions_rescaled = real_scaler.inverse_transform(real_predictions_scaled)\n",
        "    final_real_metrics = evaluate_model(y_test_real_rescaled, real_predictions_rescaled)\n",
        "    print(\"Final Real-trained Model Metrics:\", final_real_metrics)\n",
        "\n",
        "# Re-evaluate the best synthetic-trained model on the test data\n",
        "model_synthetic.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_synthetic = X_synthetic[int(len(X_synthetic) * 0.8):]\n",
        "    y_test_synthetic = y_synthetic[int(len(y_synthetic) * 0.8):]\n",
        "    X_test_synthetic_tensor = torch.tensor(X_test_synthetic, dtype=torch.float32).to(device)\n",
        "    y_test_synthetic_tensor = torch.tensor(y_test_synthetic, dtype=torch.float32).to(device)\n",
        "    synthetic_predictions_scaled = model_synthetic(X_test_synthetic_tensor).cpu().numpy()\n",
        "    y_test_synthetic_rescaled = synthetic_scaler.inverse_transform(y_test_synthetic_tensor.cpu().numpy().reshape(-1, 1))\n",
        "    synthetic_predictions_rescaled = synthetic_scaler.inverse_transform(synthetic_predictions_scaled)\n",
        "    final_synthetic_metrics = evaluate_model(y_test_synthetic_rescaled, synthetic_predictions_rescaled)\n",
        "    print(\"Final Synthetic-trained Model Metrics:\", final_synthetic_metrics)\n",
        "\n",
        "# Final comparison\n",
        "compare_models([final_real_metrics['MSE']], [final_synthetic_metrics['MSE']], metric_name=\"MSE\")"
      ],
      "metadata": {
        "id": "45IbMpHGAz4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af92b1fb-cd73-4e99-9d2a-71dcad855dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-599011271.py:25: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=start, end=end)\n",
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-599011271.py:35: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  synthetic_df = synthetic_df.fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting REAL data training for Trial 1 ---\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.2750, Val Loss: 0.0555\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0530, Val Loss: 0.0465\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0343, Val Loss: 0.0309\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0214, Val Loss: 0.0148\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0077, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0045, Val Loss: 0.0036\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0039, Val Loss: 0.0034\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0035, Val Loss: 0.0028\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [9/50], Train Loss: 0.0031, Val Loss: 0.0026\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [10/50], Train Loss: 0.0027, Val Loss: 0.0026\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [11/50], Train Loss: 0.0026, Val Loss: 0.0022\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [12/50], Train Loss: 0.0024, Val Loss: 0.0023\n",
            "Epoch [13/50], Train Loss: 0.0025, Val Loss: 0.0021\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0024, Val Loss: 0.0022\n",
            "Epoch [15/50], Train Loss: 0.0022, Val Loss: 0.0020\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [16/50], Train Loss: 0.0023, Val Loss: 0.0021\n",
            "Epoch [17/50], Train Loss: 0.0025, Val Loss: 0.0020\n",
            "Epoch [18/50], Train Loss: 0.0021, Val Loss: 0.0019\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [19/50], Train Loss: 0.0023, Val Loss: 0.0021\n",
            "Epoch [20/50], Train Loss: 0.0023, Val Loss: 0.0019\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [21/50], Train Loss: 0.0021, Val Loss: 0.0020\n",
            "Epoch [22/50], Train Loss: 0.0022, Val Loss: 0.0021\n",
            "Epoch [23/50], Train Loss: 0.0021, Val Loss: 0.0028\n",
            "Epoch [24/50], Train Loss: 0.0024, Val Loss: 0.0019\n",
            "Epoch [25/50], Train Loss: 0.0023, Val Loss: 0.0027\n",
            "Epoch [26/50], Train Loss: 0.0024, Val Loss: 0.0020\n",
            "Epoch [27/50], Train Loss: 0.0022, Val Loss: 0.0018\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [28/50], Train Loss: 0.0021, Val Loss: 0.0020\n",
            "Epoch [29/50], Train Loss: 0.0021, Val Loss: 0.0018\n",
            "Epoch [30/50], Train Loss: 0.0020, Val Loss: 0.0018\n",
            "Epoch [31/50], Train Loss: 0.0020, Val Loss: 0.0018\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [32/50], Train Loss: 0.0021, Val Loss: 0.0021\n",
            "Epoch [33/50], Train Loss: 0.0022, Val Loss: 0.0018\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [34/50], Train Loss: 0.0020, Val Loss: 0.0019\n",
            "Epoch [35/50], Train Loss: 0.0021, Val Loss: 0.0018\n",
            "Epoch [36/50], Train Loss: 0.0019, Val Loss: 0.0018\n",
            "Epoch [37/50], Train Loss: 0.0019, Val Loss: 0.0019\n",
            "Epoch [38/50], Train Loss: 0.0020, Val Loss: 0.0017\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [39/50], Train Loss: 0.0018, Val Loss: 0.0017\n",
            "Epoch [40/50], Train Loss: 0.0019, Val Loss: 0.0017\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [41/50], Train Loss: 0.0018, Val Loss: 0.0017\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [42/50], Train Loss: 0.0018, Val Loss: 0.0017\n",
            "Epoch [43/50], Train Loss: 0.0018, Val Loss: 0.0017\n",
            "Epoch [44/50], Train Loss: 0.0018, Val Loss: 0.0017\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [45/50], Train Loss: 0.0018, Val Loss: 0.0017\n",
            "Epoch [46/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
            "Epoch [47/50], Train Loss: 0.0021, Val Loss: 0.0024\n",
            "Epoch [48/50], Train Loss: 0.0021, Val Loss: 0.0020\n",
            "Epoch [49/50], Train Loss: 0.0018, Val Loss: 0.0017\n",
            "Epoch [50/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 1 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 2 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0024, Val Loss: 0.0014\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0017, Val Loss: 0.0017\n",
            "Epoch [3/50], Train Loss: 0.0018, Val Loss: 0.0016\n",
            "Epoch [4/50], Train Loss: 0.0020, Val Loss: 0.0027\n",
            "Epoch [5/50], Train Loss: 0.0024, Val Loss: 0.0017\n",
            "Epoch [6/50], Train Loss: 0.0021, Val Loss: 0.0016\n",
            "Epoch [7/50], Train Loss: 0.0018, Val Loss: 0.0015\n",
            "Epoch [8/50], Train Loss: 0.0015, Val Loss: 0.0014\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [9/50], Train Loss: 0.0015, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [10/50], Train Loss: 0.0015, Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [11/50], Train Loss: 0.0016, Val Loss: 0.0013\n",
            "Epoch [12/50], Train Loss: 0.0017, Val Loss: 0.0013\n",
            "Epoch [13/50], Train Loss: 0.0015, Val Loss: 0.0017\n",
            "Epoch [14/50], Train Loss: 0.0016, Val Loss: 0.0013\n",
            "Epoch [15/50], Train Loss: 0.0016, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [16/50], Train Loss: 0.0013, Val Loss: 0.0011\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [17/50], Train Loss: 0.0017, Val Loss: 0.0022\n",
            "Epoch [18/50], Train Loss: 0.0014, Val Loss: 0.0011\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [19/50], Train Loss: 0.0014, Val Loss: 0.0012\n",
            "Epoch [20/50], Train Loss: 0.0014, Val Loss: 0.0011\n",
            "Epoch [21/50], Train Loss: 0.0014, Val Loss: 0.0011\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [22/50], Train Loss: 0.0014, Val Loss: 0.0014\n",
            "Epoch [23/50], Train Loss: 0.0014, Val Loss: 0.0015\n",
            "Epoch [24/50], Train Loss: 0.0014, Val Loss: 0.0010\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [25/50], Train Loss: 0.0013, Val Loss: 0.0010\n",
            "Epoch [26/50], Train Loss: 0.0014, Val Loss: 0.0010\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [27/50], Train Loss: 0.0014, Val Loss: 0.0014\n",
            "Epoch [28/50], Train Loss: 0.0018, Val Loss: 0.0011\n",
            "Epoch [29/50], Train Loss: 0.0013, Val Loss: 0.0010\n",
            "Epoch [30/50], Train Loss: 0.0013, Val Loss: 0.0010\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [31/50], Train Loss: 0.0012, Val Loss: 0.0009\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [32/50], Train Loss: 0.0012, Val Loss: 0.0010\n",
            "Epoch [33/50], Train Loss: 0.0014, Val Loss: 0.0010\n",
            "Epoch [34/50], Train Loss: 0.0014, Val Loss: 0.0010\n",
            "Epoch [35/50], Train Loss: 0.0013, Val Loss: 0.0009\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [36/50], Train Loss: 0.0013, Val Loss: 0.0012\n",
            "Epoch [37/50], Train Loss: 0.0016, Val Loss: 0.0009\n",
            "Epoch [38/50], Train Loss: 0.0013, Val Loss: 0.0009\n",
            "Epoch [39/50], Train Loss: 0.0011, Val Loss: 0.0010\n",
            "Epoch [40/50], Train Loss: 0.0013, Val Loss: 0.0011\n",
            "Epoch [41/50], Train Loss: 0.0011, Val Loss: 0.0009\n",
            "Epoch [42/50], Train Loss: 0.0011, Val Loss: 0.0009\n",
            "Epoch [43/50], Train Loss: 0.0011, Val Loss: 0.0008\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [44/50], Train Loss: 0.0012, Val Loss: 0.0010\n",
            "Epoch [45/50], Train Loss: 0.0012, Val Loss: 0.0009\n",
            "Epoch [46/50], Train Loss: 0.0011, Val Loss: 0.0013\n",
            "Epoch [47/50], Train Loss: 0.0013, Val Loss: 0.0009\n",
            "Epoch [48/50], Train Loss: 0.0011, Val Loss: 0.0015\n",
            "Epoch [49/50], Train Loss: 0.0012, Val Loss: 0.0008\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [50/50], Train Loss: 0.0011, Val Loss: 0.0008\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 2 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 3 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0013, Val Loss: 0.0007\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0011, Val Loss: 0.0007\n",
            "Epoch [3/50], Train Loss: 0.0011, Val Loss: 0.0008\n",
            "Epoch [4/50], Train Loss: 0.0010, Val Loss: 0.0007\n",
            "Epoch [5/50], Train Loss: 0.0011, Val Loss: 0.0010\n",
            "Epoch [6/50], Train Loss: 0.0012, Val Loss: 0.0012\n",
            "Epoch [7/50], Train Loss: 0.0011, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0010, Val Loss: 0.0008\n",
            "Epoch [9/50], Train Loss: 0.0012, Val Loss: 0.0007\n",
            "Epoch [10/50], Train Loss: 0.0011, Val Loss: 0.0008\n",
            "Epoch [11/50], Train Loss: 0.0011, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [12/50], Train Loss: 0.0010, Val Loss: 0.0007\n",
            "Epoch [13/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
            "Epoch [14/50], Train Loss: 0.0012, Val Loss: 0.0007\n",
            "Epoch [15/50], Train Loss: 0.0011, Val Loss: 0.0006\n",
            "Epoch [16/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [17/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
            "Epoch [18/50], Train Loss: 0.0011, Val Loss: 0.0007\n",
            "Epoch [19/50], Train Loss: 0.0010, Val Loss: 0.0006\n",
            "Epoch [20/50], Train Loss: 0.0010, Val Loss: 0.0006\n",
            "Epoch [21/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
            "Epoch [22/50], Train Loss: 0.0008, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [23/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
            "Epoch [24/50], Train Loss: 0.0009, Val Loss: 0.0007\n",
            "Epoch [25/50], Train Loss: 0.0009, Val Loss: 0.0007\n",
            "Epoch [26/50], Train Loss: 0.0009, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [27/50], Train Loss: 0.0010, Val Loss: 0.0009\n",
            "Epoch [28/50], Train Loss: 0.0011, Val Loss: 0.0006\n",
            "Epoch [29/50], Train Loss: 0.0009, Val Loss: 0.0007\n",
            "Epoch [30/50], Train Loss: 0.0010, Val Loss: 0.0008\n",
            "Epoch [31/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
            "Epoch [32/50], Train Loss: 0.0009, Val Loss: 0.0007\n",
            "Epoch [33/50], Train Loss: 0.0009, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [34/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
            "Epoch [35/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Epoch [36/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
            "Epoch [37/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Epoch [38/50], Train Loss: 0.0008, Val Loss: 0.0007\n",
            "Epoch [39/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Epoch [40/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
            "Epoch [41/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
            "Epoch [42/50], Train Loss: 0.0009, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [43/50], Train Loss: 0.0009, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [44/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Epoch [45/50], Train Loss: 0.0008, Val Loss: 0.0006\n",
            "Epoch [46/50], Train Loss: 0.0009, Val Loss: 0.0005\n",
            "Epoch [47/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Epoch [48/50], Train Loss: 0.0008, Val Loss: 0.0006\n",
            "Epoch [49/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [50/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 3 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 4 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0015, Val Loss: 0.0011\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0008, Val Loss: 0.0008\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0007, Val Loss: 0.0008\n",
            "Epoch [4/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [6/50], Train Loss: 0.0007, Val Loss: 0.0009\n",
            "Epoch [7/50], Train Loss: 0.0007, Val Loss: 0.0008\n",
            "Epoch [8/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [9/50], Train Loss: 0.0006, Val Loss: 0.0008\n",
            "Epoch [10/50], Train Loss: 0.0007, Val Loss: 0.0008\n",
            "Epoch [11/50], Train Loss: 0.0008, Val Loss: 0.0008\n",
            "Epoch [12/50], Train Loss: 0.0007, Val Loss: 0.0008\n",
            "Epoch [13/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [15/50], Train Loss: 0.0008, Val Loss: 0.0007\n",
            "Epoch [16/50], Train Loss: 0.0007, Val Loss: 0.0008\n",
            "Epoch [17/50], Train Loss: 0.0007, Val Loss: 0.0010\n",
            "Epoch [18/50], Train Loss: 0.0007, Val Loss: 0.0008\n",
            "Epoch [19/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [20/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [21/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [22/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [23/50], Train Loss: 0.0007, Val Loss: 0.0009\n",
            "Epoch [24/50], Train Loss: 0.0008, Val Loss: 0.0007\n",
            "Epoch [25/50], Train Loss: 0.0006, Val Loss: 0.0008\n",
            "Epoch [26/50], Train Loss: 0.0006, Val Loss: 0.0007\n",
            "Epoch [27/50], Train Loss: 0.0007, Val Loss: 0.0011\n",
            "Epoch [28/50], Train Loss: 0.0009, Val Loss: 0.0011\n",
            "Epoch [29/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 4 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 5 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0013, Val Loss: 0.0012\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0010, Val Loss: 0.0008\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [7/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [9/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [10/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [11/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [12/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [13/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [14/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [15/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [16/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [17/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [18/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [19/50], Train Loss: 0.0008, Val Loss: 0.0006\n",
            "Epoch [20/50], Train Loss: 0.0007, Val Loss: 0.0009\n",
            "Epoch [21/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [22/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [23/50], Train Loss: 0.0008, Val Loss: 0.0006\n",
            "Epoch [24/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [25/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [26/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [27/50], Train Loss: 0.0007, Val Loss: 0.0008\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 5 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 6 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0014, Val Loss: 0.0008\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0008, Val Loss: 0.0007\n",
            "Epoch [4/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [8/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [9/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [10/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [11/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [12/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [13/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [14/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [15/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [16/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [17/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [18/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [19/50], Train Loss: 0.0008, Val Loss: 0.0006\n",
            "Epoch [20/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [21/50], Train Loss: 0.0007, Val Loss: 0.0009\n",
            "Epoch [22/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [23/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [24/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [25/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [26/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [27/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [28/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [29/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Epoch [30/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [31/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [32/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [33/50], Train Loss: 0.0008, Val Loss: 0.0007\n",
            "Epoch [34/50], Train Loss: 0.0007, Val Loss: 0.0011\n",
            "Epoch [35/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [36/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [37/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [38/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [39/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [40/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [41/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 6 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 7 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0013, Val Loss: 0.0004\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Epoch [3/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [4/50], Train Loss: 0.0007, Val Loss: 0.0008\n",
            "Epoch [5/50], Train Loss: 0.0007, Val Loss: 0.0004\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0006, Val Loss: 0.0004\n",
            "Epoch [7/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [8/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [9/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [10/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Epoch [11/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [12/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [13/50], Train Loss: 0.0007, Val Loss: 0.0004\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/50], Train Loss: 0.0006, Val Loss: 0.0004\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [15/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [16/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [17/50], Train Loss: 0.0007, Val Loss: 0.0004\n",
            "Epoch [18/50], Train Loss: 0.0007, Val Loss: 0.0004\n",
            "Epoch [19/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [20/50], Train Loss: 0.0007, Val Loss: 0.0004\n",
            "Epoch [21/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [22/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [23/50], Train Loss: 0.0006, Val Loss: 0.0004\n",
            "Epoch [24/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 7 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 8 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0011, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [3/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [5/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [6/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [7/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [8/50], Train Loss: 0.0008, Val Loss: 0.0007\n",
            "Epoch [9/50], Train Loss: 0.0007, Val Loss: 0.0007\n",
            "Epoch [10/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [11/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [12/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [13/50], Train Loss: 0.0006, Val Loss: 0.0007\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 8 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 9 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0010, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [3/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [6/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [7/50], Train Loss: 0.0008, Val Loss: 0.0007\n",
            "Epoch [8/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [9/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [10/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [11/50], Train Loss: 0.0006, Val Loss: 0.0008\n",
            "Epoch [12/50], Train Loss: 0.0008, Val Loss: 0.0010\n",
            "Epoch [13/50], Train Loss: 0.0008, Val Loss: 0.0006\n",
            "Epoch [14/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 9 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 10 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0014, Val Loss: 0.0008\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0008, Val Loss: 0.0010\n",
            "Epoch [3/50], Train Loss: 0.0008, Val Loss: 0.0007\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0006, Val Loss: 0.0007\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0006, Val Loss: 0.0007\n",
            "Epoch [7/50], Train Loss: 0.0007, Val Loss: 0.0010\n",
            "Epoch [8/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [9/50], Train Loss: 0.0006, Val Loss: 0.0007\n",
            "Epoch [10/50], Train Loss: 0.0006, Val Loss: 0.0007\n",
            "Epoch [11/50], Train Loss: 0.0006, Val Loss: 0.0007\n",
            "Epoch [12/50], Train Loss: 0.0006, Val Loss: 0.0008\n",
            "Epoch [13/50], Train Loss: 0.0006, Val Loss: 0.0008\n",
            "Epoch [14/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [15/50], Train Loss: 0.0006, Val Loss: 0.0011\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 10 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 11 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0011, Val Loss: 0.0008\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0008, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [6/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [7/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [8/50], Train Loss: 0.0006, Val Loss: 0.0008\n",
            "Epoch [9/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [10/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [11/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [12/50], Train Loss: 0.0008, Val Loss: 0.0007\n",
            "Epoch [13/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [14/50], Train Loss: 0.0008, Val Loss: 0.0006\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 11 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 12 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0011, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [5/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [7/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [8/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [9/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [10/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [11/50], Train Loss: 0.0006, Val Loss: 0.0007\n",
            "Epoch [12/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [13/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [14/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [15/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 12 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 13 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0014, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [4/50], Train Loss: 0.0006, Val Loss: 0.0007\n",
            "Epoch [5/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [8/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [9/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [10/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [11/50], Train Loss: 0.0007, Val Loss: 0.0008\n",
            "Epoch [12/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [13/50], Train Loss: 0.0008, Val Loss: 0.0006\n",
            "Epoch [14/50], Train Loss: 0.0008, Val Loss: 0.0006\n",
            "Epoch [15/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [16/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [17/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [18/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [19/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 13 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 14 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [4/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [5/50], Train Loss: 0.0008, Val Loss: 0.0007\n",
            "Epoch [6/50], Train Loss: 0.0008, Val Loss: 0.0005\n",
            "Epoch [7/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [8/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [9/50], Train Loss: 0.0008, Val Loss: 0.0007\n",
            "Epoch [10/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [11/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [12/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 14 ---\n",
            "\n",
            "--- Starting REAL data training for Trial 15 ---\n",
            "Loaded model state from lstm_model_real_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0010, Val Loss: 0.0005\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [3/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [4/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [5/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [6/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Epoch [7/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [8/50], Train Loss: 0.0007, Val Loss: 0.0006\n",
            "Epoch [9/50], Train Loss: 0.0006, Val Loss: 0.0005\n",
            "Epoch [10/50], Train Loss: 0.0007, Val Loss: 0.0005\n",
            "Epoch [11/50], Train Loss: 0.0006, Val Loss: 0.0006\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_real_best.pth (best performing model during training).\n",
            "--- Completed REAL data training for Trial 15 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 1 ---\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.1214, Val Loss: 0.0502\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0483, Val Loss: 0.0441\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0326, Val Loss: 0.0249\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0147, Val Loss: 0.0103\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0083, Val Loss: 0.0075\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0069, Val Loss: 0.0065\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0056, Val Loss: 0.0049\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0048, Val Loss: 0.0049\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [9/50], Train Loss: 0.0050, Val Loss: 0.0047\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [10/50], Train Loss: 0.0049, Val Loss: 0.0055\n",
            "Epoch [11/50], Train Loss: 0.0054, Val Loss: 0.0050\n",
            "Epoch [12/50], Train Loss: 0.0049, Val Loss: 0.0046\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [13/50], Train Loss: 0.0048, Val Loss: 0.0048\n",
            "Epoch [14/50], Train Loss: 0.0048, Val Loss: 0.0048\n",
            "Epoch [15/50], Train Loss: 0.0048, Val Loss: 0.0046\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [16/50], Train Loss: 0.0049, Val Loss: 0.0047\n",
            "Epoch [17/50], Train Loss: 0.0050, Val Loss: 0.0047\n",
            "Epoch [18/50], Train Loss: 0.0047, Val Loss: 0.0045\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [19/50], Train Loss: 0.0046, Val Loss: 0.0048\n",
            "Epoch [20/50], Train Loss: 0.0047, Val Loss: 0.0045\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [21/50], Train Loss: 0.0046, Val Loss: 0.0045\n",
            "Epoch [22/50], Train Loss: 0.0048, Val Loss: 0.0048\n",
            "Epoch [23/50], Train Loss: 0.0045, Val Loss: 0.0046\n",
            "Epoch [24/50], Train Loss: 0.0046, Val Loss: 0.0046\n",
            "Epoch [25/50], Train Loss: 0.0046, Val Loss: 0.0044\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [26/50], Train Loss: 0.0048, Val Loss: 0.0046\n",
            "Epoch [27/50], Train Loss: 0.0046, Val Loss: 0.0052\n",
            "Epoch [28/50], Train Loss: 0.0046, Val Loss: 0.0045\n",
            "Epoch [29/50], Train Loss: 0.0045, Val Loss: 0.0046\n",
            "Epoch [30/50], Train Loss: 0.0045, Val Loss: 0.0054\n",
            "Epoch [31/50], Train Loss: 0.0048, Val Loss: 0.0049\n",
            "Epoch [32/50], Train Loss: 0.0047, Val Loss: 0.0046\n",
            "Epoch [33/50], Train Loss: 0.0045, Val Loss: 0.0044\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [34/50], Train Loss: 0.0045, Val Loss: 0.0044\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [35/50], Train Loss: 0.0046, Val Loss: 0.0045\n",
            "Epoch [36/50], Train Loss: 0.0045, Val Loss: 0.0044\n",
            "Epoch [37/50], Train Loss: 0.0044, Val Loss: 0.0044\n",
            "Epoch [38/50], Train Loss: 0.0045, Val Loss: 0.0052\n",
            "Epoch [39/50], Train Loss: 0.0044, Val Loss: 0.0051\n",
            "Epoch [40/50], Train Loss: 0.0045, Val Loss: 0.0053\n",
            "Epoch [41/50], Train Loss: 0.0046, Val Loss: 0.0049\n",
            "Epoch [42/50], Train Loss: 0.0048, Val Loss: 0.0045\n",
            "Epoch [43/50], Train Loss: 0.0047, Val Loss: 0.0051\n",
            "Epoch [44/50], Train Loss: 0.0046, Val Loss: 0.0048\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 1 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 2 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0053, Val Loss: 0.0044\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0046, Val Loss: 0.0053\n",
            "Epoch [3/50], Train Loss: 0.0048, Val Loss: 0.0044\n",
            "Epoch [4/50], Train Loss: 0.0048, Val Loss: 0.0044\n",
            "Epoch [5/50], Train Loss: 0.0046, Val Loss: 0.0046\n",
            "Epoch [6/50], Train Loss: 0.0045, Val Loss: 0.0042\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0045, Val Loss: 0.0043\n",
            "Epoch [8/50], Train Loss: 0.0043, Val Loss: 0.0045\n",
            "Epoch [9/50], Train Loss: 0.0046, Val Loss: 0.0043\n",
            "Epoch [10/50], Train Loss: 0.0045, Val Loss: 0.0047\n",
            "Epoch [11/50], Train Loss: 0.0045, Val Loss: 0.0043\n",
            "Epoch [12/50], Train Loss: 0.0046, Val Loss: 0.0046\n",
            "Epoch [13/50], Train Loss: 0.0048, Val Loss: 0.0057\n",
            "Epoch [14/50], Train Loss: 0.0045, Val Loss: 0.0046\n",
            "Epoch [15/50], Train Loss: 0.0044, Val Loss: 0.0043\n",
            "Epoch [16/50], Train Loss: 0.0045, Val Loss: 0.0042\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [17/50], Train Loss: 0.0043, Val Loss: 0.0045\n",
            "Epoch [18/50], Train Loss: 0.0047, Val Loss: 0.0042\n",
            "Epoch [19/50], Train Loss: 0.0045, Val Loss: 0.0042\n",
            "Epoch [20/50], Train Loss: 0.0043, Val Loss: 0.0042\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [21/50], Train Loss: 0.0045, Val Loss: 0.0043\n",
            "Epoch [22/50], Train Loss: 0.0043, Val Loss: 0.0042\n",
            "Epoch [23/50], Train Loss: 0.0044, Val Loss: 0.0043\n",
            "Epoch [24/50], Train Loss: 0.0042, Val Loss: 0.0042\n",
            "Epoch [25/50], Train Loss: 0.0044, Val Loss: 0.0042\n",
            "Epoch [26/50], Train Loss: 0.0042, Val Loss: 0.0042\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [27/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [28/50], Train Loss: 0.0046, Val Loss: 0.0043\n",
            "Epoch [29/50], Train Loss: 0.0047, Val Loss: 0.0049\n",
            "Epoch [30/50], Train Loss: 0.0045, Val Loss: 0.0042\n",
            "Epoch [31/50], Train Loss: 0.0046, Val Loss: 0.0044\n",
            "Epoch [32/50], Train Loss: 0.0046, Val Loss: 0.0045\n",
            "Epoch [33/50], Train Loss: 0.0045, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [34/50], Train Loss: 0.0042, Val Loss: 0.0042\n",
            "Epoch [35/50], Train Loss: 0.0045, Val Loss: 0.0052\n",
            "Epoch [36/50], Train Loss: 0.0047, Val Loss: 0.0042\n",
            "Epoch [37/50], Train Loss: 0.0044, Val Loss: 0.0041\n",
            "Epoch [38/50], Train Loss: 0.0045, Val Loss: 0.0044\n",
            "Epoch [39/50], Train Loss: 0.0047, Val Loss: 0.0042\n",
            "Epoch [40/50], Train Loss: 0.0047, Val Loss: 0.0047\n",
            "Epoch [41/50], Train Loss: 0.0048, Val Loss: 0.0041\n",
            "Epoch [42/50], Train Loss: 0.0043, Val Loss: 0.0041\n",
            "Epoch [43/50], Train Loss: 0.0041, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [44/50], Train Loss: 0.0041, Val Loss: 0.0042\n",
            "Epoch [45/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [46/50], Train Loss: 0.0041, Val Loss: 0.0041\n",
            "Epoch [47/50], Train Loss: 0.0042, Val Loss: 0.0042\n",
            "Epoch [48/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Epoch [49/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Epoch [50/50], Train Loss: 0.0039, Val Loss: 0.0041\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 2 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 3 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0042, Val Loss: 0.0047\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0044, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0048, Val Loss: 0.0044\n",
            "Epoch [4/50], Train Loss: 0.0046, Val Loss: 0.0048\n",
            "Epoch [5/50], Train Loss: 0.0042, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0040, Val Loss: 0.0043\n",
            "Epoch [7/50], Train Loss: 0.0043, Val Loss: 0.0044\n",
            "Epoch [8/50], Train Loss: 0.0043, Val Loss: 0.0040\n",
            "Epoch [9/50], Train Loss: 0.0041, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [10/50], Train Loss: 0.0042, Val Loss: 0.0045\n",
            "Epoch [11/50], Train Loss: 0.0043, Val Loss: 0.0044\n",
            "Epoch [12/50], Train Loss: 0.0041, Val Loss: 0.0040\n",
            "Epoch [13/50], Train Loss: 0.0041, Val Loss: 0.0043\n",
            "Epoch [14/50], Train Loss: 0.0041, Val Loss: 0.0046\n",
            "Epoch [15/50], Train Loss: 0.0042, Val Loss: 0.0047\n",
            "Epoch [16/50], Train Loss: 0.0044, Val Loss: 0.0041\n",
            "Epoch [17/50], Train Loss: 0.0040, Val Loss: 0.0043\n",
            "Epoch [18/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Epoch [19/50], Train Loss: 0.0044, Val Loss: 0.0047\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 3 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 4 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0045, Val Loss: 0.0038\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0042, Val Loss: 0.0038\n",
            "Epoch [3/50], Train Loss: 0.0042, Val Loss: 0.0040\n",
            "Epoch [4/50], Train Loss: 0.0042, Val Loss: 0.0038\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0045, Val Loss: 0.0042\n",
            "Epoch [6/50], Train Loss: 0.0044, Val Loss: 0.0045\n",
            "Epoch [7/50], Train Loss: 0.0047, Val Loss: 0.0047\n",
            "Epoch [8/50], Train Loss: 0.0044, Val Loss: 0.0041\n",
            "Epoch [9/50], Train Loss: 0.0043, Val Loss: 0.0038\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [10/50], Train Loss: 0.0041, Val Loss: 0.0041\n",
            "Epoch [11/50], Train Loss: 0.0042, Val Loss: 0.0038\n",
            "Epoch [12/50], Train Loss: 0.0042, Val Loss: 0.0037\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [13/50], Train Loss: 0.0042, Val Loss: 0.0038\n",
            "Epoch [14/50], Train Loss: 0.0043, Val Loss: 0.0042\n",
            "Epoch [15/50], Train Loss: 0.0043, Val Loss: 0.0039\n",
            "Epoch [16/50], Train Loss: 0.0041, Val Loss: 0.0038\n",
            "Epoch [17/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Epoch [18/50], Train Loss: 0.0041, Val Loss: 0.0039\n",
            "Epoch [19/50], Train Loss: 0.0045, Val Loss: 0.0039\n",
            "Epoch [20/50], Train Loss: 0.0045, Val Loss: 0.0039\n",
            "Epoch [21/50], Train Loss: 0.0042, Val Loss: 0.0038\n",
            "Epoch [22/50], Train Loss: 0.0042, Val Loss: 0.0038\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 4 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 5 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0045, Val Loss: 0.0042\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0041, Val Loss: 0.0042\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0041, Val Loss: 0.0042\n",
            "Epoch [4/50], Train Loss: 0.0040, Val Loss: 0.0043\n",
            "Epoch [5/50], Train Loss: 0.0040, Val Loss: 0.0043\n",
            "Epoch [6/50], Train Loss: 0.0043, Val Loss: 0.0050\n",
            "Epoch [7/50], Train Loss: 0.0044, Val Loss: 0.0048\n",
            "Epoch [8/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [9/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [10/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Epoch [11/50], Train Loss: 0.0042, Val Loss: 0.0042\n",
            "Epoch [12/50], Train Loss: 0.0043, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [13/50], Train Loss: 0.0041, Val Loss: 0.0043\n",
            "Epoch [14/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Epoch [15/50], Train Loss: 0.0040, Val Loss: 0.0042\n",
            "Epoch [16/50], Train Loss: 0.0043, Val Loss: 0.0042\n",
            "Epoch [17/50], Train Loss: 0.0041, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [18/50], Train Loss: 0.0041, Val Loss: 0.0042\n",
            "Epoch [19/50], Train Loss: 0.0041, Val Loss: 0.0041\n",
            "Epoch [20/50], Train Loss: 0.0040, Val Loss: 0.0042\n",
            "Epoch [21/50], Train Loss: 0.0043, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [22/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [23/50], Train Loss: 0.0040, Val Loss: 0.0043\n",
            "Epoch [24/50], Train Loss: 0.0040, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [25/50], Train Loss: 0.0041, Val Loss: 0.0041\n",
            "Epoch [26/50], Train Loss: 0.0046, Val Loss: 0.0053\n",
            "Epoch [27/50], Train Loss: 0.0044, Val Loss: 0.0044\n",
            "Epoch [28/50], Train Loss: 0.0041, Val Loss: 0.0041\n",
            "Epoch [29/50], Train Loss: 0.0039, Val Loss: 0.0046\n",
            "Epoch [30/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Epoch [31/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [32/50], Train Loss: 0.0040, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [33/50], Train Loss: 0.0040, Val Loss: 0.0042\n",
            "Epoch [34/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [35/50], Train Loss: 0.0040, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [36/50], Train Loss: 0.0040, Val Loss: 0.0042\n",
            "Epoch [37/50], Train Loss: 0.0040, Val Loss: 0.0043\n",
            "Epoch [38/50], Train Loss: 0.0042, Val Loss: 0.0043\n",
            "Epoch [39/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [40/50], Train Loss: 0.0040, Val Loss: 0.0040\n",
            "Epoch [41/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [42/50], Train Loss: 0.0040, Val Loss: 0.0044\n",
            "Epoch [43/50], Train Loss: 0.0041, Val Loss: 0.0041\n",
            "Epoch [44/50], Train Loss: 0.0040, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [45/50], Train Loss: 0.0042, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [46/50], Train Loss: 0.0040, Val Loss: 0.0040\n",
            "Epoch [47/50], Train Loss: 0.0040, Val Loss: 0.0042\n",
            "Epoch [48/50], Train Loss: 0.0041, Val Loss: 0.0043\n",
            "Epoch [49/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Epoch [50/50], Train Loss: 0.0039, Val Loss: 0.0042\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 5 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 6 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0047, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0043, Val Loss: 0.0030\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0044, Val Loss: 0.0031\n",
            "Epoch [4/50], Train Loss: 0.0042, Val Loss: 0.0031\n",
            "Epoch [5/50], Train Loss: 0.0042, Val Loss: 0.0034\n",
            "Epoch [6/50], Train Loss: 0.0041, Val Loss: 0.0031\n",
            "Epoch [7/50], Train Loss: 0.0043, Val Loss: 0.0032\n",
            "Epoch [8/50], Train Loss: 0.0042, Val Loss: 0.0033\n",
            "Epoch [9/50], Train Loss: 0.0043, Val Loss: 0.0033\n",
            "Epoch [10/50], Train Loss: 0.0043, Val Loss: 0.0039\n",
            "Epoch [11/50], Train Loss: 0.0043, Val Loss: 0.0033\n",
            "Epoch [12/50], Train Loss: 0.0042, Val Loss: 0.0032\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 6 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 7 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0042, Val Loss: 0.0036\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0042, Val Loss: 0.0034\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0043, Val Loss: 0.0036\n",
            "Epoch [4/50], Train Loss: 0.0043, Val Loss: 0.0034\n",
            "Epoch [5/50], Train Loss: 0.0041, Val Loss: 0.0036\n",
            "Epoch [6/50], Train Loss: 0.0040, Val Loss: 0.0033\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0040, Val Loss: 0.0035\n",
            "Epoch [8/50], Train Loss: 0.0042, Val Loss: 0.0037\n",
            "Epoch [9/50], Train Loss: 0.0041, Val Loss: 0.0035\n",
            "Epoch [10/50], Train Loss: 0.0043, Val Loss: 0.0035\n",
            "Epoch [11/50], Train Loss: 0.0043, Val Loss: 0.0037\n",
            "Epoch [12/50], Train Loss: 0.0042, Val Loss: 0.0035\n",
            "Epoch [13/50], Train Loss: 0.0043, Val Loss: 0.0035\n",
            "Epoch [14/50], Train Loss: 0.0042, Val Loss: 0.0035\n",
            "Epoch [15/50], Train Loss: 0.0042, Val Loss: 0.0034\n",
            "Epoch [16/50], Train Loss: 0.0041, Val Loss: 0.0034\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 7 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 8 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0044, Val Loss: 0.0033\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0040, Val Loss: 0.0036\n",
            "Epoch [3/50], Train Loss: 0.0041, Val Loss: 0.0036\n",
            "Epoch [4/50], Train Loss: 0.0044, Val Loss: 0.0033\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0042, Val Loss: 0.0035\n",
            "Epoch [6/50], Train Loss: 0.0041, Val Loss: 0.0033\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [7/50], Train Loss: 0.0041, Val Loss: 0.0033\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0041, Val Loss: 0.0033\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [9/50], Train Loss: 0.0041, Val Loss: 0.0033\n",
            "Epoch [10/50], Train Loss: 0.0042, Val Loss: 0.0034\n",
            "Epoch [11/50], Train Loss: 0.0043, Val Loss: 0.0039\n",
            "Epoch [12/50], Train Loss: 0.0043, Val Loss: 0.0038\n",
            "Epoch [13/50], Train Loss: 0.0043, Val Loss: 0.0033\n",
            "Epoch [14/50], Train Loss: 0.0041, Val Loss: 0.0041\n",
            "Epoch [15/50], Train Loss: 0.0043, Val Loss: 0.0038\n",
            "Epoch [16/50], Train Loss: 0.0041, Val Loss: 0.0036\n",
            "Epoch [17/50], Train Loss: 0.0041, Val Loss: 0.0036\n",
            "Epoch [18/50], Train Loss: 0.0044, Val Loss: 0.0033\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 8 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 9 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0041, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [4/50], Train Loss: 0.0042, Val Loss: 0.0042\n",
            "Epoch [5/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Epoch [6/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [7/50], Train Loss: 0.0044, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [8/50], Train Loss: 0.0039, Val Loss: 0.0040\n",
            "Epoch [9/50], Train Loss: 0.0040, Val Loss: 0.0042\n",
            "Epoch [10/50], Train Loss: 0.0039, Val Loss: 0.0040\n",
            "Epoch [11/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [12/50], Train Loss: 0.0044, Val Loss: 0.0041\n",
            "Epoch [13/50], Train Loss: 0.0043, Val Loss: 0.0041\n",
            "Epoch [14/50], Train Loss: 0.0043, Val Loss: 0.0042\n",
            "Epoch [15/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [16/50], Train Loss: 0.0039, Val Loss: 0.0041\n",
            "Epoch [17/50], Train Loss: 0.0043, Val Loss: 0.0044\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 9 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 10 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0047, Val Loss: 0.0044\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0043, Val Loss: 0.0036\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0041, Val Loss: 0.0040\n",
            "Epoch [4/50], Train Loss: 0.0041, Val Loss: 0.0035\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0040, Val Loss: 0.0035\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0042, Val Loss: 0.0036\n",
            "Epoch [7/50], Train Loss: 0.0043, Val Loss: 0.0037\n",
            "Epoch [8/50], Train Loss: 0.0041, Val Loss: 0.0035\n",
            "Epoch [9/50], Train Loss: 0.0042, Val Loss: 0.0039\n",
            "Epoch [10/50], Train Loss: 0.0040, Val Loss: 0.0035\n",
            "Epoch [11/50], Train Loss: 0.0042, Val Loss: 0.0037\n",
            "Epoch [12/50], Train Loss: 0.0043, Val Loss: 0.0039\n",
            "Epoch [13/50], Train Loss: 0.0043, Val Loss: 0.0038\n",
            "Epoch [14/50], Train Loss: 0.0041, Val Loss: 0.0036\n",
            "Epoch [15/50], Train Loss: 0.0039, Val Loss: 0.0037\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 10 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 11 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0043, Val Loss: 0.0041\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0043, Val Loss: 0.0037\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0043, Val Loss: 0.0040\n",
            "Epoch [4/50], Train Loss: 0.0044, Val Loss: 0.0036\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [5/50], Train Loss: 0.0043, Val Loss: 0.0035\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [6/50], Train Loss: 0.0042, Val Loss: 0.0035\n",
            "Epoch [7/50], Train Loss: 0.0045, Val Loss: 0.0041\n",
            "Epoch [8/50], Train Loss: 0.0042, Val Loss: 0.0035\n",
            "Epoch [9/50], Train Loss: 0.0040, Val Loss: 0.0036\n",
            "Epoch [10/50], Train Loss: 0.0041, Val Loss: 0.0038\n",
            "Epoch [11/50], Train Loss: 0.0040, Val Loss: 0.0036\n",
            "Epoch [12/50], Train Loss: 0.0041, Val Loss: 0.0044\n",
            "Epoch [13/50], Train Loss: 0.0042, Val Loss: 0.0036\n",
            "Epoch [14/50], Train Loss: 0.0041, Val Loss: 0.0037\n",
            "Epoch [15/50], Train Loss: 0.0042, Val Loss: 0.0045\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 11 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 12 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0044, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0040, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0039, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0039, Val Loss: 0.0039\n",
            "Epoch [5/50], Train Loss: 0.0042, Val Loss: 0.0043\n",
            "Epoch [6/50], Train Loss: 0.0039, Val Loss: 0.0039\n",
            "Epoch [7/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [8/50], Train Loss: 0.0045, Val Loss: 0.0051\n",
            "Epoch [9/50], Train Loss: 0.0041, Val Loss: 0.0040\n",
            "Epoch [10/50], Train Loss: 0.0039, Val Loss: 0.0040\n",
            "Epoch [11/50], Train Loss: 0.0042, Val Loss: 0.0040\n",
            "Epoch [12/50], Train Loss: 0.0041, Val Loss: 0.0051\n",
            "Epoch [13/50], Train Loss: 0.0043, Val Loss: 0.0041\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 12 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 13 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0042, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0040, Val Loss: 0.0042\n",
            "Epoch [3/50], Train Loss: 0.0040, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [4/50], Train Loss: 0.0039, Val Loss: 0.0041\n",
            "Epoch [5/50], Train Loss: 0.0040, Val Loss: 0.0048\n",
            "Epoch [6/50], Train Loss: 0.0039, Val Loss: 0.0041\n",
            "Epoch [7/50], Train Loss: 0.0039, Val Loss: 0.0049\n",
            "Epoch [8/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [9/50], Train Loss: 0.0039, Val Loss: 0.0041\n",
            "Epoch [10/50], Train Loss: 0.0039, Val Loss: 0.0041\n",
            "Epoch [11/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [12/50], Train Loss: 0.0041, Val Loss: 0.0042\n",
            "Epoch [13/50], Train Loss: 0.0039, Val Loss: 0.0045\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 13 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 14 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0040, Val Loss: 0.0038\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0040, Val Loss: 0.0039\n",
            "Epoch [3/50], Train Loss: 0.0039, Val Loss: 0.0040\n",
            "Epoch [4/50], Train Loss: 0.0041, Val Loss: 0.0046\n",
            "Epoch [5/50], Train Loss: 0.0044, Val Loss: 0.0044\n",
            "Epoch [6/50], Train Loss: 0.0041, Val Loss: 0.0039\n",
            "Epoch [7/50], Train Loss: 0.0040, Val Loss: 0.0041\n",
            "Epoch [8/50], Train Loss: 0.0045, Val Loss: 0.0048\n",
            "Epoch [9/50], Train Loss: 0.0043, Val Loss: 0.0038\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [10/50], Train Loss: 0.0039, Val Loss: 0.0039\n",
            "Epoch [11/50], Train Loss: 0.0039, Val Loss: 0.0039\n",
            "Epoch [12/50], Train Loss: 0.0042, Val Loss: 0.0043\n",
            "Epoch [13/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Epoch [14/50], Train Loss: 0.0039, Val Loss: 0.0040\n",
            "Epoch [15/50], Train Loss: 0.0039, Val Loss: 0.0040\n",
            "Epoch [16/50], Train Loss: 0.0041, Val Loss: 0.0040\n",
            "Epoch [17/50], Train Loss: 0.0040, Val Loss: 0.0044\n",
            "Epoch [18/50], Train Loss: 0.0042, Val Loss: 0.0043\n",
            "Epoch [19/50], Train Loss: 0.0042, Val Loss: 0.0041\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 14 ---\n",
            "\n",
            "--- Starting SYNTHETIC data training for Trial 15 ---\n",
            "Loaded model state from lstm_model_synthetic_best.pth to continue training.\n",
            "Starting training with patience: 10\n",
            "Epoch [1/50], Train Loss: 0.0042, Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [2/50], Train Loss: 0.0040, Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_synthetic_best.pth\n",
            "Epoch [3/50], Train Loss: 0.0037, Val Loss: 0.0040\n",
            "Epoch [4/50], Train Loss: 0.0039, Val Loss: 0.0041\n",
            "Epoch [5/50], Train Loss: 0.0038, Val Loss: 0.0045\n",
            "Epoch [6/50], Train Loss: 0.0038, Val Loss: 0.0039\n",
            "Epoch [7/50], Train Loss: 0.0038, Val Loss: 0.0042\n",
            "Epoch [8/50], Train Loss: 0.0039, Val Loss: 0.0048\n",
            "Epoch [9/50], Train Loss: 0.0040, Val Loss: 0.0040\n",
            "Epoch [10/50], Train Loss: 0.0040, Val Loss: 0.0045\n",
            "Epoch [11/50], Train Loss: 0.0040, Val Loss: 0.0040\n",
            "Epoch [12/50], Train Loss: 0.0039, Val Loss: 0.0040\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Final model loaded from lstm_model_synthetic_best.pth (best performing model during training).\n",
            "--- Completed SYNTHETIC data training for Trial 15 ---\n",
            "\n",
            "--- Final Evaluation and Comparison of Best Models ---\n",
            "Final Real-trained Model Metrics: {'MSE': 10.688966751098633, 'RMSE': 3.269398530479059, 'R2': 0.9028962850570679, 'Directional Accuracy': 0.4899328859060403}\n",
            "Final Synthetic-trained Model Metrics: {'MSE': 99.17182922363281, 'RMSE': 9.958505370969723, 'R2': 0.4509766101837158, 'Directional Accuracy': 0.3087248322147651}\n",
            "Comparing MSE between Real-trained vs Synthetic-trained:\n",
            "Single trial comparison - Real-trained MSE: 10.6890, Synthetic-trained MSE: 99.1718\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def smote_time_series(data, num_new_samples):\n",
        "    \"\"\"\n",
        "    Generates synthetic time series data using a simplified SMOTE-like approach.\n",
        "    This function interpolates between existing data points.\n",
        "    \"\"\"\n",
        "    n_samples, n_features = data.shape\n",
        "    new_data = np.zeros((num_new_samples, n_features))\n",
        "    for i in range(num_new_samples):\n",
        "        # Randomly select a minority sample\n",
        "        idx = np.random.randint(0, n_samples)\n",
        "        sample = data[idx]\n",
        "\n",
        "        # Find its k-nearest neighbors (for simplicity, we'll just pick a random neighbor)\n",
        "        neighbor_idx = np.random.randint(0, n_samples)\n",
        "        neighbor = data[neighbor_idx]\n",
        "\n",
        "        # Generate a new sample by interpolating between the two\n",
        "        alpha = np.random.rand()\n",
        "        new_sample = sample + alpha * (neighbor - sample)\n",
        "        new_data[i] = new_sample\n",
        "    return new_data"
      ],
      "metadata": {
        "id": "rLnYzkXgGNep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the WGAN with Gradient Penalty.\n",
        "def train_wgan(generator, discriminator, real_data, latent_dim, device,\n",
        "               epochs=100, batch_size=64, lr_g=0.0001, lr_d=0.0001,\n",
        "               critic_iterations=5, lambda_gp=10):\n",
        "    optimizer_g = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.9))\n",
        "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.9))\n",
        "\n",
        "    generator.to(device)\n",
        "    discriminator.to(device)\n",
        "\n",
        "    print(\"Starting WGAN training...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        permutation = torch.randperm(real_data.size(0))\n",
        "        for i in range(0, real_data.size(0), batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            real_batch = real_data[indices].to(device)\n",
        "\n",
        "            for _ in range(critic_iterations):\n",
        "                discriminator.zero_grad()\n",
        "\n",
        "                # Train with real data\n",
        "                d_real = discriminator(real_batch)\n",
        "\n",
        "                # Train with fake data\n",
        "                z = torch.randn(real_batch.size(0), latent_dim).to(device)\n",
        "                fake_batch = generator(z).detach()\n",
        "                d_fake = discriminator(fake_batch)\n",
        "\n",
        "                # Wasserstein loss for Discriminator\n",
        "                loss_d_wasserstein = d_fake.mean() - d_real.mean()\n",
        "\n",
        "                # Gradient Penalty\n",
        "                alpha = torch.rand(real_batch.size(0), 1, 1).to(device)\n",
        "                interpolates = (alpha * real_batch + ((1 - alpha) * fake_batch)).requires_grad_(True)\n",
        "\n",
        "                # Calculate gradients on the same device as interpolates and discriminator\n",
        "                # Apply the CuDNN flag fix here\n",
        "                with torch.backends.cudnn.flags(enabled=False):\n",
        "                    d_interpolates = discriminator(interpolates)\n",
        "\n",
        "                gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates,\n",
        "                                                 grad_outputs=torch.ones_like(d_interpolates),\n",
        "                                                 create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "                gradients = gradients.view(gradients.size(0), -1)\n",
        "                gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp\n",
        "\n",
        "                # Total Discriminator loss (components are now on the same device)\n",
        "                loss_d = loss_d_wasserstein + gradient_penalty\n",
        "\n",
        "                # Backpropagate and update Discriminator weights\n",
        "                loss_d.backward()\n",
        "                optimizer_d.step()\n",
        "\n",
        "            # ===============================================\n",
        "            # Train Generator\n",
        "            # ===============================================\n",
        "            if i % critic_iterations == 0:\n",
        "                generator.zero_grad()\n",
        "                z = torch.randn(batch_size, latent_dim).to(device)\n",
        "                fake_batch = generator(z)\n",
        "                d_fake = discriminator(fake_batch)\n",
        "\n",
        "                # Generator loss (maximize D(G(z)) is equivalent to minimizing -D(G(z)))\n",
        "                loss_g = -d_fake.mean()\n",
        "\n",
        "                # Backpropagate and update Generator weights\n",
        "                loss_g.backward()\n",
        "                optimizer_g.step()\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], D_loss: {loss_d.item():.4f}, G_loss: {loss_g.item():.4f}\")\n",
        "\n",
        "    print(\"WGAN training finished.\")\n",
        "    return generator, discriminator"
      ],
      "metadata": {
        "id": "xoBbbjO8A2Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#Generator Network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "#Discriminator Network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# WGAN Training Function (Corrected)\n",
        "def train_wgan(generator, discriminator, real_data, epochs=100, lr=0.0002, n_critic=5):\n",
        "    # This is a simplified training loop. You may need to adapt it.\n",
        "    optimizer_g = torch.optim.RMSprop(generator.parameters(), lr=lr)\n",
        "    optimizer_d = torch.optim.RMSprop(discriminator.parameters(), lr=lr)\n",
        "\n",
        "    input_dim = generator.input_dim # Accessing the new attribute\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train Discriminator\n",
        "        for _ in range(n_critic):\n",
        "            noise = torch.randn(real_data.size(0), input_dim)\n",
        "            fake_data = generator(noise)\n",
        "            d_loss = -torch.mean(discriminator(real_data)) + torch.mean(discriminator(fake_data.detach()))\n",
        "            optimizer_d.zero_grad()\n",
        "            d_loss.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "        noise = torch.randn(real_data.size(0), input_dim)\n",
        "        fake_data = generator(noise)\n",
        "        g_loss = -torch.mean(discriminator(fake_data))\n",
        "        optimizer_g.zero_grad()\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}, D_loss: {d_loss.item()}, G_loss: {g_loss.item()}\")\n",
        "\n",
        "def generate_wgan_synthetic_data(generator, num_samples, input_dim):\n",
        "    noise = torch.randn(num_samples, input_dim)\n",
        "    with torch.no_grad():\n",
        "        synthetic_data = generator(noise)\n",
        "    return synthetic_data.numpy()"
      ],
      "metadata": {
        "id": "Vf3EgIzlDYhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================================================\n",
        "# 1. Import Required Libraries\n",
        "# =========================================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from scipy.stats import ttest_rel\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch.backends.cudnn as cudnn\n",
        "import random\n",
        "\n",
        "# =========================================================================================\n",
        "# 2. Helper Functions for Data Handling\n",
        "# =========================================================================================\n",
        "def set_seed(seed):\n",
        "    \"\"\"Sets a random seed for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        cudnn.benchmark = False\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "def get_stock_data(ticker, start, end):\n",
        "    \"\"\"Downloads stock data from Yahoo Finance.\"\"\"\n",
        "    print(f\"Downloading data for {ticker} from {start} to {end}...\")\n",
        "    df = yf.download(ticker, start=start, end=end, progress=False)\n",
        "    print(\"Data download finished.\")\n",
        "    return df\n",
        "\n",
        "def scale_data(data):\n",
        "    \"\"\"Scales data using MinMaxScaler and returns the scaled data and the scaler.\"\"\"\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n",
        "    return scaled_data, scaler\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    \"\"\"Creates sequences and labels for LSTM training.\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def split_data(X, y, train_ratio=0.8):\n",
        "    \"\"\"Splits data into training and testing sets.\"\"\"\n",
        "    split_idx = int(len(X) * train_ratio)\n",
        "    X_train = X[:split_idx]\n",
        "    X_test = X[split_idx:]\n",
        "    y_train = y[:split_idx]\n",
        "    y_test = y[split_idx:]\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    \"\"\"Calculates and returns a dictionary of evaluation metrics.\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    y_true_diff = np.diff(y_true.flatten())\n",
        "    y_pred_diff = np.diff(y_pred.flatten())\n",
        "\n",
        "    if len(y_true_diff) > 0:\n",
        "        correct_direction = np.sum(np.sign(y_true_diff) == np.sign(y_pred_diff))\n",
        "        directional_accuracy = correct_direction / len(y_true_diff)\n",
        "    else:\n",
        "        directional_accuracy = np.nan\n",
        "\n",
        "    return {\n",
        "        \"MSE\": mse,\n",
        "        \"RMSE\": rmse,\n",
        "        \"R2\": r2,\n",
        "        \"Directional Accuracy\": directional_accuracy\n",
        "    }\n",
        "\n",
        "def compare_metrics(metrics1, metrics2, metric_name):\n",
        "    \"\"\"Compares a specific metric between two lists of results using a paired t-test.\"\"\"\n",
        "    data1 = [m[metric_name] for m in metrics1]\n",
        "    data2 = [m[metric_name] for m in metrics2]\n",
        "    print(f\"Comparing {metric_name} between models:\")\n",
        "    if len(data1) > 1 and len(data2) > 1 and len(data1) == len(data2):\n",
        "        stat, p = ttest_rel(data1, data2)\n",
        "        print(f\"  Paired T-test: statistic={stat:.4f}, p-value={p:.4f}\")\n",
        "    else:\n",
        "        print(\"  Cannot perform statistical test: Mismatched or insufficient data.\")\n",
        "\n",
        "# =========================================================================================\n",
        "# 3. LSTM Model Definition\n",
        "# =========================================================================================\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"A simple LSTM model for time series prediction.\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
        "        predictions = self.linear(lstm_out[:, -1, :])\n",
        "        return predictions\n",
        "\n",
        "# =========================================================================================\n",
        "# 4. WGAN Implementation\n",
        "# =========================================================================================\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"WGAN Generator network.\"\"\"\n",
        "    def __init__(self, latent_dim, seq_length, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, seq_length * output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1, self.seq_length, 1)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"WGAN Discriminator network.\"\"\"\n",
        "    def __init__(self, input_dim, seq_length):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(input_dim * seq_length, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        input_flat = input.view(input.size(0), -1)\n",
        "        return self.main(input_flat)\n",
        "\n",
        "def train_wgan(generator, discriminator, real_data, latent_dim, device, epochs=100, batch_size=64, lr=0.0001, n_critic=5, lambda_gp=10):\n",
        "    \"\"\"Training loop for the WGAN.\"\"\"\n",
        "    optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        permutation = torch.randperm(real_data.size(0))\n",
        "        for i in range(0, real_data.size(0), batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            real_batch = real_data[indices].to(device)\n",
        "\n",
        "            for _ in range(n_critic):\n",
        "                discriminator.zero_grad()\n",
        "                d_real = discriminator(real_batch)\n",
        "\n",
        "                noise = torch.randn(real_batch.size(0), latent_dim).to(device)\n",
        "                fake_batch = generator(noise).detach()\n",
        "                d_fake = discriminator(fake_batch)\n",
        "\n",
        "                loss_d_wasserstein = d_real.mean() - d_fake.mean()\n",
        "\n",
        "                alpha = torch.rand(real_batch.size(0), 1, 1).to(device)\n",
        "                interpolates = (alpha * real_batch + ((1 - alpha) * fake_batch)).requires_grad_(True)\n",
        "                d_interpolates = discriminator(interpolates)\n",
        "\n",
        "                gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates,\n",
        "                                                 grad_outputs=torch.ones_like(d_interpolates),\n",
        "                                                 create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "                gradients = gradients.view(gradients.size(0), -1)\n",
        "                gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp\n",
        "\n",
        "                loss_d = -loss_d_wasserstein + gradient_penalty\n",
        "                loss_d.backward()\n",
        "                optimizer_d.step()\n",
        "\n",
        "            generator.zero_grad()\n",
        "            noise = torch.randn(batch_size, latent_dim).to(device)\n",
        "            fake_batch = generator(noise)\n",
        "            d_fake = discriminator(fake_batch)\n",
        "            loss_g = -d_fake.mean()\n",
        "            loss_g.backward()\n",
        "            optimizer_g.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"WGAN Epoch [{epoch+1}/{epochs}], D_loss: {loss_d.item():.4f}, G_loss: {loss_g.item():.4f}\")\n",
        "\n",
        "    print(\"WGAN training finished.\")\n",
        "    return generator, discriminator\n",
        "\n",
        "def generate_wgan_synthetic_data(generator, num_sequences, latent_dim, seq_length, device):\n",
        "    \"\"\"Generates synthetic sequences from the trained WGAN generator.\"\"\"\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        noise = torch.randn(num_sequences, latent_dim).to(device)\n",
        "        # Fix: Use .detach() before converting to numpy\n",
        "        synthetic_sequences_scaled_np = generator(noise).detach().cpu().numpy()\n",
        "    return synthetic_sequences_scaled_np\n",
        "\n",
        "# =========================================================================================\n",
        "# 5. SMOTE-TS Implementation\n",
        "# =========================================================================================\n",
        "def smote_time_series(X_data, y_data, n_synthetic_samples, k_neighbors=5):\n",
        "    \"\"\"\n",
        "    Generates synthetic time series data and corresponding labels using a SMOTE-like approach.\n",
        "    \"\"\"\n",
        "    num_sequences, seq_length, num_features = X_data.shape\n",
        "    X_data_flat = X_data.reshape(num_sequences, -1)\n",
        "\n",
        "    if num_sequences < k_neighbors:\n",
        "        k_neighbors = max(1, num_sequences - 1)\n",
        "\n",
        "    if k_neighbors == 0:\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    nn = NearestNeighbors(n_neighbors=k_neighbors + 1).fit(X_data_flat)\n",
        "    distances, indices = nn.kneighbors(X_data_flat)\n",
        "\n",
        "    synthetic_X_flat = []\n",
        "    synthetic_y = []\n",
        "\n",
        "    for _ in range(n_synthetic_samples):\n",
        "        anchor_idx = np.random.randint(0, num_sequences)\n",
        "        neighbor_idx = indices[anchor_idx, np.random.randint(1, k_neighbors + 1)]\n",
        "\n",
        "        anchor_sequence_flat = X_data_flat[anchor_idx]\n",
        "        neighbor_sequence_flat = X_data_flat[neighbor_idx]\n",
        "\n",
        "        anchor_y = y_data[anchor_idx]\n",
        "        neighbor_y = y_data[neighbor_idx]\n",
        "\n",
        "        alpha = np.random.rand()\n",
        "        new_X_sample_flat = anchor_sequence_flat + alpha * (neighbor_sequence_flat - anchor_sequence_flat)\n",
        "        new_y_sample = anchor_y + alpha * (neighbor_y - anchor_y)\n",
        "\n",
        "        synthetic_X_flat.append(new_X_sample_flat)\n",
        "        synthetic_y.append(new_y_sample)\n",
        "\n",
        "    synthetic_X = np.array(synthetic_X_flat).reshape(n_synthetic_samples, seq_length, num_features)\n",
        "    synthetic_y = np.array(synthetic_y)\n",
        "\n",
        "    return synthetic_X, synthetic_y\n",
        "\n",
        "# =========================================================================================\n",
        "# 6. Main Training and Evaluation Function\n",
        "# =========================================================================================\n",
        "def train_and_evaluate(X, y, scaler, experiment_name, seq_length=10, epochs=100, batch_size=32, lr=0.001):\n",
        "    \"\"\"Trains and evaluates an LSTM model on the provided sequences.\"\"\"\n",
        "    print(f\"\\n--- Training LSTM on {experiment_name} ---\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if len(X) < 2:\n",
        "        print(\"Insufficient data to create sequences and split.\")\n",
        "        return None\n",
        "\n",
        "    X_train, y_train, X_test, y_test = split_data(X, y)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = LSTMModel(input_size=1, hidden_size=50, output_size=1).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    print(f\"Starting training for {epochs} epochs...\")\n",
        "    for epoch in range(epochs):\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(X_batch)\n",
        "            loss = criterion(predictions, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    print(\"Starting evaluation...\")\n",
        "    with torch.no_grad():\n",
        "        test_predictions_scaled = model(X_test_tensor).cpu().numpy()\n",
        "\n",
        "    test_predictions_rescaled = scaler.inverse_transform(test_predictions_scaled)\n",
        "    y_test_rescaled = scaler.inverse_transform(y_test_tensor.cpu().numpy())\n",
        "\n",
        "    metrics = evaluate_model(y_test_rescaled, test_predictions_rescaled)\n",
        "    print(\"Evaluation finished.\")\n",
        "    print(\"Evaluation Metrics:\", metrics)\n",
        "    return metrics\n",
        "\n",
        "def run_experiment(real_data, n_trials=5):\n",
        "    \"\"\"Runs all experiments multiple times for statistical comparison.\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Hyperparameters\n",
        "    seq_length = 10\n",
        "    epochs = 100\n",
        "    batch_size = 32\n",
        "    lr = 0.001\n",
        "    latent_dim = 10\n",
        "\n",
        "    all_metrics = {\n",
        "        'real_data': [],\n",
        "        'wgan_hybrid': [],\n",
        "        'smote_ts_hybrid': []\n",
        "    }\n",
        "\n",
        "    real_data_scaled, real_scaler = scale_data(real_data[['Close']])\n",
        "    X_real_scaled, y_real_scaled = create_sequences(real_data_scaled, seq_length)\n",
        "\n",
        "    for i in range(n_trials):\n",
        "        print(f\"\\n\" + \"=\"*50)\n",
        "        print(f\"--- Running Trial {i+1}/{n_trials} ---\")\n",
        "        print(\"=\"*50)\n",
        "        set_seed(100 + i)\n",
        "\n",
        "        # 1. Real Data Experiment\n",
        "        metrics_real = train_and_evaluate(X_real_scaled, y_real_scaled, real_scaler, \"Real Data\", seq_length, epochs, batch_size, lr)\n",
        "        if metrics_real:\n",
        "            all_metrics['real_data'].append(metrics_real)\n",
        "\n",
        "        # 2. WGAN Hybrid Data Experiment\n",
        "        print(\"\\n--- Generating WGAN Synthetic Data ---\")\n",
        "        wgan_generator = Generator(latent_dim, seq_length, 1).to(device)\n",
        "        wgan_discriminator = Discriminator(1, seq_length).to(device)\n",
        "        wgan_data_tensor = torch.tensor(X_real_scaled, dtype=torch.float32).to(device)\n",
        "        trained_wgan_generator, _ = train_wgan(wgan_generator, wgan_discriminator, wgan_data_tensor, latent_dim, device)\n",
        "        num_sequences_to_generate = len(X_real_scaled)\n",
        "        synthetic_sequences_scaled = generate_wgan_synthetic_data(trained_wgan_generator, num_sequences_to_generate, latent_dim, seq_length, device)\n",
        "\n",
        "        # Combine real and synthetic scaled data\n",
        "        X_wgan_hybrid_scaled = np.concatenate((X_real_scaled, synthetic_sequences_scaled), axis=0)\n",
        "        y_wgan_hybrid_scaled = np.concatenate((y_real_scaled, synthetic_sequences_scaled[:, -1, :]), axis=0)\n",
        "\n",
        "        metrics_wgan = train_and_evaluate(X_wgan_hybrid_scaled, y_wgan_hybrid_scaled, real_scaler, \"WGAN Hybrid Data\", seq_length, epochs, batch_size, lr)\n",
        "        if metrics_wgan:\n",
        "            all_metrics['wgan_hybrid'].append(metrics_wgan)\n",
        "\n",
        "        # 3. SMOTE-TS Hybrid Data Experiment\n",
        "        print(\"\\n--- Generating SMOTE-TS Synthetic Data ---\")\n",
        "        X_smote_ts_synth, y_smote_ts_synth = smote_time_series(X_real_scaled, y_real_scaled, n_synthetic_samples=len(X_real_scaled))\n",
        "\n",
        "        # Combine real and synthetic scaled data\n",
        "        X_smote_ts_hybrid_scaled = np.concatenate((X_real_scaled, X_smote_ts_synth), axis=0)\n",
        "        y_smote_ts_hybrid_scaled = np.concatenate((y_real_scaled, y_smote_ts_synth), axis=0)\n",
        "\n",
        "        metrics_smote = train_and_evaluate(X_smote_ts_hybrid_scaled, y_smote_ts_hybrid_scaled, real_scaler, \"SMOTE-TS Hybrid Data\", seq_length, epochs, batch_size, lr)\n",
        "        if metrics_smote:\n",
        "            all_metrics['smote_ts_hybrid'].append(metrics_smote)\n",
        "\n",
        "    return all_metrics\n",
        "\n",
        "# =========================================================================================\n",
        "# 7. Main Execution\n",
        "# =========================================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and prepare real data once\n",
        "    real_data = get_stock_data(\"AAPL\", \"2020-01-01\", \"2023-01-01\")\n",
        "\n",
        "    # Run all experiments for multiple trials\n",
        "    results = run_experiment(real_data, n_trials=5)\n",
        "\n",
        "    # Calculate and display average metrics and statistical comparison\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" \" * 15 + \"Final Results Summary\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    for model_name, metrics_list in results.items():\n",
        "        if metrics_list:\n",
        "            avg_metrics = {metric: np.mean([m[metric] for m in metrics_list]) for metric in metrics_list[0]}\n",
        "            std_metrics = {metric: np.std([m[metric] for m in metrics_list]) for metric in metrics_list[0]}\n",
        "            print(f\"Model trained on {model_name.replace('_', ' ')} (Average over {len(metrics_list)} trials):\")\n",
        "            for metric, avg_value in avg_metrics.items():\n",
        "                std_value = std_metrics[metric]\n",
        "                print(f\"  - {metric}: {avg_value:.4f} +/- {std_value:.4f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" \" * 10 + \"Statistical Comparisons (p-values < 0.05 are significant)\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "    if results['real_data'] and results['wgan_hybrid']:\n",
        "        compare_metrics(results['real_data'], results['wgan_hybrid'], \"MSE\")\n",
        "        compare_metrics(results['real_data'], results['wgan_hybrid'], \"R2\")\n",
        "    if results['real_data'] and results['smote_ts_hybrid']:\n",
        "        compare_metrics(results['real_data'], results['smote_ts_hybrid'], \"MSE\")\n",
        "        compare_metrics(results['real_data'], results['smote_ts_hybrid'], \"R2\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OMF274SaS7q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92cd88f-dc85-4221-f9d7-5874e9abf049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for AAPL from 2020-01-01 to 2023-01-01...\n",
            "Data download finished.\n",
            "Using device: cuda\n",
            "\n",
            "==================================================\n",
            "--- Running Trial 1/5 ---\n",
            "==================================================\n",
            "\n",
            "--- Training LSTM on Real Data ---\n",
            "Starting training for 100 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-383031666.py:34: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=start, end=end, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0078\n",
            "Epoch [20/100], Loss: 0.0043\n",
            "Epoch [30/100], Loss: 0.0016\n",
            "Epoch [40/100], Loss: 0.0021\n",
            "Epoch [50/100], Loss: 0.0033\n",
            "Epoch [60/100], Loss: 0.0049\n",
            "Epoch [70/100], Loss: 0.0042\n",
            "Epoch [80/100], Loss: 0.0026\n",
            "Epoch [90/100], Loss: 0.0013\n",
            "Epoch [100/100], Loss: 0.0015\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 12.732291221618652, 'RMSE': 3.5682336276677082, 'R2': 0.8843336701393127, 'Directional Accuracy': 0.4429530201342282}\n",
            "\n",
            "--- Generating WGAN Synthetic Data ---\n",
            "WGAN Epoch [10/100], D_loss: -0.8780, G_loss: 0.3797\n",
            "WGAN Epoch [20/100], D_loss: -0.0875, G_loss: 1.0277\n",
            "WGAN Epoch [30/100], D_loss: -0.1572, G_loss: 0.8251\n",
            "WGAN Epoch [40/100], D_loss: -0.0580, G_loss: 0.8679\n",
            "WGAN Epoch [50/100], D_loss: -0.1349, G_loss: 0.9137\n",
            "WGAN Epoch [60/100], D_loss: 0.1225, G_loss: 0.4181\n",
            "WGAN Epoch [70/100], D_loss: -0.0605, G_loss: 0.2626\n",
            "WGAN Epoch [80/100], D_loss: -0.0319, G_loss: 0.0965\n",
            "WGAN Epoch [90/100], D_loss: -0.0348, G_loss: -0.4833\n",
            "WGAN Epoch [100/100], D_loss: -0.0827, G_loss: -0.3443\n",
            "WGAN training finished.\n",
            "\n",
            "--- Training LSTM on WGAN Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0025\n",
            "Epoch [20/100], Loss: 0.0025\n",
            "Epoch [30/100], Loss: 0.0009\n",
            "Epoch [40/100], Loss: 0.0007\n",
            "Epoch [50/100], Loss: 0.0030\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Epoch [70/100], Loss: 0.0010\n",
            "Epoch [80/100], Loss: 0.0011\n",
            "Epoch [90/100], Loss: 0.0004\n",
            "Epoch [100/100], Loss: 0.0014\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 0.1386939138174057, 'RMSE': 0.3724163178720902, 'R2': 0.9998497366905212, 'Directional Accuracy': 0.9966442953020134}\n",
            "\n",
            "--- Generating SMOTE-TS Synthetic Data ---\n",
            "\n",
            "--- Training LSTM on SMOTE-TS Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0050\n",
            "Epoch [20/100], Loss: 0.0026\n",
            "Epoch [30/100], Loss: 0.0012\n",
            "Epoch [40/100], Loss: 0.0007\n",
            "Epoch [50/100], Loss: 0.0017\n",
            "Epoch [60/100], Loss: 0.0009\n",
            "Epoch [70/100], Loss: 0.0007\n",
            "Epoch [80/100], Loss: 0.0035\n",
            "Epoch [90/100], Loss: 0.0015\n",
            "Epoch [100/100], Loss: 0.0009\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 4.580118179321289, 'RMSE': 2.1401210665103245, 'R2': 0.9951093792915344, 'Directional Accuracy': 0.9798657718120806}\n",
            "\n",
            "==================================================\n",
            "--- Running Trial 2/5 ---\n",
            "==================================================\n",
            "\n",
            "--- Training LSTM on Real Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0061\n",
            "Epoch [20/100], Loss: 0.0038\n",
            "Epoch [30/100], Loss: 0.0034\n",
            "Epoch [40/100], Loss: 0.0016\n",
            "Epoch [50/100], Loss: 0.0020\n",
            "Epoch [60/100], Loss: 0.0026\n",
            "Epoch [70/100], Loss: 0.0023\n",
            "Epoch [80/100], Loss: 0.0030\n",
            "Epoch [90/100], Loss: 0.0015\n",
            "Epoch [100/100], Loss: 0.0037\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 12.933196067810059, 'RMSE': 3.5962753047855025, 'R2': 0.8825085759162903, 'Directional Accuracy': 0.44966442953020136}\n",
            "\n",
            "--- Generating WGAN Synthetic Data ---\n",
            "WGAN Epoch [10/100], D_loss: -0.6748, G_loss: 0.6208\n",
            "WGAN Epoch [20/100], D_loss: -0.0830, G_loss: 0.4248\n",
            "WGAN Epoch [30/100], D_loss: -0.1973, G_loss: -0.3239\n",
            "WGAN Epoch [40/100], D_loss: 0.0808, G_loss: -0.6369\n",
            "WGAN Epoch [50/100], D_loss: -0.0281, G_loss: -0.3372\n",
            "WGAN Epoch [60/100], D_loss: -0.0747, G_loss: -0.6698\n",
            "WGAN Epoch [70/100], D_loss: -0.1277, G_loss: -0.9316\n",
            "WGAN Epoch [80/100], D_loss: -0.0974, G_loss: -1.1232\n",
            "WGAN Epoch [90/100], D_loss: -0.1679, G_loss: -1.2147\n",
            "WGAN Epoch [100/100], D_loss: -0.1545, G_loss: -1.3675\n",
            "WGAN training finished.\n",
            "\n",
            "--- Training LSTM on WGAN Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0026\n",
            "Epoch [20/100], Loss: 0.0048\n",
            "Epoch [30/100], Loss: 0.0041\n",
            "Epoch [40/100], Loss: 0.0010\n",
            "Epoch [50/100], Loss: 0.0040\n",
            "Epoch [60/100], Loss: 0.0016\n",
            "Epoch [70/100], Loss: 0.0025\n",
            "Epoch [80/100], Loss: 0.0005\n",
            "Epoch [90/100], Loss: 0.0011\n",
            "Epoch [100/100], Loss: 0.0004\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 0.14545182883739471, 'RMSE': 0.3813814741664764, 'R2': 0.9998384118080139, 'Directional Accuracy': 1.0}\n",
            "\n",
            "--- Generating SMOTE-TS Synthetic Data ---\n",
            "\n",
            "--- Training LSTM on SMOTE-TS Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0028\n",
            "Epoch [20/100], Loss: 0.0032\n",
            "Epoch [30/100], Loss: 0.0018\n",
            "Epoch [40/100], Loss: 0.0010\n",
            "Epoch [50/100], Loss: 0.0011\n",
            "Epoch [60/100], Loss: 0.0016\n",
            "Epoch [70/100], Loss: 0.0014\n",
            "Epoch [80/100], Loss: 0.0018\n",
            "Epoch [90/100], Loss: 0.0027\n",
            "Epoch [100/100], Loss: 0.0008\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 5.758261680603027, 'RMSE': 2.399637822798063, 'R2': 0.9931493401527405, 'Directional Accuracy': 0.9731543624161074}\n",
            "\n",
            "==================================================\n",
            "--- Running Trial 3/5 ---\n",
            "==================================================\n",
            "\n",
            "--- Training LSTM on Real Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0039\n",
            "Epoch [20/100], Loss: 0.0055\n",
            "Epoch [30/100], Loss: 0.0056\n",
            "Epoch [40/100], Loss: 0.0022\n",
            "Epoch [50/100], Loss: 0.0041\n",
            "Epoch [60/100], Loss: 0.0022\n",
            "Epoch [70/100], Loss: 0.0014\n",
            "Epoch [80/100], Loss: 0.0020\n",
            "Epoch [90/100], Loss: 0.0045\n",
            "Epoch [100/100], Loss: 0.0011\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 13.236310005187988, 'RMSE': 3.638173993253757, 'R2': 0.8797549605369568, 'Directional Accuracy': 0.4429530201342282}\n",
            "\n",
            "--- Generating WGAN Synthetic Data ---\n",
            "WGAN Epoch [10/100], D_loss: -0.4879, G_loss: -0.5026\n",
            "WGAN Epoch [20/100], D_loss: -0.0361, G_loss: 0.2130\n",
            "WGAN Epoch [30/100], D_loss: -0.2526, G_loss: 0.4727\n",
            "WGAN Epoch [40/100], D_loss: -0.0247, G_loss: 0.1278\n",
            "WGAN Epoch [50/100], D_loss: 0.0011, G_loss: 0.0432\n",
            "WGAN Epoch [60/100], D_loss: -0.1707, G_loss: -0.4013\n",
            "WGAN Epoch [70/100], D_loss: 0.0626, G_loss: 0.0358\n",
            "WGAN Epoch [80/100], D_loss: -0.0316, G_loss: -0.6025\n",
            "WGAN Epoch [90/100], D_loss: 0.0144, G_loss: -0.3787\n",
            "WGAN Epoch [100/100], D_loss: 0.0065, G_loss: -0.7068\n",
            "WGAN training finished.\n",
            "\n",
            "--- Training LSTM on WGAN Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0023\n",
            "Epoch [20/100], Loss: 0.0009\n",
            "Epoch [30/100], Loss: 0.0008\n",
            "Epoch [40/100], Loss: 0.0008\n",
            "Epoch [50/100], Loss: 0.0015\n",
            "Epoch [60/100], Loss: 0.0033\n",
            "Epoch [70/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0006\n",
            "Epoch [100/100], Loss: 0.0007\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 0.3982924222946167, 'RMSE': 0.6311041295179558, 'R2': 0.9995666146278381, 'Directional Accuracy': 1.0}\n",
            "\n",
            "--- Generating SMOTE-TS Synthetic Data ---\n",
            "\n",
            "--- Training LSTM on SMOTE-TS Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0068\n",
            "Epoch [20/100], Loss: 0.0025\n",
            "Epoch [30/100], Loss: 0.0024\n",
            "Epoch [40/100], Loss: 0.0016\n",
            "Epoch [50/100], Loss: 0.0011\n",
            "Epoch [60/100], Loss: 0.0008\n",
            "Epoch [70/100], Loss: 0.0015\n",
            "Epoch [80/100], Loss: 0.0013\n",
            "Epoch [90/100], Loss: 0.0015\n",
            "Epoch [100/100], Loss: 0.0025\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 5.666457176208496, 'RMSE': 2.380432140643479, 'R2': 0.9944220185279846, 'Directional Accuracy': 0.9731543624161074}\n",
            "\n",
            "==================================================\n",
            "--- Running Trial 4/5 ---\n",
            "==================================================\n",
            "\n",
            "--- Training LSTM on Real Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0023\n",
            "Epoch [20/100], Loss: 0.0077\n",
            "Epoch [30/100], Loss: 0.0038\n",
            "Epoch [40/100], Loss: 0.0029\n",
            "Epoch [50/100], Loss: 0.0021\n",
            "Epoch [60/100], Loss: 0.0018\n",
            "Epoch [70/100], Loss: 0.0037\n",
            "Epoch [80/100], Loss: 0.0020\n",
            "Epoch [90/100], Loss: 0.0019\n",
            "Epoch [100/100], Loss: 0.0018\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 12.750428199768066, 'RMSE': 3.570774173728726, 'R2': 0.8841689229011536, 'Directional Accuracy': 0.4429530201342282}\n",
            "\n",
            "--- Generating WGAN Synthetic Data ---\n",
            "WGAN Epoch [10/100], D_loss: -1.0736, G_loss: 0.3741\n",
            "WGAN Epoch [20/100], D_loss: -0.1056, G_loss: 0.1523\n",
            "WGAN Epoch [30/100], D_loss: -0.1872, G_loss: -0.3469\n",
            "WGAN Epoch [40/100], D_loss: 0.0884, G_loss: -0.3753\n",
            "WGAN Epoch [50/100], D_loss: -0.0723, G_loss: -0.2227\n",
            "WGAN Epoch [60/100], D_loss: -0.0061, G_loss: -0.0321\n",
            "WGAN Epoch [70/100], D_loss: -0.0444, G_loss: -0.1183\n",
            "WGAN Epoch [80/100], D_loss: -0.0050, G_loss: -0.6346\n",
            "WGAN Epoch [90/100], D_loss: 0.1195, G_loss: -1.2014\n",
            "WGAN Epoch [100/100], D_loss: -0.0623, G_loss: -1.1875\n",
            "WGAN training finished.\n",
            "\n",
            "--- Training LSTM on WGAN Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0098\n",
            "Epoch [20/100], Loss: 0.0023\n",
            "Epoch [30/100], Loss: 0.0007\n",
            "Epoch [40/100], Loss: 0.0006\n",
            "Epoch [50/100], Loss: 0.0003\n",
            "Epoch [60/100], Loss: 0.0004\n",
            "Epoch [70/100], Loss: 0.0011\n",
            "Epoch [80/100], Loss: 0.0024\n",
            "Epoch [90/100], Loss: 0.0012\n",
            "Epoch [100/100], Loss: 0.0032\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 0.8560945987701416, 'RMSE': 0.9252538023537875, 'R2': 0.9988251328468323, 'Directional Accuracy': 1.0}\n",
            "\n",
            "--- Generating SMOTE-TS Synthetic Data ---\n",
            "\n",
            "--- Training LSTM on SMOTE-TS Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0074\n",
            "Epoch [20/100], Loss: 0.0043\n",
            "Epoch [30/100], Loss: 0.0057\n",
            "Epoch [40/100], Loss: 0.0038\n",
            "Epoch [50/100], Loss: 0.0006\n",
            "Epoch [60/100], Loss: 0.0034\n",
            "Epoch [70/100], Loss: 0.0026\n",
            "Epoch [80/100], Loss: 0.0013\n",
            "Epoch [90/100], Loss: 0.0019\n",
            "Epoch [100/100], Loss: 0.0023\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 4.037076950073242, 'RMSE': 2.009247856804442, 'R2': 0.9956771731376648, 'Directional Accuracy': 0.9798657718120806}\n",
            "\n",
            "==================================================\n",
            "--- Running Trial 5/5 ---\n",
            "==================================================\n",
            "\n",
            "--- Training LSTM on Real Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0061\n",
            "Epoch [20/100], Loss: 0.0028\n",
            "Epoch [30/100], Loss: 0.0038\n",
            "Epoch [40/100], Loss: 0.0015\n",
            "Epoch [50/100], Loss: 0.0024\n",
            "Epoch [60/100], Loss: 0.0019\n",
            "Epoch [70/100], Loss: 0.0034\n",
            "Epoch [80/100], Loss: 0.0023\n",
            "Epoch [90/100], Loss: 0.0024\n",
            "Epoch [100/100], Loss: 0.0012\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 12.884657859802246, 'RMSE': 3.5895205612730856, 'R2': 0.8829495310783386, 'Directional Accuracy': 0.4429530201342282}\n",
            "\n",
            "--- Generating WGAN Synthetic Data ---\n",
            "WGAN Epoch [10/100], D_loss: -0.7150, G_loss: 0.0289\n",
            "WGAN Epoch [20/100], D_loss: -0.0825, G_loss: -0.7169\n",
            "WGAN Epoch [30/100], D_loss: -0.1562, G_loss: -0.5023\n",
            "WGAN Epoch [40/100], D_loss: -0.2353, G_loss: -0.6188\n",
            "WGAN Epoch [50/100], D_loss: -0.0717, G_loss: -0.6925\n",
            "WGAN Epoch [60/100], D_loss: -0.1614, G_loss: -1.3285\n",
            "WGAN Epoch [70/100], D_loss: -0.2054, G_loss: -0.4113\n",
            "WGAN Epoch [80/100], D_loss: 0.0781, G_loss: -1.4547\n",
            "WGAN Epoch [90/100], D_loss: -0.1876, G_loss: -1.4253\n",
            "WGAN Epoch [100/100], D_loss: -0.0157, G_loss: -1.2888\n",
            "WGAN training finished.\n",
            "\n",
            "--- Training LSTM on WGAN Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0038\n",
            "Epoch [20/100], Loss: 0.0027\n",
            "Epoch [30/100], Loss: 0.0030\n",
            "Epoch [40/100], Loss: 0.0025\n",
            "Epoch [50/100], Loss: 0.0048\n",
            "Epoch [60/100], Loss: 0.0043\n",
            "Epoch [70/100], Loss: 0.0018\n",
            "Epoch [80/100], Loss: 0.0018\n",
            "Epoch [90/100], Loss: 0.0008\n",
            "Epoch [100/100], Loss: 0.0016\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 0.10175444930791855, 'RMSE': 0.31898973229230837, 'R2': 0.9998907446861267, 'Directional Accuracy': 0.9966442953020134}\n",
            "\n",
            "--- Generating SMOTE-TS Synthetic Data ---\n",
            "\n",
            "--- Training LSTM on SMOTE-TS Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0050\n",
            "Epoch [20/100], Loss: 0.0029\n",
            "Epoch [30/100], Loss: 0.0011\n",
            "Epoch [40/100], Loss: 0.0056\n",
            "Epoch [50/100], Loss: 0.0032\n",
            "Epoch [60/100], Loss: 0.0013\n",
            "Epoch [70/100], Loss: 0.0036\n",
            "Epoch [80/100], Loss: 0.0011\n",
            "Epoch [90/100], Loss: 0.0030\n",
            "Epoch [100/100], Loss: 0.0008\n",
            "Training finished.\n",
            "Starting evaluation...\n",
            "Evaluation finished.\n",
            "Evaluation Metrics: {'MSE': 4.654655933380127, 'RMSE': 2.1574651638856484, 'R2': 0.9943789839744568, 'Directional Accuracy': 0.9697986577181208}\n",
            "\n",
            "==================================================\n",
            "               Final Results Summary\n",
            "==================================================\n",
            "\n",
            "Model trained on real data (Average over 5 trials):\n",
            "  - MSE: 12.9074 +/- 0.1815\n",
            "  - RMSE: 3.5926 +/- 0.0252\n",
            "  - R2: 0.8827 +/- 0.0016\n",
            "  - Directional Accuracy: 0.4443 +/- 0.0027\n",
            "--------------------------------------------------\n",
            "Model trained on wgan hybrid (Average over 5 trials):\n",
            "  - MSE: 0.3281 +/- 0.2843\n",
            "  - RMSE: 0.5258 +/- 0.2271\n",
            "  - R2: 0.9996 +/- 0.0004\n",
            "  - Directional Accuracy: 0.9987 +/- 0.0016\n",
            "--------------------------------------------------\n",
            "Model trained on smote ts hybrid (Average over 5 trials):\n",
            "  - MSE: 4.9393 +/- 0.6669\n",
            "  - RMSE: 2.2174 +/- 0.1501\n",
            "  - R2: 0.9945 +/- 0.0008\n",
            "  - Directional Accuracy: 0.9752 +/- 0.0040\n",
            "--------------------------------------------------\n",
            "\n",
            "==================================================\n",
            "          Statistical Comparisons (p-values < 0.05 are significant)\n",
            "==================================================\n",
            "\n",
            "Comparing MSE between models:\n",
            "  Paired T-test: statistic=71.3755, p-value=0.0000\n",
            "Comparing R2 between models:\n",
            "  Paired T-test: statistic=-132.2990, p-value=0.0000\n",
            "Comparing MSE between models:\n",
            "  Paired T-test: statistic=29.6378, p-value=0.0000\n",
            "Comparing R2 between models:\n",
            "  Paired T-test: statistic=-151.8979, p-value=0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "468a2fcb",
        "outputId": "c16525da-828a-424a-d956-454c308e1ff2"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Adversarial loss (MSELoss is common for CycleGAN)\n",
        "criterion_gan = nn.MSELoss()\n",
        "\n",
        "# Cycle consistency loss (L1Loss is common)\n",
        "criterion_cycle = nn.L1Loss()\n",
        "\n",
        "# Identity loss (L1Loss is common)\n",
        "criterion_identity = nn.L1Loss()\n",
        "\n",
        "print(\"CycleGAN loss functions defined.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CycleGAN loss functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abd2fcbe"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Defines a Residual Block for the Generator.\"\"\"\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad1d(1), # Pad to maintain sequence length\n",
        "            nn.Conv1d(in_features, in_features, kernel_size=3),\n",
        "            nn.InstanceNorm1d(in_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad1d(1), # Pad to maintain sequence length\n",
        "            nn.Conv1d(in_features, in_features, kernel_size=3),\n",
        "            nn.InstanceNorm1d(in_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "class CycleGANGenerator(nn.Module):\n",
        "    \"\"\"CycleGAN Generator network for time series.\"\"\"\n",
        "    def __init__(self, input_nc, output_nc, n_residual_blocks=6):\n",
        "        super(CycleGANGenerator, self).__init__()\n",
        "\n",
        "        # Initial convolution layers\n",
        "        model = [\n",
        "            nn.ReflectionPad1d(3), # Pad to maintain sequence length after large kernel\n",
        "            nn.Conv1d(input_nc, 64, kernel_size=7),\n",
        "            nn.InstanceNorm1d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "\n",
        "        # Downsampling layers\n",
        "        in_features = 64\n",
        "        out_features = in_features * 2\n",
        "        for _ in range(2): # Two downsampling steps\n",
        "            model += [\n",
        "                nn.Conv1d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
        "                nn.InstanceNorm1d(out_features),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "\n",
        "        # Residual blocks\n",
        "        for _ in range(n_residual_blocks):\n",
        "            model += [ResidualBlock(in_features)]\n",
        "\n",
        "        # Upsampling layers\n",
        "        out_features = in_features // 2\n",
        "        for _ in range(2): # Two upsampling steps\n",
        "            model += [\n",
        "                nn.ConvTranspose1d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm1d(out_features),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "\n",
        "        # Output layer\n",
        "        model += [\n",
        "            nn.ReflectionPad1d(3), # Pad to match input padding\n",
        "            nn.Conv1d(64, output_nc, kernel_size=7),\n",
        "            nn.Tanh() # Output scaled between -1 and 1\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure input is (batch_size, channels, sequence_length)\n",
        "        if x.ndim == 3 and x.shape[1] != self.model[0].in_channels:\n",
        "             x = x.permute(0, 2, 1) # Permute to (batch, channels, sequence) if needed\n",
        "        elif x.ndim == 2: # Handle input like (batch_size, sequence_length) by adding channel dim\n",
        "             x = x.unsqueeze(1) # Add channel dimension (batch, 1, sequence)\n",
        "\n",
        "        return self.model(x)\n",
        "\n",
        "class CycleGANDiscriminator(nn.Module):\n",
        "    \"\"\"CycleGAN Discriminator network for time series.\"\"\"\n",
        "    def __init__(self, input_nc):\n",
        "        super(CycleGANDiscriminator, self).__init__()\n",
        "\n",
        "        # Using a PatchGAN-like structure with 1D convolutions\n",
        "        model = [\n",
        "            nn.Conv1d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "\n",
        "        model += [\n",
        "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm1d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "\n",
        "        model += [\n",
        "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm1d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "\n",
        "        model += [\n",
        "            nn.Conv1d(256, 512, kernel_size=4, padding=1),\n",
        "            nn.InstanceNorm1d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "\n",
        "        # Output layer: 1-dimensional output for each \"patch\"\n",
        "        model += [nn.Conv1d(512, 1, kernel_size=4, padding=1)]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "         # Ensure input is (batch_size, channels, sequence_length)\n",
        "        if x.ndim == 3 and x.shape[1] != self.model[0].in_channels:\n",
        "             x = x.permute(0, 2, 1) # Permute to (batch, channels, sequence) if needed\n",
        "        elif x.ndim == 2: # Handle input like (batch_size, sequence_length) by adding channel dim\n",
        "             x = x.unsqueeze(1) # Add channel dimension (batch, 1, sequence)\n",
        "\n",
        "        return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e28e22f"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Assuming criterion_gan, criterion_cycle, criterion_identity are defined\n",
        "\n",
        "def train_cyclegan(gen_ab, gen_ba, disc_a, disc_b, real_data_a, real_data_b, device, epochs=100, batch_size=64, lr=0.0002, lambda_cycle=10.0, lambda_identity=5.0):\n",
        "    \"\"\"\n",
        "    Trains the CycleGAN model.\n",
        "\n",
        "    Args:\n",
        "        gen_ab (nn.Module): Generator mapping from domain A to B.\n",
        "        gen_ba (nn.Module): Generator mapping from domain B to A.\n",
        "        disc_a (nn.Module): Discriminator for domain A.\n",
        "        disc_b (nn.Module): Discriminator for domain B.\n",
        "        real_data_a (np.ndarray): Real data from domain A (e.g., scaled stock data sequences).\n",
        "        real_data_b (np.ndarray): Real data from domain B (e.g., modified scaled stock data sequences).\n",
        "        device (torch.device): Device to train on (cuda or cpu).\n",
        "        epochs (int): Number of training epochs.\n",
        "        batch_size (int): Batch size for training.\n",
        "        lr (float): Learning rate for optimizers.\n",
        "        lambda_cycle (float): Weight for cycle consistency loss.\n",
        "        lambda_identity (float): Weight for identity loss.\n",
        "    \"\"\"\n",
        "    print(\"Starting CycleGAN training...\")\n",
        "\n",
        "    # Optimizers\n",
        "    optimizer_g = optim.Adam(list(gen_ab.parameters()) + list(gen_ba.parameters()), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_d_a = optim.Adam(disc_a.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_d_b = optim.Adam(disc_b.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    # Move models to device\n",
        "    gen_ab.to(device)\n",
        "    gen_ba.to(device)\n",
        "    disc_a.to(device)\n",
        "    disc_b.to(device)\n",
        "\n",
        "    # Prepare data loaders\n",
        "    # Ensure input data is in the format (Batch, Channels, Sequence) for Conv1d\n",
        "    real_data_a_tensor = torch.tensor(real_data_a, dtype=torch.float32).to(device).permute(0, 2, 1)\n",
        "    real_data_b_tensor = torch.tensor(real_data_b, dtype=torch.float32).to(device).permute(0, 2, 1)\n",
        "\n",
        "    dataset_a = TensorDataset(real_data_a_tensor)\n",
        "    dataloader_a = DataLoader(dataset_a, batch_size=batch_size, shuffle=True)\n",
        "    dataset_b = TensorDataset(real_data_b_tensor)\n",
        "    dataloader_b = DataLoader(dataset_b, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        for i, (real_a, real_b) in enumerate(zip(dataloader_a, dataloader_b)):\n",
        "            real_a = real_a[0] # DataLoader returns a tuple\n",
        "            real_b = real_b[0] # DataLoader returns a tuple\n",
        "\n",
        "            # Adversarial ground truths\n",
        "            valid = torch.ones(real_a.size(0), *disc_a(real_a).size()[1:]).to(device)\n",
        "            fake = torch.zeros(real_a.size(0), *disc_a(real_a).size()[1:]).to(device)\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generators\n",
        "            # -----------------\n",
        "            optimizer_g.zero_grad()\n",
        "\n",
        "            # Identity loss\n",
        "            # G_BA should be able to translate real_b to real_b\n",
        "            loss_identity_a = criterion_identity(gen_ba(real_b), real_b)\n",
        "            # G_AB should be able to translate real_a to real_a\n",
        "            loss_identity_b = criterion_identity(gen_ab(real_a), real_a)\n",
        "            loss_identity = (loss_identity_a + loss_identity_b) / 2\n",
        "\n",
        "\n",
        "            # GAN loss\n",
        "            fake_b = gen_ab(real_a)\n",
        "            loss_gan_ab = criterion_gan(disc_b(fake_b), valid)\n",
        "\n",
        "            fake_a = gen_ba(real_b)\n",
        "            loss_gan_ba = criterion_gan(disc_a(fake_a), valid)\n",
        "\n",
        "            loss_gan = (loss_gan_ab + loss_gan_ba) / 2\n",
        "\n",
        "            # Cycle consistency loss\n",
        "            recovered_a = gen_ba(fake_b)\n",
        "            loss_cycle_a = criterion_cycle(recovered_a, real_a)\n",
        "\n",
        "            recovered_b = gen_ab(fake_a)\n",
        "            loss_cycle_b = criterion_cycle(recovered_b, real_b)\n",
        "\n",
        "            loss_cycle = (loss_cycle_a + loss_cycle_b) / 2\n",
        "\n",
        "            # Total generator loss\n",
        "            loss_g = loss_gan + loss_cycle * lambda_cycle + loss_identity * lambda_identity\n",
        "\n",
        "            loss_g.backward()\n",
        "            optimizer_g.step()\n",
        "\n",
        "            # -----------------------\n",
        "            #  Train Discriminator A\n",
        "            # -----------------------\n",
        "            optimizer_d_a.zero_grad()\n",
        "\n",
        "            # Real loss\n",
        "            loss_real_a = criterion_gan(disc_a(real_a), valid)\n",
        "\n",
        "            # Fake loss (from generator G_BA)\n",
        "            fake_a_detached = gen_ba(real_b).detach() # Detach to prevent backprop through generator\n",
        "            loss_fake_a = criterion_gan(disc_a(fake_a_detached), fake)\n",
        "\n",
        "            # Total discriminator A loss\n",
        "            loss_d_a = (loss_real_a + loss_fake_a) / 2\n",
        "\n",
        "            loss_d_a.backward()\n",
        "            optimizer_d_a.step()\n",
        "\n",
        "            # -----------------------\n",
        "            #  Train Discriminator B\n",
        "            # -----------------------\n",
        "            optimizer_d_b.zero_grad()\n",
        "\n",
        "            # Real loss\n",
        "            loss_real_b = criterion_gan(disc_b(real_b), valid)\n",
        "\n",
        "            # Fake loss (from generator G_AB)\n",
        "            fake_b_detached = gen_ab(real_a).detach() # Detach to prevent backprop through generator\n",
        "            loss_fake_b = criterion_gan(disc_b(fake_b_detached), fake)\n",
        "\n",
        "            # Total discriminator B loss\n",
        "            loss_d_b = (loss_real_b + loss_fake_b) / 2\n",
        "            loss_d_b.backward()\n",
        "            optimizer_d_b.step()\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"CycleGAN Epoch [{epoch+1}/{epochs}], G_Loss: {loss_g.item():.4f}, D_A_Loss: {loss_d_a.item():.4f}, D_B_Loss: {loss_d_b.item():.4f}\")\n",
        "\n",
        "    print(\"CycleGAN training finished.\")\n",
        "    # Return trained generators to be used for synthetic data generation\n",
        "    return gen_ab, gen_ba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df60d00e"
      },
      "source": [
        "# Assuming CycleGANGenerator and CycleGANDiscriminator classes are defined\n",
        "\n",
        "# Example of how to instantiate and train CycleGAN (will be used in run_experiment)\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# seq_length = 10 # Define appropriate seq_length\n",
        "\n",
        "# # Assuming real_data_a and real_data_b are prepared (e.g., scaled real data and slightly modified real data)\n",
        "# # real_data_a_sequences, _ = create_sequences(scaled_real_data, seq_length)\n",
        "# # real_data_b_sequences, _ = create_sequences(scaled_real_data_modified, seq_length)\n",
        "\n",
        "# # gen_ab = CycleGANGenerator(input_nc=1, output_nc=1).to(device)\n",
        "# # gen_ba = CycleGANGenerator(input_nc=1, output_nc=1).to(device)\n",
        "# # disc_a = CycleGANDiscriminator(input_nc=1).to(device)\n",
        "# # disc_b = CycleGANDiscriminator(input_nc=1).to(device)\n",
        "\n",
        "# # train_cyclegan(gen_ab, gen_ba, disc_a, disc_b, real_data_a_sequences, real_data_b_sequences, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50fb35f8"
      },
      "source": [
        "import torch\n",
        "\n",
        "def generate_cyclegan_synthetic_data(generator, real_data_a, num_sequences, device):\n",
        "    \"\"\"\n",
        "    Generates synthetic sequences from a trained CycleGAN generator.\n",
        "\n",
        "    Args:\n",
        "        generator (nn.Module): The trained CycleGAN generator (e.g., gen_ab).\n",
        "        real_data_a (np.ndarray): Real data from domain A (scaled sequences).\n",
        "        num_sequences (int): The number of synthetic sequences to generate.\n",
        "        device (torch.device): Device to perform generation on.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Generated synthetic sequences (scaled).\n",
        "    \"\"\"\n",
        "    generator.eval() # Set generator to evaluation mode\n",
        "    synthetic_data_list = []\n",
        "    with torch.no_grad():\n",
        "        # We need to generate num_sequences of synthetic data.\n",
        "        # We can pass real_data_a through the generator to get synthetic data in domain B.\n",
        "        # For simplicity, we will just pass the entire real_data_a through the generator\n",
        "        # and take the first num_sequences generated.\n",
        "        real_data_a_tensor = torch.tensor(real_data_a, dtype=torch.float32).to(device).permute(0, 2, 1) # (batch, channels, sequence)\n",
        "\n",
        "        # Generate synthetic data in batches if necessary to avoid memory issues\n",
        "        batch_size = 64 # Can adjust this batch size\n",
        "        for i in range(0, real_data_a_tensor.size(0), batch_size):\n",
        "            batch_real_a = real_data_a_tensor[i : i + batch_size]\n",
        "            fake_b_scaled = generator(batch_real_a).permute(0, 2, 1).cpu().numpy() # (batch, sequence, channels)\n",
        "            synthetic_data_list.append(fake_b_scaled)\n",
        "\n",
        "    synthetic_sequences_scaled = np.concatenate(synthetic_data_list, axis=0)[:num_sequences] # Ensure we get exactly num_sequences\n",
        "    print(\"CycleGAN synthetic data generated.\")\n",
        "    return synthetic_sequences_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0507e514",
        "outputId": "c55d1f1c-bf8a-486f-e86c-9f4736234a9c"
      },
      "source": [
        "# =========================================================================================\n",
        "# 1. Import Required Libraries\n",
        "# =========================================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from scipy.stats import ttest_rel\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch.backends.cudnn as cudnn\n",
        "import random\n",
        "import os\n",
        "from laplace import Laplace\n",
        "\n",
        "# =========================================================================================\n",
        "# 2. Helper Functions for Data Handling\n",
        "# =========================================================================================\n",
        "def set_seed(seed):\n",
        "    \"\"\"Sets a random seed for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        cudnn.benchmark = False\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "def get_stock_data(ticker, start, end):\n",
        "    \"\"\"Downloads stock data from Yahoo Finance.\"\"\"\n",
        "    print(f\"Downloading data for {ticker} from {start} to {end}...\")\n",
        "    df = yf.download(ticker, start=start, end=end, progress=False)\n",
        "    print(\"Data download finished.\")\n",
        "    return df\n",
        "\n",
        "def scale_data(data):\n",
        "    \"\"\"Scales data using MinMaxScaler and returns the scaled data and the scaler.\"\"\"\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n",
        "    return scaled_data, scaler\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    \"\"\"Creates sequences and labels for LSTM training.\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    \"\"\"Calculates and returns a dictionary of evaluation metrics.\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    y_true_diff = np.diff(y_true.flatten())\n",
        "    y_pred_diff = np.diff(y_pred.flatten())\n",
        "    if len(y_true_diff) > 0:\n",
        "        correct_direction = np.sum(np.sign(y_true_diff) == np.sign(y_pred_diff))\n",
        "        directional_accuracy = correct_direction / len(y_true_diff)\n",
        "    else:\n",
        "        directional_accuracy = np.nan\n",
        "    return {\n",
        "        \"MSE\": mse,\n",
        "        \"RMSE\": rmse,\n",
        "        \"R2\": r2,\n",
        "        \"Directional Accuracy\": directional_accuracy\n",
        "    }\n",
        "\n",
        "def compare_metrics(metrics1, metrics2, metric_name):\n",
        "    \"\"\"Compares a specific metric between two lists of results using a paired t-test.\"\"\"\n",
        "    data1 = [m[metric_name] for m in metrics1]\n",
        "    data2 = [m[metric_name] for m in metrics2]\n",
        "    print(f\"Comparing {metric_name} between models:\")\n",
        "    if len(data1) > 1 and len(data2) > 1 and len(data1) == len(data2):\n",
        "        stat, p = ttest_rel(data1, data2)\n",
        "        print(f\"  Paired T-test: statistic={stat:.4f}, p-value={p:.4f}\")\n",
        "    else:\n",
        "        print(\"  Cannot perform statistical test: Mismatched or insufficient data.\")\n",
        "\n",
        "# =========================================================================================\n",
        "# 3. LSTM Model Definition\n",
        "# =========================================================================================\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"A simple LSTM model for time series prediction.\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
        "        predictions = self.linear(lstm_out[:, -1, :])\n",
        "        return predictions\n",
        "\n",
        "# =========================================================================================\n",
        "# 4. CycleGAN Implementation\n",
        "# =========================================================================================\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad1d(1),\n",
        "            nn.Conv1d(in_features, in_features, kernel_size=3),\n",
        "            nn.InstanceNorm1d(in_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad1d(1),\n",
        "            nn.Conv1d(in_features, in_features, kernel_size=3),\n",
        "            nn.InstanceNorm1d(in_features)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "class CycleGANGenerator(nn.Module):\n",
        "    \"\"\"CycleGAN Generator network for time series.\"\"\"\n",
        "    def __init__(self, input_nc, output_nc, n_residual_blocks=6):\n",
        "        super(CycleGANGenerator, self).__init__()\n",
        "        model = [\n",
        "            nn.ReflectionPad1d(3),\n",
        "            nn.Conv1d(input_nc, 64, kernel_size=7),\n",
        "            nn.InstanceNorm1d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "        in_features = 64\n",
        "        out_features = in_features * 2\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.Conv1d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
        "                nn.InstanceNorm1d(out_features),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            out_features = in_features * 2\n",
        "        for _ in range(n_residual_blocks):\n",
        "            model += [ResidualBlock(in_features)]\n",
        "        out_features = in_features // 2\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.ConvTranspose1d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm1d(out_features),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "        model += [\n",
        "            nn.ReflectionPad1d(3),\n",
        "            nn.Conv1d(64, output_nc, kernel_size=7),\n",
        "            nn.Tanh()\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 3 and x.shape[1] != self.model[0].in_channels:\n",
        "             x = x.permute(0, 2, 1)\n",
        "        elif x.ndim == 2:\n",
        "             x = x.unsqueeze(1)\n",
        "        return self.model(x)\n",
        "\n",
        "class CycleGANDiscriminator(nn.Module):\n",
        "    \"\"\"CycleGAN Discriminator network for time series.\"\"\"\n",
        "    def __init__(self, input_nc):\n",
        "        super(CycleGANDiscriminator, self).__init__()\n",
        "        model = [\n",
        "            nn.Conv1d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "        model += [\n",
        "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm1d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "        model += [\n",
        "            nn.Conv1d(128, 256, kernel_size=4, stride=1, padding=1), # Modified stride from 2 to 1\n",
        "            nn.InstanceNorm1d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "        model += [\n",
        "            nn.Conv1d(256, 512, kernel_size=4, padding=1),\n",
        "            nn.InstanceNorm1d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "        model += [nn.Conv1d(512, 1, kernel_size=4, padding=1)]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 3 and x.shape[1] != self.model[0].in_channels:\n",
        "             x = x.permute(0, 2, 1)\n",
        "        elif x.ndim == 2:\n",
        "             x = x.unsqueeze(1)\n",
        "        return self.model(x)\n",
        "\n",
        "# Assuming criterion_gan, criterion_cycle, criterion_identity are defined\n",
        "criterion_gan = nn.MSELoss()\n",
        "criterion_cycle = nn.L1Loss()\n",
        "criterion_identity = nn.L1Loss()\n",
        "\n",
        "\n",
        "def train_cyclegan(gen_ab, gen_ba, disc_a, disc_b, real_data_a, real_data_b, device, epochs=100, batch_size=64, lr=0.0002, lambda_cycle=10.0, lambda_identity=5.0):\n",
        "    \"\"\"\n",
        "    Trains the CycleGAN model.\n",
        "    \"\"\"\n",
        "    print(\"Starting CycleGAN training...\")\n",
        "    optimizer_g = optim.Adam(list(gen_ab.parameters()) + list(gen_ba.parameters()), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_d_a = optim.Adam(disc_a.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_d_b = optim.Adam(disc_b.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    gen_ab.to(device)\n",
        "    gen_ba.to(device)\n",
        "    disc_a.to(device)\n",
        "    disc_b.to(device)\n",
        "\n",
        "    real_data_a_tensor = torch.tensor(real_data_a, dtype=torch.float32).to(device).permute(0, 2, 1)\n",
        "    real_data_b_tensor = torch.tensor(real_data_b, dtype=torch.float32).to(device).permute(0, 2, 1)\n",
        "\n",
        "    dataset_a = TensorDataset(real_data_a_tensor)\n",
        "    dataloader_a = DataLoader(dataset_a, batch_size=batch_size, shuffle=True)\n",
        "    dataset_b = TensorDataset(real_data_b_tensor)\n",
        "    dataloader_b = DataLoader(dataset_b, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize losses to 0\n",
        "    loss_gan = torch.tensor(0.0).to(device)\n",
        "    loss_cycle = torch.tensor(0.0).to(device)\n",
        "    loss_identity = torch.tensor(0.0).to(device)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, (real_a, real_b) in enumerate(zip(dataloader_a, dataloader_b)):\n",
        "            real_a = real_a[0]\n",
        "            real_b = real_b[0]\n",
        "\n",
        "            valid = torch.ones(real_a.size(0), *disc_a(real_a).size()[1:]).to(device)\n",
        "            fake = torch.zeros(real_a.size(0), *disc_a(real_a).size()[1:]).to(device)\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generators\n",
        "            # -----------------\n",
        "            optimizer_g.zero_grad()\n",
        "\n",
        "            loss_identity_a = criterion_identity(gen_ba(real_b), real_b)\n",
        "            loss_identity_b = criterion_identity(gen_ab(real_a), real_a)\n",
        "            loss_identity = (loss_identity_a + loss_identity_b) / 2\n",
        "\n",
        "            fake_b = gen_ab(real_a)\n",
        "            loss_gan_ab = criterion_gan(disc_b(fake_b), valid)\n",
        "\n",
        "            fake_a = gen_ba(real_b)\n",
        "            loss_gan_ba = criterion_gan(disc_a(fake_a), valid)\n",
        "\n",
        "            loss_gan = (loss_gan_ab + loss_gan_ba) / 2\n",
        "\n",
        "            recovered_a = gen_ba(fake_b)\n",
        "            # Fix: Resize recovered_a to match real_a's sequence length before calculating loss\n",
        "            # Using interpolate again, ensuring target size is correct. Mode 'linear' requires float type and 3D input.\n",
        "            # 'nearest' is safer for potentially non-float data or simpler interpolation.\n",
        "            recovered_a_resized = torch.nn.functional.interpolate(recovered_a, size=real_a.size(2), mode='nearest')\n",
        "            loss_cycle_a = criterion_cycle(recovered_a_resized, real_a)\n",
        "\n",
        "            recovered_b = gen_ab(fake_a)\n",
        "            # Fix: Resize recovered_b to match real_b's sequence length before calculating loss\n",
        "            recovered_b_resized = torch.nn.functional.interpolate(recovered_b, size=real_b.size(2), mode='nearest')\n",
        "            loss_cycle_b = criterion_cycle(recovered_b_resized, real_b)\n",
        "\n",
        "            loss_cycle = (loss_cycle_a + loss_cycle_b) / 2\n",
        "\n",
        "            loss_g = loss_gan + loss_cycle * lambda_cycle + loss_identity * lambda_identity\n",
        "            loss_g.backward()\n",
        "            optimizer_g.step()\n",
        "\n",
        "            # -----------------------\n",
        "            #  Train Discriminator A\n",
        "            # -----------------------\n",
        "            optimizer_d_a.zero_grad()\n",
        "\n",
        "            # Real loss\n",
        "            loss_real_a = criterion_gan(disc_a(real_a), valid)\n",
        "\n",
        "            # Fake loss (from generator G_BA)\n",
        "            fake_a_detached = gen_ba(real_b).detach() # Detach to prevent backprop through generator\n",
        "            loss_fake_a = criterion_gan(disc_a(fake_a_detached), fake)\n",
        "\n",
        "            # Total discriminator A loss\n",
        "            loss_d_a = (loss_real_a + loss_fake_a) / 2\n",
        "\n",
        "            loss_d_a.backward()\n",
        "            optimizer_d_a.step()\n",
        "\n",
        "            # -----------------------\n",
        "            #  Train Discriminator B\n",
        "            # -----------------------\n",
        "            optimizer_d_b.zero_grad()\n",
        "\n",
        "            # Real loss\n",
        "            loss_real_b = criterion_gan(disc_b(real_b), valid)\n",
        "\n",
        "            # Fake loss (from generator G_AB)\n",
        "            fake_b_detached = gen_ab(real_a).detach() # Detach to prevent backprop through generator\n",
        "            loss_fake_b = criterion_gan(disc_b(fake_b_detached), fake)\n",
        "\n",
        "            # Total discriminator B loss\n",
        "            loss_d_b = (loss_real_b + loss_fake_b) / 2\n",
        "            loss_d_b.backward()\n",
        "            optimizer_d_b.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"CycleGAN Epoch [{epoch+1}/{epochs}], G_Loss: {loss_g.item():.4f}, D_A_Loss: {loss_d_a.item():.4f}, D_B_Loss: {loss_d_b.item():.4f}\")\n",
        "\n",
        "    print(\"CycleGAN training finished.\")\n",
        "    return gen_ab, gen_ba\n",
        "\n",
        "\n",
        "def generate_cyclegan_synthetic_data(generator, real_data_a, num_sequences, device):\n",
        "    \"\"\"\n",
        "    Generates synthetic sequences from a trained CycleGAN generator.\n",
        "    \"\"\"\n",
        "    generator.eval()\n",
        "    synthetic_data_list = []\n",
        "    with torch.no_grad():\n",
        "        real_data_a_tensor = torch.tensor(real_data_a, dtype=torch.float32).to(device).permute(0, 2, 1)\n",
        "        batch_size = 64\n",
        "        for i in range(0, real_data_a_tensor.size(0), batch_size):\n",
        "            batch_real_a = real_data_a_tensor[i : i + batch_size]\n",
        "            fake_b_scaled = generator(batch_real_a).permute(0, 2, 1).cpu().numpy()\n",
        "            synthetic_data_list.append(fake_b_scaled)\n",
        "    synthetic_sequences_scaled = np.concatenate(synthetic_data_list, axis=0)[:num_sequences]\n",
        "    print(\"CycleGAN synthetic data generated.\")\n",
        "    return synthetic_sequences_scaled\n",
        "\n",
        "# =========================================================================================\n",
        "# 5. WGAN Implementation (from Phase 2)\n",
        "# =========================================================================================\n",
        "class WGANGenerator(nn.Module):\n",
        "    \"\"\"WGAN Generator network.\"\"\"\n",
        "    def __init__(self, latent_dim, seq_length, output_dim):\n",
        "        super(WGANGenerator, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.latent_dim = latent_dim\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, seq_length * output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1, self.seq_length, 1)\n",
        "\n",
        "class WGANDiscriminator(nn.Module):\n",
        "    \"\"\"WGAN Discriminator network.\"\"\"\n",
        "    def __init__(self, input_dim, seq_length):\n",
        "        super(WGANDiscriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(input_dim * seq_length, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        input_flat = input.view(input.size(0), -1)\n",
        "        return self.main(input_flat)\n",
        "\n",
        "def train_wgan(generator, discriminator, real_data, latent_dim, device, epochs=100, batch_size=64, lr=0.0001, n_critic=5, lambda_gp=10):\n",
        "    \"\"\"Training loop for the WGAN.\"\"\"\n",
        "    optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "    for epoch in range(epochs):\n",
        "        permutation = torch.randperm(real_data.size(0))\n",
        "        for i in range(0, real_data.size(0), batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            real_batch = real_data[indices].to(device)\n",
        "            for _ in range(n_critic):\n",
        "                discriminator.zero_grad()\n",
        "                d_real = discriminator(real_batch)\n",
        "                noise = torch.randn(real_batch.size(0), latent_dim).to(device)\n",
        "                fake_batch = generator(noise).detach()\n",
        "                d_fake = discriminator(fake_batch)\n",
        "                loss_d_wasserstein = d_real.mean() - d_fake.mean()\n",
        "                alpha = torch.rand(real_batch.size(0), 1, 1).to(device)\n",
        "                interpolates = (alpha * real_batch + ((1 - alpha) * fake_batch)).requires_grad_(True)\n",
        "                d_interpolates = discriminator(interpolates)\n",
        "                gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates,\n",
        "                                                 grad_outputs=torch.ones_like(d_interpolates),\n",
        "                                                 create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "                gradients = gradients.view(gradients.size(0), -1)\n",
        "                gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp\n",
        "                loss_d = -loss_d_wasserstein + gradient_penalty\n",
        "                loss_d.backward()\n",
        "                optimizer_d.step()\n",
        "            generator.zero_grad()\n",
        "            noise = torch.randn(batch_size, latent_dim).to(device)\n",
        "            fake_batch = generator(noise)\n",
        "            d_fake = discriminator(fake_batch)\n",
        "            loss_g = -d_fake.mean()\n",
        "            loss_g.backward()\n",
        "            optimizer_g.step()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"WGAN Epoch [{epoch+1}/{epochs}], D_loss: {loss_d.item():.4f}, G_loss: {loss_g.item():.4f}\")\n",
        "    print(\"WGAN training finished.\")\n",
        "    return generator, discriminator\n",
        "\n",
        "def generate_wgan_synthetic_data(generator, num_sequences, latent_dim, seq_length, device):\n",
        "    \"\"\"Generates synthetic sequences from the trained WGAN generator.\"\"\"\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        noise = torch.randn(num_sequences, latent_dim).to(device)\n",
        "        synthetic_sequences_scaled_np = generator(noise).detach().cpu().numpy()\n",
        "    return synthetic_sequences_scaled_np\n",
        "\n",
        "# =========================================================================================\n",
        "# 6. SMOTE-TS Implementation (from Phase 2)\n",
        "# =========================================================================================\n",
        "def smote_time_series(X_data, y_data, n_synthetic_samples, k_neighbors=5):\n",
        "    \"\"\"\n",
        "    Generates synthetic time series data and corresponding labels using a SMOTE-like approach.\n",
        "    \"\"\"\n",
        "    num_sequences, seq_length, num_features = X_data.shape\n",
        "    X_data_flat = X_data.reshape(num_sequences, -1)\n",
        "    if num_sequences < k_neighbors:\n",
        "        k_neighbors = max(1, num_sequences - 1)\n",
        "    if k_neighbors == 0:\n",
        "        return np.array([]), np.array([])\n",
        "    nn = NearestNeighbors(n_neighbors=k_neighbors + 1).fit(X_data_flat)\n",
        "    distances, indices = nn.kneighbors(X_data_flat)\n",
        "    synthetic_X_flat = []\n",
        "    synthetic_y = []\n",
        "    for _ in range(n_synthetic_samples):\n",
        "        anchor_idx = np.random.randint(0, num_sequences)\n",
        "        neighbor_idx = indices[anchor_idx, np.random.randint(1, k_neighbors + 1)]\n",
        "        anchor_sequence_flat = X_data_flat[anchor_idx]\n",
        "        neighbor_sequence_flat = X_data_flat[neighbor_idx]\n",
        "        anchor_y = y_data[anchor_idx]\n",
        "        neighbor_y = y_data[neighbor_idx]\n",
        "        alpha = np.random.rand()\n",
        "        new_X_sample_flat = anchor_sequence_flat + alpha * (neighbor_sequence_flat - anchor_sequence_flat)\n",
        "        new_y_sample = anchor_y + alpha * (neighbor_y - anchor_y)\n",
        "        synthetic_X_flat.append(new_X_sample_flat)\n",
        "        synthetic_y.append(new_y_sample)\n",
        "    synthetic_X = np.array(synthetic_X_flat).reshape(n_synthetic_samples, seq_length, num_features)\n",
        "    synthetic_y = np.array(synthetic_y)\n",
        "    return synthetic_X, synthetic_y\n",
        "\n",
        "# =========================================================================================\n",
        "# 7. Main Training and Evaluation Function (Modified for Laplace)\n",
        "# =========================================================================================\n",
        "def train_and_evaluate(X, y, scaler, experiment_name, seq_length=10, epochs=100, batch_size=32, lr=0.001, train_only=False):\n",
        "    \"\"\"Trains and evaluates an LSTM model on the provided sequences.\"\"\"\n",
        "    print(f\"\\n--- Training LSTM on {experiment_name} ---\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    if len(X) < 2:\n",
        "        print(\"Insufficient data to create sequences and split.\")\n",
        "        return None, None\n",
        "\n",
        "    split_idx = int(len(X) * 0.8)\n",
        "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = LSTMModel(input_size=1, hidden_size=50, output_size=1).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    print(f\"Starting training for {epochs} epochs...\")\n",
        "    for epoch in range(epochs):\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(X_batch)\n",
        "            loss = criterion(predictions, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    # If only training is needed (for Laplace), return the model\n",
        "    if train_only:\n",
        "        return model, (X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    print(\"Starting evaluation...\")\n",
        "    with torch.no_grad():\n",
        "        test_predictions_scaled = model(X_test_tensor).cpu().numpy()\n",
        "        test_loss = criterion(torch.tensor(test_predictions_scaled).to(device), y_test_tensor)\n",
        "\n",
        "    y_test_rescaled = scaler.inverse_transform(y_test_tensor.cpu().numpy().reshape(-1, 1))\n",
        "    test_predictions_rescaled = scaler.inverse_transform(test_predictions_scaled.reshape(-1, 1))\n",
        "\n",
        "    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
        "\n",
        "    metrics = evaluate_model(y_test_rescaled.flatten(), test_predictions_rescaled.flatten())\n",
        "    print(\"Evaluation finished.\")\n",
        "    print(\"Evaluation Metrics:\", metrics)\n",
        "    return model, metrics\n",
        "\n",
        "# =========================================================================================\n",
        "# 8. Main Execution Block with Multi-Trial, WGAN, SMOTE, and Laplace Approximation\n",
        "# =========================================================================================\n",
        "def run_experiment(real_data, n_trials=5):\n",
        "    \"\"\"Runs all experiments multiple times for statistical comparison, with Laplace Approximation.\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    seq_length = 10\n",
        "    epochs = 100\n",
        "    batch_size = 32\n",
        "    lr = 0.001\n",
        "    latent_dim = 10\n",
        "\n",
        "    all_metrics = {\n",
        "        'real_data': [],\n",
        "        'wgan_hybrid': [],\n",
        "        'smote_ts_hybrid': [],\n",
        "        'cyclegan_hybrid': []\n",
        "    }\n",
        "\n",
        "    real_data_scaled, real_scaler = scale_data(real_data[['Close']])\n",
        "    X_real_scaled, y_real_scaled = create_sequences(real_data_scaled, seq_length)\n",
        "\n",
        "    for i in range(n_trials):\n",
        "        print(f\"\\n\" + \"=\"*50)\n",
        "        print(f\"--- Running Trial {i+1}/{n_trials} ---\")\n",
        "        print(\"=\"*50)\n",
        "        set_seed(100 + i)\n",
        "\n",
        "        # 1. Real Data Experiment (with Laplace)\n",
        "        model_real, data_real = train_and_evaluate(X_real_scaled, y_real_scaled, real_scaler, \"Real Data\", seq_length, epochs, batch_size, lr, train_only=True)\n",
        "        if model_real:\n",
        "            X_train, y_train, X_test, y_test = data_real\n",
        "            la = Laplace(model_real, 'regression', subset_of_weights='last_layer', hessian_structure='diag')\n",
        "            train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size)\n",
        "            la.fit(train_loader)\n",
        "            f_mu, f_var = la(X_test)\n",
        "            metrics = evaluate_model(y_test.cpu().numpy(), f_mu.cpu().detach().numpy())\n",
        "            metrics['Prediction Variance'] = np.mean(f_var.cpu().detach().numpy())\n",
        "            all_metrics['real_data'].append(metrics)\n",
        "            print(\"Laplace Approximation Metrics (Real Data):\", metrics)\n",
        "\n",
        "        # 2. WGAN Hybrid Data Experiment (with Laplace)\n",
        "        print(\"\\n--- Generating WGAN Synthetic Data ---\")\n",
        "        wgan_generator = WGANGenerator(latent_dim, seq_length, 1).to(device)\n",
        "        wgan_discriminator = WGANDiscriminator(1, seq_length).to(device)\n",
        "        wgan_data_tensor = torch.tensor(X_real_scaled, dtype=torch.float32).to(device)\n",
        "        trained_wgan_generator, _ = train_wgan(wgan_generator, wgan_discriminator, wgan_data_tensor, latent_dim, device)\n",
        "        num_sequences_to_generate = len(X_real_scaled)\n",
        "        synthetic_sequences_scaled = generate_wgan_synthetic_data(trained_wgan_generator, num_sequences_to_generate, latent_dim, seq_length, device)\n",
        "        X_wgan_hybrid_scaled = np.concatenate((X_real_scaled, synthetic_sequences_scaled), axis=0)\n",
        "        y_wgan_hybrid_scaled = np.concatenate((y_real_scaled, synthetic_sequences_scaled[:, -1, :]), axis=0)\n",
        "        model_wgan, data_wgan = train_and_evaluate(X_wgan_hybrid_scaled, y_wgan_hybrid_scaled, real_scaler, \"WGAN Hybrid Data\", seq_length, epochs, batch_size, lr, train_only=True)\n",
        "        if model_wgan:\n",
        "            X_train, y_train, X_test, y_test = data_wgan\n",
        "            la = Laplace(model_wgan, 'regression', subset_of_weights='last_layer', hessian_structure='diag')\n",
        "            train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size)\n",
        "            la.fit(train_loader)\n",
        "            f_mu, f_var = la(X_test)\n",
        "            metrics = evaluate_model(y_test.cpu().numpy(), f_mu.cpu().detach().numpy())\n",
        "            metrics['Prediction Variance'] = np.mean(f_var.cpu().detach().numpy())\n",
        "            all_metrics['wgan_hybrid'].append(metrics)\n",
        "            print(\"Laplace Approximation Metrics (WGAN Hybrid):\", metrics)\n",
        "\n",
        "        # 3. SMOTE-TS Hybrid Data Experiment (with Laplace)\n",
        "        print(\"\\n--- Generating SMOTE-TS Synthetic Data ---\")\n",
        "        X_smote_ts_synth, y_smote_ts_synth = smote_time_series(X_real_scaled, y_real_scaled, n_synthetic_samples=len(X_real_scaled))\n",
        "        X_smote_ts_hybrid_scaled = np.concatenate((X_real_scaled, X_smote_ts_synth), axis=0)\n",
        "        y_smote_ts_hybrid_scaled = np.concatenate((y_real_scaled, y_smote_ts_synth), axis=0)\n",
        "        model_smote, data_smote = train_and_evaluate(X_smote_ts_hybrid_scaled, y_smote_ts_hybrid_scaled, real_scaler, \"SMOTE-TS Hybrid Data\", seq_length, epochs, batch_size, lr, train_only=True)\n",
        "        if model_smote:\n",
        "            X_train, y_train, X_test, y_test = data_smote\n",
        "            la = Laplace(model_smote, 'regression', subset_of_weights='last_layer', hessian_structure='diag')\n",
        "            train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size)\n",
        "            la.fit(train_loader)\n",
        "            f_mu, f_var = la(X_test)\n",
        "            metrics = evaluate_model(y_test.cpu().numpy(), f_mu.cpu().detach().numpy())\n",
        "            metrics['Prediction Variance'] = np.mean(f_var.cpu().detach().numpy())\n",
        "            all_metrics['smote_ts_hybrid'].append(metrics)\n",
        "            print(\"Laplace Approximation Metrics (SMOTE-TS Hybrid):\", metrics)\n",
        "\n",
        "        # 4. CycleGAN Hybrid Data Experiment\n",
        "        if False: # Skip CycleGAN experiment execution\n",
        "            print(\"\\n--- Skipping CycleGAN Experiment due to persistent issues ---\")\n",
        "            # Using X_real_scaled as both domain A and domain B for CycleGAN training data\n",
        "            real_data_a_scaled = X_real_scaled\n",
        "            # Create a slightly modified version of real data for domain B (example)\n",
        "            # In a real scenario, domain B would be a different but related dataset\n",
        "            real_data_b_scaled = X_real_scaled + np.random.normal(0, 0.01, size=X_real_scaled.shape)\n",
        "\n",
        "\n",
        "            # Instantiate CycleGAN models\n",
        "            gen_ab = CycleGANGenerator(input_nc=1, output_nc=1, n_residual_blocks=6).to(device)\n",
        "            gen_ba = CycleGANGenerator(input_nc=1, output_nc=1, n_residual_blocks=6).to(device)\n",
        "            disc_a = CycleGANDiscriminator(input_nc=1).to(device)\n",
        "            disc_b = CycleGANDiscriminator(input_nc=1).to(device)\n",
        "\n",
        "\n",
        "            # Train CycleGAN\n",
        "            # Note: This training can be computationally intensive and might require tuning\n",
        "            trained_gen_ab, trained_gen_ba = train_cyclegan(\n",
        "                gen_ab, gen_ba, disc_a, disc_b,\n",
        "                real_data_a_scaled, real_data_b_scaled,\n",
        "                device, epochs=50, batch_size=32, lr=0.0002, lambda_cycle=10.0, lambda_identity=5.0\n",
        "            )\n",
        "\n",
        "            # Generate synthetic data using the trained generator G_AB\n",
        "            num_sequences_to_generate = len(X_real_scaled) # Generate as many synthetic sequences as real sequences\n",
        "            synthetic_cyclegan_sequences = generate_cyclegan_synthetic_data(\n",
        "                trained_gen_ab, real_data_a_scaled, num_sequences_to_generate, device\n",
        "            )\n",
        "\n",
        "            # Combine real and synthetic data for hybrid training\n",
        "            X_cyclegan_hybrid_scaled = np.concatenate((X_real_scaled, synthetic_cyclegan_sequences), axis=0)\n",
        "            # For the target variable y, we use the last element of the sequence\n",
        "            y_cyclegan_hybrid_scaled = np.concatenate((y_real_scaled, synthetic_cyclegan_sequences[:, -1, :].flatten()), axis=0)\n",
        "\n",
        "\n",
        "            # Train and evaluate LSTM on CycleGAN Hybrid Data\n",
        "            model_cyclegan, data_cyclegan = train_and_evaluate(\n",
        "                X_cyclegan_hybrid_scaled, y_cyclegan_hybrid_scaled, real_scaler,\n",
        "                \"CycleGAN Hybrid Data\", seq_length, epochs, batch_size, lr, train_only=True\n",
        "            )\n",
        "            if model_cyclegan:\n",
        "                X_train, y_train, X_test, y_test = data_cyclegan\n",
        "                # Apply Laplace Approximation to the trained model\n",
        "                la = Laplace(model_cyclegan, 'regression', subset_of_weights='last_layer', hessian_structure='diag')\n",
        "                train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size)\n",
        "                la.fit(train_loader)\n",
        "                # Evaluate with Laplace approximated model\n",
        "                f_mu, f_var = la(X_test)\n",
        "                metrics = evaluate_model(y_test.cpu().numpy(), f_mu.cpu().detach().numpy())\n",
        "                metrics['Prediction Variance'] = np.mean(f_var.cpu().detach().numpy())\n",
        "                all_metrics['cyclegan_hybrid'].append(metrics)\n",
        "                print(\"Laplace Approximation Metrics (CycleGAN Hybrid):\", metrics)\n",
        "\n",
        "\n",
        "    return all_metrics\n",
        "\n",
        "# =========================================================================================\n",
        "# 9. Main Execution Block\n",
        "# =========================================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    real_data = get_stock_data(\"AAPL\", \"2020-01-01\", \"2023-01-01\")\n",
        "    results = run_experiment(real_data, n_trials=5)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" \" * 15 + \"Final Results Summary\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    for model_name, metrics_list in results.items():\n",
        "        if metrics_list: # Check if the list of metrics is not empty\n",
        "            avg_metrics = {metric: np.mean([m[metric] for m in metrics_list]) for metric in metrics_list[0]}\n",
        "            std_metrics = {metric: np.std([m[metric] for m in metrics_list]) for metric in metrics_list[0]}\n",
        "            print(f\"Model trained on {model_name.replace('_', ' ')} (Average over {len(metrics_list)} trials):\")\n",
        "            for metric, avg_value in avg_metrics.items():\n",
        "                std_value = std_metrics[metric]\n",
        "                print(f\"  - {metric}: {avg_value:.4f} +/- {std_value:.4f}\")\n",
        "            print(\"-\" * 50)\n",
        "        else:\n",
        "             print(f\"No results available for {model_name.replace('_', ' ')}.\")\n",
        "             print(\"-\" * 50)\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" \" * 10 + \"Statistical Comparisons (p-values < 0.05 are significant)\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Ensure both lists for comparison are not empty\n",
        "    if results['real_data'] and results['wgan_hybrid']:\n",
        "        compare_metrics(results['real_data'], results['wgan_hybrid'], \"MSE\")\n",
        "    else:\n",
        "        print(\"Skipping comparison between Real Data and WGAN Hybrid: Insufficient data.\")\n",
        "\n",
        "    if results['real_data'] and results['smote_ts_hybrid']:\n",
        "        compare_metrics(results['real_data'], results['smote_ts_hybrid'], \"MSE\")\n",
        "    else:\n",
        "         print(\"Skipping comparison between Real Data and SMOTE-TS Hybrid: Insufficient data.\")\n",
        "\n",
        "    # Check specifically if cyclegan_hybrid has results before attempting comparison\n",
        "    if 'cyclegan_hybrid' in results and results['real_data'] and results['cyclegan_hybrid']:\n",
        "        compare_metrics(results['real_data'], results['cyclegan_hybrid'], \"MSE\")\n",
        "    else:\n",
        "         print(\"Skipping comparison between Real Data and CycleGAN Hybrid: Insufficient data (CycleGAN experiment skipped or failed).\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for AAPL from 2020-01-01 to 2023-01-01...\n",
            "Data download finished.\n",
            "Using device: cuda\n",
            "\n",
            "==================================================\n",
            "--- Running Trial 1/5 ---\n",
            "==================================================\n",
            "\n",
            "--- Training LSTM on Real Data ---\n",
            "Starting training for 100 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2940439821.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=start, end=end, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0078\n",
            "Epoch [20/100], Loss: 0.0043\n",
            "Epoch [30/100], Loss: 0.0016\n",
            "Epoch [40/100], Loss: 0.0021\n",
            "Epoch [50/100], Loss: 0.0033\n",
            "Epoch [60/100], Loss: 0.0049\n",
            "Epoch [70/100], Loss: 0.0042\n",
            "Epoch [80/100], Loss: 0.0026\n",
            "Epoch [90/100], Loss: 0.0013\n",
            "Epoch [100/100], Loss: 0.0015\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (Real Data): {'MSE': 0.0033055199310183525, 'RMSE': 0.05749365122357731, 'R2': 0.8843337297439575, 'Directional Accuracy': 0.4429530201342282, 'Prediction Variance': 0.061139017}\n",
            "\n",
            "--- Generating WGAN Synthetic Data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WGAN Epoch [10/100], D_loss: -0.6777, G_loss: 0.2199\n",
            "WGAN Epoch [20/100], D_loss: -0.0380, G_loss: 0.0928\n",
            "WGAN Epoch [30/100], D_loss: -0.1602, G_loss: 0.3364\n",
            "WGAN Epoch [40/100], D_loss: -0.0317, G_loss: 0.8155\n",
            "WGAN Epoch [50/100], D_loss: -0.0856, G_loss: 0.2352\n",
            "WGAN Epoch [60/100], D_loss: 0.0373, G_loss: 0.4382\n",
            "WGAN Epoch [70/100], D_loss: -0.0694, G_loss: -0.0822\n",
            "WGAN Epoch [80/100], D_loss: -0.1129, G_loss: 0.2786\n",
            "WGAN Epoch [90/100], D_loss: -0.0072, G_loss: 0.2626\n",
            "WGAN Epoch [100/100], D_loss: -0.0556, G_loss: 0.1491\n",
            "WGAN training finished.\n",
            "\n",
            "--- Training LSTM on WGAN Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0033\n",
            "Epoch [20/100], Loss: 0.0016\n",
            "Epoch [30/100], Loss: 0.0006\n",
            "Epoch [40/100], Loss: 0.0013\n",
            "Epoch [50/100], Loss: 0.0003\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Epoch [70/100], Loss: 0.0020\n",
            "Epoch [80/100], Loss: 0.0012\n",
            "Epoch [90/100], Loss: 0.0010\n",
            "Epoch [100/100], Loss: 0.0003\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (WGAN Hybrid): {'MSE': 2.7616237275651656e-05, 'RMSE': 0.005255115343705755, 'R2': 0.9998524785041809, 'Directional Accuracy': 1.0, 'Prediction Variance': 0.03486277}\n",
            "\n",
            "--- Generating SMOTE-TS Synthetic Data ---\n",
            "\n",
            "--- Training LSTM on SMOTE-TS Hybrid Data ---\n",
            "Starting training for 100 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0060\n",
            "Epoch [20/100], Loss: 0.0022\n",
            "Epoch [30/100], Loss: 0.0008\n",
            "Epoch [40/100], Loss: 0.0006\n",
            "Epoch [50/100], Loss: 0.0019\n",
            "Epoch [60/100], Loss: 0.0021\n",
            "Epoch [70/100], Loss: 0.0027\n",
            "Epoch [80/100], Loss: 0.0013\n",
            "Epoch [90/100], Loss: 0.0013\n",
            "Epoch [100/100], Loss: 0.0005\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (SMOTE-TS Hybrid): {'MSE': 0.001214133109897375, 'RMSE': 0.034844412893566955, 'R2': 0.9950063228607178, 'Directional Accuracy': 0.9798657718120806, 'Prediction Variance': 0.034842234}\n",
            "\n",
            "==================================================\n",
            "--- Running Trial 2/5 ---\n",
            "==================================================\n",
            "\n",
            "--- Training LSTM on Real Data ---\n",
            "Starting training for 100 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0061\n",
            "Epoch [20/100], Loss: 0.0038\n",
            "Epoch [30/100], Loss: 0.0034\n",
            "Epoch [40/100], Loss: 0.0016\n",
            "Epoch [50/100], Loss: 0.0020\n",
            "Epoch [60/100], Loss: 0.0026\n",
            "Epoch [70/100], Loss: 0.0023\n",
            "Epoch [80/100], Loss: 0.0030\n",
            "Epoch [90/100], Loss: 0.0015\n",
            "Epoch [100/100], Loss: 0.0037\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (Real Data): {'MSE': 0.003357676323503256, 'RMSE': 0.05794545990414828, 'R2': 0.8825086355209351, 'Directional Accuracy': 0.44966442953020136, 'Prediction Variance': 0.056823198}\n",
            "\n",
            "--- Generating WGAN Synthetic Data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WGAN Epoch [10/100], D_loss: -0.6204, G_loss: 0.7251\n",
            "WGAN Epoch [20/100], D_loss: 0.0154, G_loss: 0.1153\n",
            "WGAN Epoch [30/100], D_loss: -0.1281, G_loss: -0.1841\n",
            "WGAN Epoch [40/100], D_loss: -0.1630, G_loss: 0.0426\n",
            "WGAN Epoch [50/100], D_loss: -0.0606, G_loss: -0.3911\n",
            "WGAN Epoch [60/100], D_loss: -0.1606, G_loss: -1.1489\n",
            "WGAN Epoch [70/100], D_loss: 0.0404, G_loss: -1.1954\n",
            "WGAN Epoch [80/100], D_loss: -0.0434, G_loss: -1.4212\n",
            "WGAN Epoch [90/100], D_loss: -0.0385, G_loss: -1.4676\n",
            "WGAN Epoch [100/100], D_loss: -0.1129, G_loss: -1.5629\n",
            "WGAN training finished.\n",
            "\n",
            "--- Training LSTM on WGAN Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0028\n",
            "Epoch [20/100], Loss: 0.0027\n",
            "Epoch [30/100], Loss: 0.0011\n",
            "Epoch [40/100], Loss: 0.0036\n",
            "Epoch [50/100], Loss: 0.0012\n",
            "Epoch [60/100], Loss: 0.0021\n",
            "Epoch [70/100], Loss: 0.0005\n",
            "Epoch [80/100], Loss: 0.0024\n",
            "Epoch [90/100], Loss: 0.0028\n",
            "Epoch [100/100], Loss: 0.0008\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (WGAN Hybrid): {'MSE': 0.00020494080672506243, 'RMSE': 0.014315753795209753, 'R2': 0.9990876913070679, 'Directional Accuracy': 1.0, 'Prediction Variance': 0.036722522}\n",
            "\n",
            "--- Generating SMOTE-TS Synthetic Data ---\n",
            "\n",
            "--- Training LSTM on SMOTE-TS Hybrid Data ---\n",
            "Starting training for 100 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0065\n",
            "Epoch [20/100], Loss: 0.0040\n",
            "Epoch [30/100], Loss: 0.0021\n",
            "Epoch [40/100], Loss: 0.0016\n",
            "Epoch [50/100], Loss: 0.0035\n",
            "Epoch [60/100], Loss: 0.0017\n",
            "Epoch [70/100], Loss: 0.0025\n",
            "Epoch [80/100], Loss: 0.0016\n",
            "Epoch [90/100], Loss: 0.0026\n",
            "Epoch [100/100], Loss: 0.0019\n",
            "Training finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laplace Approximation Metrics (SMOTE-TS Hybrid): {'MSE': 0.0011859240476042032, 'RMSE': 0.0344372479679228, 'R2': 0.9945654273033142, 'Directional Accuracy': 0.9731543624161074, 'Prediction Variance': 0.0362118}\n",
            "\n",
            "==================================================\n",
            "--- Running Trial 3/5 ---\n",
            "==================================================\n",
            "\n",
            "--- Training LSTM on Real Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0039\n",
            "Epoch [20/100], Loss: 0.0055\n",
            "Epoch [30/100], Loss: 0.0056\n",
            "Epoch [40/100], Loss: 0.0022\n",
            "Epoch [50/100], Loss: 0.0041\n",
            "Epoch [60/100], Loss: 0.0022\n",
            "Epoch [70/100], Loss: 0.0014\n",
            "Epoch [80/100], Loss: 0.0020\n",
            "Epoch [90/100], Loss: 0.0045\n",
            "Epoch [100/100], Loss: 0.0011\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (Real Data): {'MSE': 0.0034363730810582638, 'RMSE': 0.05862058581299119, 'R2': 0.879754900932312, 'Directional Accuracy': 0.4429530201342282, 'Prediction Variance': 0.07856027}\n",
            "\n",
            "--- Generating WGAN Synthetic Data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WGAN Epoch [10/100], D_loss: -0.7800, G_loss: 0.4649\n",
            "WGAN Epoch [20/100], D_loss: -0.0112, G_loss: -0.2302\n",
            "WGAN Epoch [30/100], D_loss: -0.0636, G_loss: -0.2488\n",
            "WGAN Epoch [40/100], D_loss: -0.2508, G_loss: 0.3766\n",
            "WGAN Epoch [50/100], D_loss: 0.1222, G_loss: -0.1571\n",
            "WGAN Epoch [60/100], D_loss: -0.0947, G_loss: -0.1259\n",
            "WGAN Epoch [70/100], D_loss: -0.1059, G_loss: -0.7708\n",
            "WGAN Epoch [80/100], D_loss: -0.0694, G_loss: -0.8324\n",
            "WGAN Epoch [90/100], D_loss: -0.0951, G_loss: -0.9986\n",
            "WGAN Epoch [100/100], D_loss: 0.0318, G_loss: -1.1830\n",
            "WGAN training finished.\n",
            "\n",
            "--- Training LSTM on WGAN Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0057\n",
            "Epoch [20/100], Loss: 0.0039\n",
            "Epoch [30/100], Loss: 0.0028\n",
            "Epoch [40/100], Loss: 0.0017\n",
            "Epoch [50/100], Loss: 0.0040\n",
            "Epoch [60/100], Loss: 0.0006\n",
            "Epoch [70/100], Loss: 0.0013\n",
            "Epoch [80/100], Loss: 0.0025\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [100/100], Loss: 0.0004\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (WGAN Hybrid): {'MSE': 5.723411959479563e-05, 'RMSE': 0.007565323495713559, 'R2': 0.9997432827949524, 'Directional Accuracy': 1.0, 'Prediction Variance': 0.038538836}\n",
            "\n",
            "--- Generating SMOTE-TS Synthetic Data ---\n",
            "\n",
            "--- Training LSTM on SMOTE-TS Hybrid Data ---\n",
            "Starting training for 100 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0024\n",
            "Epoch [20/100], Loss: 0.0025\n",
            "Epoch [30/100], Loss: 0.0040\n",
            "Epoch [40/100], Loss: 0.0027\n",
            "Epoch [50/100], Loss: 0.0022\n",
            "Epoch [60/100], Loss: 0.0011\n",
            "Epoch [70/100], Loss: 0.0027\n",
            "Epoch [80/100], Loss: 0.0028\n",
            "Epoch [90/100], Loss: 0.0057\n",
            "Epoch [100/100], Loss: 0.0012\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (SMOTE-TS Hybrid): {'MSE': 0.0015442036092281342, 'RMSE': 0.03929635618257925, 'R2': 0.994144856929779, 'Directional Accuracy': 0.9731543624161074, 'Prediction Variance': 0.036723703}\n",
            "\n",
            "==================================================\n",
            "--- Running Trial 4/5 ---\n",
            "==================================================\n",
            "\n",
            "--- Training LSTM on Real Data ---\n",
            "Starting training for 100 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0023\n",
            "Epoch [20/100], Loss: 0.0077\n",
            "Epoch [30/100], Loss: 0.0038\n",
            "Epoch [40/100], Loss: 0.0029\n",
            "Epoch [50/100], Loss: 0.0021\n",
            "Epoch [60/100], Loss: 0.0018\n",
            "Epoch [70/100], Loss: 0.0037\n",
            "Epoch [80/100], Loss: 0.0020\n",
            "Epoch [90/100], Loss: 0.0019\n",
            "Epoch [100/100], Loss: 0.0018\n",
            "Training finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laplace Approximation Metrics (Real Data): {'MSE': 0.0033102293964475393, 'RMSE': 0.057534593041469746, 'R2': 0.8841689229011536, 'Directional Accuracy': 0.4429530201342282, 'Prediction Variance': 0.071940035}\n",
            "\n",
            "--- Generating WGAN Synthetic Data ---\n",
            "WGAN Epoch [10/100], D_loss: -0.9051, G_loss: 0.8061\n",
            "WGAN Epoch [20/100], D_loss: -0.1107, G_loss: 0.7090\n",
            "WGAN Epoch [30/100], D_loss: -0.1950, G_loss: 0.1922\n",
            "WGAN Epoch [40/100], D_loss: -0.1288, G_loss: -0.0369\n",
            "WGAN Epoch [50/100], D_loss: -0.1277, G_loss: 0.2949\n",
            "WGAN Epoch [60/100], D_loss: -0.0502, G_loss: -0.0808\n",
            "WGAN Epoch [70/100], D_loss: -0.1291, G_loss: -0.2381\n",
            "WGAN Epoch [80/100], D_loss: -0.0620, G_loss: 0.4623\n",
            "WGAN Epoch [90/100], D_loss: 0.1055, G_loss: -0.1610\n",
            "WGAN Epoch [100/100], D_loss: -0.0534, G_loss: -0.4722\n",
            "WGAN training finished.\n",
            "\n",
            "--- Training LSTM on WGAN Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0033\n",
            "Epoch [20/100], Loss: 0.0014\n",
            "Epoch [30/100], Loss: 0.0007\n",
            "Epoch [40/100], Loss: 0.0015\n",
            "Epoch [50/100], Loss: 0.0018\n",
            "Epoch [60/100], Loss: 0.0028\n",
            "Epoch [70/100], Loss: 0.0012\n",
            "Epoch [80/100], Loss: 0.0014\n",
            "Epoch [90/100], Loss: 0.0056\n",
            "Epoch [100/100], Loss: 0.0011\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (WGAN Hybrid): {'MSE': 4.4356977014103904e-05, 'RMSE': 0.006660103378634892, 'R2': 0.9998209476470947, 'Directional Accuracy': 0.9899328859060402, 'Prediction Variance': 0.039192054}\n",
            "\n",
            "--- Generating SMOTE-TS Synthetic Data ---\n",
            "\n",
            "--- Training LSTM on SMOTE-TS Hybrid Data ---\n",
            "Starting training for 100 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0085\n",
            "Epoch [20/100], Loss: 0.0021\n",
            "Epoch [30/100], Loss: 0.0023\n",
            "Epoch [40/100], Loss: 0.0007\n",
            "Epoch [50/100], Loss: 0.0034\n",
            "Epoch [60/100], Loss: 0.0005\n",
            "Epoch [70/100], Loss: 0.0015\n",
            "Epoch [80/100], Loss: 0.0006\n",
            "Epoch [90/100], Loss: 0.0008\n",
            "Epoch [100/100], Loss: 0.0026\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (SMOTE-TS Hybrid): {'MSE': 0.0011618542484939098, 'RMSE': 0.03408598316748264, 'R2': 0.9952079653739929, 'Directional Accuracy': 0.9765100671140939, 'Prediction Variance': 0.035374954}\n",
            "\n",
            "==================================================\n",
            "--- Running Trial 5/5 ---\n",
            "==================================================\n",
            "\n",
            "--- Training LSTM on Real Data ---\n",
            "Starting training for 100 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0061\n",
            "Epoch [20/100], Loss: 0.0028\n",
            "Epoch [30/100], Loss: 0.0038\n",
            "Epoch [40/100], Loss: 0.0015\n",
            "Epoch [50/100], Loss: 0.0024\n",
            "Epoch [60/100], Loss: 0.0019\n",
            "Epoch [70/100], Loss: 0.0034\n",
            "Epoch [80/100], Loss: 0.0023\n",
            "Epoch [90/100], Loss: 0.0024\n",
            "Epoch [100/100], Loss: 0.0012\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (Real Data): {'MSE': 0.0033450769260525703, 'RMSE': 0.057836639996221864, 'R2': 0.8829495310783386, 'Directional Accuracy': 0.4429530201342282, 'Prediction Variance': 0.07480829}\n",
            "\n",
            "--- Generating WGAN Synthetic Data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WGAN Epoch [10/100], D_loss: -0.7147, G_loss: 0.7180\n",
            "WGAN Epoch [20/100], D_loss: -0.2552, G_loss: 0.5557\n",
            "WGAN Epoch [30/100], D_loss: -0.2436, G_loss: 0.3649\n",
            "WGAN Epoch [40/100], D_loss: -0.0893, G_loss: 0.2212\n",
            "WGAN Epoch [50/100], D_loss: -0.0126, G_loss: 0.3015\n",
            "WGAN Epoch [60/100], D_loss: -0.1596, G_loss: -0.2555\n",
            "WGAN Epoch [70/100], D_loss: -0.1642, G_loss: 0.0554\n",
            "WGAN Epoch [80/100], D_loss: -0.0326, G_loss: -0.3289\n",
            "WGAN Epoch [90/100], D_loss: -0.1600, G_loss: 0.0008\n",
            "WGAN Epoch [100/100], D_loss: -0.0195, G_loss: -0.7538\n",
            "WGAN training finished.\n",
            "\n",
            "--- Training LSTM on WGAN Hybrid Data ---\n",
            "Starting training for 100 epochs...\n",
            "Epoch [10/100], Loss: 0.0068\n",
            "Epoch [20/100], Loss: 0.0060\n",
            "Epoch [30/100], Loss: 0.0048\n",
            "Epoch [40/100], Loss: 0.0005\n",
            "Epoch [50/100], Loss: 0.0013\n",
            "Epoch [60/100], Loss: 0.0007\n",
            "Epoch [70/100], Loss: 0.0007\n",
            "Epoch [80/100], Loss: 0.0012\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [100/100], Loss: 0.0048\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (WGAN Hybrid): {'MSE': 8.73083554324694e-05, 'RMSE': 0.009343894018687788, 'R2': 0.9996134638786316, 'Directional Accuracy': 1.0, 'Prediction Variance': 0.03491588}\n",
            "\n",
            "--- Generating SMOTE-TS Synthetic Data ---\n",
            "\n",
            "--- Training LSTM on SMOTE-TS Hybrid Data ---\n",
            "Starting training for 100 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0058\n",
            "Epoch [20/100], Loss: 0.0017\n",
            "Epoch [30/100], Loss: 0.0063\n",
            "Epoch [40/100], Loss: 0.0019\n",
            "Epoch [50/100], Loss: 0.0033\n",
            "Epoch [60/100], Loss: 0.0029\n",
            "Epoch [70/100], Loss: 0.0020\n",
            "Epoch [80/100], Loss: 0.0015\n",
            "Epoch [90/100], Loss: 0.0013\n",
            "Epoch [100/100], Loss: 0.0015\n",
            "Training finished.\n",
            "Laplace Approximation Metrics (SMOTE-TS Hybrid): {'MSE': 0.0012338380329310894, 'RMSE': 0.03512603070275788, 'R2': 0.9942607879638672, 'Directional Accuracy': 0.9697986577181208, 'Prediction Variance': 0.0348219}\n",
            "\n",
            "==================================================\n",
            "               Final Results Summary\n",
            "==================================================\n",
            "\n",
            "Model trained on real data (Average over 5 trials):\n",
            "  - MSE: 0.0034 +/- 0.0000\n",
            "  - RMSE: 0.0579 +/- 0.0004\n",
            "  - R2: 0.8827 +/- 0.0016\n",
            "  - Directional Accuracy: 0.4443 +/- 0.0027\n",
            "  - Prediction Variance: 0.0687 +/- 0.0083\n",
            "--------------------------------------------------\n",
            "Model trained on wgan hybrid (Average over 5 trials):\n",
            "  - MSE: 0.0001 +/- 0.0001\n",
            "  - RMSE: 0.0086 +/- 0.0031\n",
            "  - R2: 0.9996 +/- 0.0003\n",
            "  - Directional Accuracy: 0.9980 +/- 0.0040\n",
            "  - Prediction Variance: 0.0368 +/- 0.0018\n",
            "--------------------------------------------------\n",
            "Model trained on smote ts hybrid (Average over 5 trials):\n",
            "  - MSE: 0.0013 +/- 0.0001\n",
            "  - RMSE: 0.0356 +/- 0.0019\n",
            "  - R2: 0.9946 +/- 0.0004\n",
            "  - Directional Accuracy: 0.9745 +/- 0.0034\n",
            "  - Prediction Variance: 0.0356 +/- 0.0008\n",
            "--------------------------------------------------\n",
            "No results available for cyclegan hybrid.\n",
            "--------------------------------------------------\n",
            "\n",
            "==================================================\n",
            "          Statistical Comparisons (p-values < 0.05 are significant)\n",
            "==================================================\n",
            "\n",
            "Comparing MSE between models:\n",
            "  Paired T-test: statistic=90.8879, p-value=0.0000\n",
            "Comparing MSE between models:\n",
            "  Paired T-test: statistic=41.8971, p-value=0.0000\n",
            "Skipping comparison between Real Data and CycleGAN Hybrid: Insufficient data (CycleGAN experiment skipped or failed).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from scipy.stats import ttest_rel\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from laplace import Laplace\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =========================================================================================\n",
        "# 1. Helper Functions (Unchanged from previous versions)\n",
        "# =========================================================================================\n",
        "def set_seed(seed):\n",
        "    \"\"\"Sets a random seed for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        cudnn.benchmark = False\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "def get_stock_data(ticker, start, end):\n",
        "    \"\"\"Downloads stock data from Yahoo Finance.\"\"\"\n",
        "    print(f\"Downloading data for {ticker} from {start} to {end}...\")\n",
        "    df = yf.download(ticker, start=start, end=end, progress=False)\n",
        "    print(\"Data download finished.\")\n",
        "    df = df[['Close']].dropna()\n",
        "    return df\n",
        "\n",
        "def scale_data(data):\n",
        "    \"\"\"Scales data using MinMaxScaler and returns the scaled data and the scaler.\"\"\"\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n",
        "    return scaled_data, scaler\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    \"\"\"Creates sequences and labels for LSTM training.\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    \"\"\"Calculates and returns a dictionary of evaluation metrics.\"\"\"\n",
        "    y_true_flat = y_true.flatten()\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "    mse = mean_squared_error(y_true_flat, y_pred_flat)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true_flat, y_pred_flat)\n",
        "    y_true_diff = np.diff(y_true_flat)\n",
        "    y_pred_diff = np.diff(y_pred_flat)\n",
        "    if len(y_true_diff) > 0:\n",
        "        correct_direction = np.sum(np.sign(y_true_diff) == np.sign(y_pred_diff))\n",
        "        directional_accuracy = correct_direction / len(y_true_diff)\n",
        "    else:\n",
        "        directional_accuracy = np.nan\n",
        "    return {\"MSE\": mse, \"RMSE\": rmse, \"R2\": r2, \"Directional Accuracy\": directional_accuracy}\n",
        "\n",
        "# =========================================================================================\n",
        "# 2. LSTM and Laplace Approximation Functions (Unchanged)\n",
        "# =========================================================================================\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"A simple LSTM model for time series prediction.\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
        "        predictions = self.linear(lstm_out[:, -1, :])\n",
        "        return predictions\n",
        "\n",
        "def train_lstm_model_with_laplace(data, model_save_path, seq_length=10, epochs=100, patience=10, batch_size=32, lr=0.001):\n",
        "    \"\"\"Trains an LSTM model and applies Laplace approximation.\"\"\"\n",
        "    scaled_data, scaler = scale_data(data)\n",
        "    X, y = create_sequences(scaled_data, seq_length)\n",
        "\n",
        "    X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train_np, dtype=torch.float32).view(-1, 1)\n",
        "    X_val = torch.tensor(X_val_np, dtype=torch.float32)\n",
        "    y_val = torch.tensor(y_val_np, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = LSTMModel(input_size=X_train.shape[-1], hidden_size=50, output_size=1)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    print(f\"Starting training for {epochs} epochs with patience {patience}...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_x, batch_y in train_dataloader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val.to(device))\n",
        "            val_loss = criterion(val_outputs, y_val.to(device))\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Val Loss: {val_loss.item():.4f}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Validation loss improved. Model saved to {model_save_path}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == patience:\n",
        "                print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(model_save_path))\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Applying Laplace Approximation...\")\n",
        "    la_model = Laplace(model, 'regression', subset_of_weights='last_layer')\n",
        "    la_model.fit(train_dataloader)\n",
        "\n",
        "    X_full = torch.tensor(X, dtype=torch.float32)\n",
        "    y_full = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        f_mu, f_var = la_model(X_test.to(device))\n",
        "\n",
        "    predictions = f_mu.cpu().numpy()\n",
        "    y_test_np = y_test.cpu().numpy()\n",
        "\n",
        "    predictions_rescaled = scaler.inverse_transform(predictions)\n",
        "    y_test_rescaled = scaler.inverse_transform(y_test_np)\n",
        "\n",
        "    metrics = evaluate_model(y_test_rescaled, predictions_rescaled)\n",
        "    print(\"Evaluation finished on the final test set.\")\n",
        "    print(\"Evaluation Metrics:\", metrics)\n",
        "\n",
        "    return metrics, model, scaler\n",
        "\n",
        "# =========================================================================================\n",
        "# 3. WGAN-GP and SMOTE-TS Functions (Unchanged)\n",
        "# =========================================================================================\n",
        "class Generator_WGAN(nn.Module):\n",
        "    def __init__(self, latent_dim, seq_length, output_dim):\n",
        "        super(Generator_WGAN, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.latent_dim = latent_dim\n",
        "        self.main = nn.Sequential(nn.Linear(latent_dim, 256), nn.ReLU(True),\n",
        "                                  nn.Linear(256, 512), nn.ReLU(True),\n",
        "                                  nn.Linear(512, seq_length * output_dim), nn.Tanh())\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1, self.seq_length, 1)\n",
        "\n",
        "class Discriminator_WGAN(nn.Module):\n",
        "    def __init__(self, input_dim, seq_length):\n",
        "        super(Discriminator_WGAN, self).__init__()\n",
        "        self.main = nn.Sequential(nn.Linear(input_dim * seq_length, 512), nn.LeakyReLU(0.2, inplace=True),\n",
        "                                  nn.Linear(512, 256), nn.LeakyReLU(0.2, inplace=True),\n",
        "                                  nn.Linear(256, 1))\n",
        "    def forward(self, input):\n",
        "        input = input.view(input.size(0), -1)\n",
        "        return self.main(input)\n",
        "\n",
        "def train_wgan(generator, discriminator, real_data, latent_dim, device, epochs=50, batch_size=64, lr_g=0.0001, lr_d=0.0001, critic_iterations=5, lambda_gp=10):\n",
        "    optimizer_g = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.9))\n",
        "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.9))\n",
        "    generator.to(device)\n",
        "    discriminator.to(device)\n",
        "    print(\"Starting WGAN training...\")\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    for epoch in range(epochs):\n",
        "        permutation = torch.randperm(real_data.size(0))\n",
        "        for i in range(0, real_data.size(0), batch_size):\n",
        "            indices = permutation[i:i + batch_size]\n",
        "            real_batch = real_data[indices].to(device)\n",
        "            for _ in range(critic_iterations):\n",
        "                discriminator.zero_grad()\n",
        "                d_real = discriminator(real_batch)\n",
        "                z = torch.randn(real_batch.size(0), latent_dim).to(device)\n",
        "                fake_batch = generator(z).detach()\n",
        "                d_fake = discriminator(fake_batch)\n",
        "                loss_d_wasserstein = d_fake.mean() - d_real.mean()\n",
        "                alpha = torch.rand(real_batch.size(0), 1, 1).to(device)\n",
        "                interpolates = (alpha * real_batch + ((1 - alpha) * fake_batch)).requires_grad_(True)\n",
        "                d_interpolates = discriminator(interpolates)\n",
        "                gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates,\n",
        "                                                grad_outputs=torch.ones_like(d_interpolates),\n",
        "                                                create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "                gradients = gradients.view(gradients.size(0), -1)\n",
        "                gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp\n",
        "                loss_d = loss_d_wasserstein + gradient_penalty\n",
        "                loss_d.backward()\n",
        "                optimizer_d.step()\n",
        "            generator.zero_grad()\n",
        "            z = torch.randn(batch_size, latent_dim).to(device)\n",
        "            fake_batch = generator(z)\n",
        "            d_fake = discriminator(fake_batch)\n",
        "            loss_g = -d_fake.mean()\n",
        "            loss_g.backward()\n",
        "            optimizer_g.step()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], D_loss: {loss_d.item():.4f}, G_loss: {loss_g.item():.4f}\")\n",
        "    torch.backends.cudnn.enabled = True\n",
        "    print(\"WGAN training finished.\")\n",
        "    return generator\n",
        "\n",
        "def generate_wgan_data(generator, real_data_df, real_scaler, latent_dim, seq_length, device):\n",
        "    num_synthetic_samples = len(real_data_df) - seq_length\n",
        "    noise = torch.randn(num_synthetic_samples, latent_dim).to(device)\n",
        "    with torch.no_grad():\n",
        "        synthetic_sequences = generator(noise).cpu().numpy()\n",
        "    synthetic_data_long = synthetic_sequences.reshape(-1, 1)\n",
        "    return pd.DataFrame(real_scaler.inverse_transform(synthetic_data_long), columns=['Close'])\n",
        "\n",
        "def smote_ts(data, seq_length, n_samples):\n",
        "    X, y = create_sequences(data, seq_length)\n",
        "    if len(X) == 0:\n",
        "        return pd.DataFrame(columns=['Close'])\n",
        "    X_smote = []\n",
        "\n",
        "    for _ in tqdm(range(n_samples), desc=\"Generating SMOTE-TS data\"):\n",
        "        idx = np.random.randint(0, len(X))\n",
        "        sequence1 = X[idx].squeeze()\n",
        "\n",
        "        neighbor_idx = np.random.randint(0, len(X))\n",
        "        sequence2 = X[neighbor_idx].squeeze()\n",
        "\n",
        "        alpha = np.random.rand()\n",
        "        new_sequence = alpha * sequence1 + (1 - alpha) * sequence2\n",
        "        X_smote.append(new_sequence)\n",
        "\n",
        "    synthetic_data = np.array(X_smote).reshape(-1, 1)\n",
        "    return pd.DataFrame(synthetic_data, columns=['Close'])\n",
        "\n",
        "# =========================================================================================\n",
        "# 4. CycleGAN Implementation (Alternative Simplified)\n",
        "# =========================================================================================\n",
        "class CycleGANGenerator(nn.Module):\n",
        "    def __init__(self, input_dim=1, output_dim=1):\n",
        "        super(CycleGANGenerator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv1d(input_dim, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(64, output_dim, kernel_size=3, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.model(x)\n",
        "        return x.permute(0, 2, 1)\n",
        "\n",
        "class CycleGANDiscriminator(nn.Module):\n",
        "    def __init__(self, input_dim=1):\n",
        "        super(CycleGANDiscriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.relu1 = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.relu2 = nn.LeakyReLU(0.2, inplace=True)\n",
        "        # This layer should receive 128 input channels from conv2\n",
        "        self.conv3 = nn.Conv1d(128, 1, kernel_size=3, padding=1)\n",
        "        self.relu3 = nn.LeakyReLU(0.2, inplace=True) # Added activation\n",
        "        # Removed Flatten and Linear for debugging\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1) # Permute to (batch_size, channels, sequence_length)\n",
        "        print(f\"Shape after permute: {x.shape}\")\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        print(f\"Shape after conv1 and relu1: {x.shape}\")\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        print(f\"Shape after conv2 and relu2: {x.shape}\")\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x) # Apply activation\n",
        "        print(f\"Shape after conv3 and relu3: {x.shape}\")\n",
        "        return x # Return the output of the convolutional layers\n",
        "\n",
        "def train_cyclegan_simplified(gen_AB, gen_BA, disc_A, disc_B, data_A, data_B, device, epochs=100, lr=0.0002, lambda_cycle=10.0, lambda_identity=5.0):\n",
        "    gen_AB.to(device)\n",
        "    gen_BA.to(device)\n",
        "    disc_A.to(device)\n",
        "    disc_B.to(device)\n",
        "\n",
        "    optimizer_G = optim.Adam(list(gen_AB.parameters()) + list(gen_BA.parameters()), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_D_A = optim.Adam(disc_A.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_D_B = optim.Adam(disc_B.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    criterion_GAN = nn.MSELoss()\n",
        "    criterion_cycle = nn.L1Loss()\n",
        "    criterion_identity = nn.L1Loss()\n",
        "\n",
        "    print(\"Starting Simplified CycleGAN training...\")\n",
        "\n",
        "    dataset_A = TensorDataset(data_A.to(device))\n",
        "    dataloader_A = DataLoader(dataset_A, batch_size=64, shuffle=True)\n",
        "    dataset_B = TensorDataset(data_B.to(device))\n",
        "    dataloader_B = DataLoader(dataset_B, batch_size=64, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, (real_A,) in enumerate(dataloader_A):\n",
        "            try:\n",
        "                real_B = next(iter_b)[0]\n",
        "            except (StopIteration, NameError):\n",
        "                iter_b = iter(dataloader_B)\n",
        "                real_B = next(iter_b)[0]\n",
        "\n",
        "            # --- Train Generators (Gen_A to B, Gen_B to A) ---\n",
        "            gen_AB.train()\n",
        "            gen_BA.train()\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Forward pass through generators\n",
        "            fake_B = gen_AB(real_A)\n",
        "            cycle_A = gen_BA(fake_B)\n",
        "            fake_A = gen_BA(real_B)\n",
        "            cycle_B = gen_AB(fake_A)\n",
        "\n",
        "            # GAN loss\n",
        "            # Need to adjust target shape for GAN loss as Discriminator output shape has changed\n",
        "            valid_gan_target = torch.ones_like(disc_B(fake_B))\n",
        "            fake_gan_target = torch.zeros_like(disc_B(fake_B))\n",
        "\n",
        "            loss_gan_AB = criterion_GAN(disc_B(fake_B), valid_gan_target)\n",
        "            loss_gan_BA = criterion_GAN(disc_A(fake_A), valid_gan_target)\n",
        "\n",
        "            # Cycle loss\n",
        "            loss_cycle_A = criterion_cycle(cycle_A, real_A) * lambda_cycle\n",
        "            loss_cycle_B = criterion_cycle(cycle_B, real_B) * lambda_cycle\n",
        "\n",
        "            # Identity loss\n",
        "            loss_identity_A = criterion_identity(gen_BA(real_A), real_A) * lambda_identity\n",
        "            loss_identity_B = criterion_identity(gen_AB(real_B), real_B) * lambda_identity\n",
        "\n",
        "            # Total Generator Loss\n",
        "            loss_G = loss_gan_AB + loss_gan_BA + loss_cycle_A + loss_cycle_B + loss_identity_A + loss_identity_B\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # --- Train Discriminators ---\n",
        "\n",
        "            # Train Discriminator A\n",
        "            optimizer_D_A.zero_grad()\n",
        "            d_real_A = disc_A(real_A)\n",
        "            d_fake_A = disc_A(fake_A.detach())\n",
        "            loss_D_A = (criterion_GAN(d_real_A, torch.ones_like(d_real_A)) + criterion_GAN(d_fake_A, torch.zeros_like(d_fake_A))) / 2 # Use adjusted targets\n",
        "            loss_D_A.backward()\n",
        "            optimizer_D_A.step()\n",
        "\n",
        "            # Train Discriminator B\n",
        "            optimizer_D_B.zero_grad()\n",
        "            d_real_B = disc_B(real_B)\n",
        "            d_fake_B = disc_B(fake_B.detach())\n",
        "            loss_D_B = (criterion_GAN(d_real_B, torch.ones_like(d_real_B)) + criterion_GAN(d_fake_B, torch.zeros_like(d_fake_B))) / 2 # Use adjusted targets\n",
        "            loss_D_B.backward()\n",
        "            optimizer_D_B.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], G_loss: {loss_G.item():.4f}, D_A_loss: {loss_D_A.item():.4f}, D_B_loss: {loss_D_B.item():.4f}\")\n",
        "\n",
        "    print(\"Simplified CycleGAN training finished.\")\n",
        "    return gen_AB, gen_BA\n",
        "\n",
        "def generate_cyclegan_data(generator, real_data_df, real_scaler, seq_length, device):\n",
        "    scaled_data, _ = scale_data(real_data_df)\n",
        "    X, _ = create_sequences(scaled_data, seq_length)\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32).view(-1, SEQ_LENGTH, 1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        synthetic_sequences = generator(X_tensor).cpu().numpy()\n",
        "\n",
        "    synthetic_data_long = synthetic_sequences.reshape(-1, 1)\n",
        "    return pd.DataFrame(real_scaler.inverse_transform(synthetic_data_long), columns=['Close'])\n",
        "\n",
        "# =========================================================================================\n",
        "# 5. Main Execution\n",
        "# =========================================================================================\n",
        "if __name__ == '__main__':\n",
        "    set_seed(42)\n",
        "\n",
        "    TICKER_A = \"AAPL\"\n",
        "    TICKER_B = \"MSFT\" # A second ticker for CycleGAN\n",
        "    START_DATE = \"2020-01-01\"\n",
        "    END_DATE = \"2023-01-01\"\n",
        "    SEQ_LENGTH = 10\n",
        "    LATENT_DIM = 100\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    real_data_df_A = get_stock_data(TICKER_A, START_DATE, END_DATE)\n",
        "    scaled_real_data_A, real_scaler_A = scale_data(real_data_df_A)\n",
        "\n",
        "    real_data_df_B = get_stock_data(TICKER_B, START_DATE, END_DATE)\n",
        "    scaled_real_data_B, real_scaler_B = scale_data(real_data_df_B)\n",
        "\n",
        "    X_real_wgan_A, _ = create_sequences(scaled_real_data_A, SEQ_LENGTH)\n",
        "    X_real_wgan_A = torch.tensor(X_real_wgan_A, dtype=torch.float32)\n",
        "\n",
        "    print(\"\\nTraining WGAN-GP...\")\n",
        "    generator_wgan = Generator_WGAN(latent_dim=LATENT_DIM, seq_length=SEQ_LENGTH, output_dim=1)\n",
        "    discriminator_wgan = Discriminator_WGAN(input_dim=1, seq_length=SEQ_LENGTH)\n",
        "    trained_generator_wgan = train_wgan(generator_wgan, discriminator_wgan, X_real_wgan_A, latent_dim=LATENT_DIM, device=device)\n",
        "    synthetic_wgan_df = generate_wgan_data(trained_generator_wgan, real_data_df_A, real_scaler_A, LATENT_DIM, SEQ_LENGTH, device)\n",
        "\n",
        "    print(\"\\nTraining Simplified CycleGAN...\")\n",
        "    X_real_cycle_A, _ = create_sequences(scaled_real_data_A, SEQ_LENGTH)\n",
        "    X_real_cycle_B, _ = create_sequences(scaled_real_data_B, SEQ_LENGTH)\n",
        "    X_real_cycle_A = torch.tensor(X_real_cycle_A, dtype=torch.float32).view(-1, SEQ_LENGTH, 1)\n",
        "    X_real_cycle_B = torch.tensor(X_real_cycle_B, dtype=torch.float32).view(-1, SEQ_LENGTH, 1)\n",
        "\n",
        "    gen_AB = CycleGANGenerator(input_dim=1, output_dim=1)\n",
        "    gen_BA = CycleGANGenerator(input_dim=1, output_dim=1)\n",
        "    disc_A = CycleGANDiscriminator(input_dim=1)\n",
        "    disc_B = CycleGANDiscriminator(input_dim=1)\n",
        "    gen_AB, gen_BA = train_cyclegan_simplified(gen_AB, gen_BA, disc_A, disc_B, X_real_cycle_A, X_real_cycle_B, device)\n",
        "    synthetic_cyclegan_df = generate_cyclegan_data(gen_AB, real_data_df_A, real_scaler_A, SEQ_LENGTH, device)\n",
        "\n",
        "    print(\"\\nGenerating SMOTE-TS data...\")\n",
        "    synthetic_smote_df = smote_ts(scaled_real_data_A, SEQ_LENGTH, n_samples=len(real_data_df_A))\n",
        "    synthetic_smote_df = pd.DataFrame(real_scaler_A.inverse_transform(synthetic_smote_df.values), columns=['Close'])\n",
        "\n",
        "    print(\"\\nTraining LSTM on REAL data...\")\n",
        "    real_metrics, _, _ = train_lstm_model_with_laplace(data=real_data_df_A, model_save_path='lstm_model_real_best.pth', seq_length=SEQ_LENGTH)\n",
        "\n",
        "    print(\"\\nTraining LSTM on WGAN-GP data...\")\n",
        "    synthetic_wgan_metrics, _, _ = train_lstm_model_with_laplace(data=synthetic_wgan_df, model_save_path='lstm_model_wgan_best.pth', seq_length=SEQ_LENGTH)\n",
        "\n",
        "    print(\"\\nTraining LSTM on SMOTE-TS data...\")\n",
        "    synthetic_smote_metrics, _, _ = train_lstm_model_with_laplace(data=synthetic_smote_df, model_save_path='lstm_model_smote_best.pth', seq_length=SEQ_LENGTH)\n",
        "\n",
        "    print(\"\\nTraining LSTM on CycleGAN data...\")\n",
        "    synthetic_cyclegan_metrics, _, _ = train_lstm_model_with_laplace(data=synthetic_cyclegan_df, model_save_path='lstm_model_cyclegan_best.pth', seq_length=SEQ_LENGTH)\n",
        "\n",
        "    print(\"\\nFinal Results Comparison:\")\n",
        "    print(f\"Metrics from LSTM on REAL data: {real_metrics}\")\n",
        "    print(f\"Metrics from LSTM on WGAN-GP data: {synthetic_wgan_metrics}\")\n",
        "    print(f\"Metrics from LSTM on SMOTE-TS data: {synthetic_smote_metrics}\")\n",
        "    print(f\"Metrics from LSTM on CycleGAN data: {synthetic_cyclegan_metrics}\")"
      ],
      "metadata": {
        "id": "byWKRpManQBI",
        "outputId": "1a3302ff-dea9-4a34-84ba-e4caf1eb31df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for AAPL from 2020-01-01 to 2023-01-01...\n",
            "Data download finished.\n",
            "Downloading data for MSFT from 2020-01-01 to 2023-01-01...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-916598548.py:33: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=start, end=end, progress=False)\n",
            "/tmp/ipython-input-916598548.py:33: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=start, end=end, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Epoch [90/100], G_loss: 1.2784, D_A_loss: 0.2461, D_B_loss: 0.2458\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([64, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([64, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([64, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([64, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Shape after permute: torch.Size([42, 1, 10])\n",
            "Shape after conv1 and relu1: torch.Size([42, 64, 5])\n",
            "Shape after conv2 and relu2: torch.Size([42, 128, 3])\n",
            "Shape after conv3 and relu3: torch.Size([42, 1, 3])\n",
            "Epoch [100/100], G_loss: 1.2285, D_A_loss: 0.2189, D_B_loss: 0.2183\n",
            "Simplified CycleGAN training finished.\n",
            "\n",
            "Generating SMOTE-TS data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating SMOTE-TS data: 100%|██████████| 756/756 [00:00<00:00, 42476.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training LSTM on REAL data...\n",
            "Using device: cuda\n",
            "Starting training for 100 epochs with patience 10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Val Loss: 0.1827\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [2/100], Val Loss: 0.0504\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [3/100], Val Loss: 0.0071\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [4/100], Val Loss: 0.0057\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [5/100], Val Loss: 0.0054\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [6/100], Val Loss: 0.0051\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [7/100], Val Loss: 0.0048\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [8/100], Val Loss: 0.0047\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [9/100], Val Loss: 0.0045\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [10/100], Val Loss: 0.0049\n",
            "Epoch [11/100], Val Loss: 0.0044\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [12/100], Val Loss: 0.0043\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [13/100], Val Loss: 0.0042\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [14/100], Val Loss: 0.0040\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [15/100], Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [16/100], Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [17/100], Val Loss: 0.0040\n",
            "Epoch [18/100], Val Loss: 0.0039\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [19/100], Val Loss: 0.0036\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [20/100], Val Loss: 0.0036\n",
            "Epoch [21/100], Val Loss: 0.0035\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [22/100], Val Loss: 0.0037\n",
            "Epoch [23/100], Val Loss: 0.0036\n",
            "Epoch [24/100], Val Loss: 0.0040\n",
            "Epoch [25/100], Val Loss: 0.0032\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [26/100], Val Loss: 0.0032\n",
            "Epoch [27/100], Val Loss: 0.0032\n",
            "Epoch [28/100], Val Loss: 0.0033\n",
            "Epoch [29/100], Val Loss: 0.0040\n",
            "Epoch [30/100], Val Loss: 0.0030\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [31/100], Val Loss: 0.0031\n",
            "Epoch [32/100], Val Loss: 0.0029\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [33/100], Val Loss: 0.0035\n",
            "Epoch [34/100], Val Loss: 0.0029\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [35/100], Val Loss: 0.0028\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [36/100], Val Loss: 0.0030\n",
            "Epoch [37/100], Val Loss: 0.0029\n",
            "Epoch [38/100], Val Loss: 0.0027\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [39/100], Val Loss: 0.0029\n",
            "Epoch [40/100], Val Loss: 0.0032\n",
            "Epoch [41/100], Val Loss: 0.0026\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [42/100], Val Loss: 0.0026\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [43/100], Val Loss: 0.0028\n",
            "Epoch [44/100], Val Loss: 0.0027\n",
            "Epoch [45/100], Val Loss: 0.0025\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [46/100], Val Loss: 0.0026\n",
            "Epoch [47/100], Val Loss: 0.0029\n",
            "Epoch [48/100], Val Loss: 0.0025\n",
            "Epoch [49/100], Val Loss: 0.0027\n",
            "Epoch [50/100], Val Loss: 0.0025\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [51/100], Val Loss: 0.0024\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [52/100], Val Loss: 0.0026\n",
            "Epoch [53/100], Val Loss: 0.0028\n",
            "Epoch [54/100], Val Loss: 0.0030\n",
            "Epoch [55/100], Val Loss: 0.0026\n",
            "Epoch [56/100], Val Loss: 0.0024\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [57/100], Val Loss: 0.0026\n",
            "Epoch [58/100], Val Loss: 0.0024\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [59/100], Val Loss: 0.0024\n",
            "Epoch [60/100], Val Loss: 0.0023\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [61/100], Val Loss: 0.0026\n",
            "Epoch [62/100], Val Loss: 0.0023\n",
            "Epoch [63/100], Val Loss: 0.0023\n",
            "Epoch [64/100], Val Loss: 0.0023\n",
            "Epoch [65/100], Val Loss: 0.0023\n",
            "Epoch [66/100], Val Loss: 0.0023\n",
            "Epoch [67/100], Val Loss: 0.0024\n",
            "Epoch [68/100], Val Loss: 0.0025\n",
            "Epoch [69/100], Val Loss: 0.0028\n",
            "Epoch [70/100], Val Loss: 0.0022\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [71/100], Val Loss: 0.0021\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [72/100], Val Loss: 0.0022\n",
            "Epoch [73/100], Val Loss: 0.0022\n",
            "Epoch [74/100], Val Loss: 0.0023\n",
            "Epoch [75/100], Val Loss: 0.0021\n",
            "Epoch [76/100], Val Loss: 0.0026\n",
            "Epoch [77/100], Val Loss: 0.0024\n",
            "Epoch [78/100], Val Loss: 0.0028\n",
            "Epoch [79/100], Val Loss: 0.0021\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [80/100], Val Loss: 0.0021\n",
            "Epoch [81/100], Val Loss: 0.0021\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [82/100], Val Loss: 0.0022\n",
            "Epoch [83/100], Val Loss: 0.0021\n",
            "Epoch [84/100], Val Loss: 0.0022\n",
            "Epoch [85/100], Val Loss: 0.0023\n",
            "Epoch [86/100], Val Loss: 0.0021\n",
            "Epoch [87/100], Val Loss: 0.0021\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [88/100], Val Loss: 0.0020\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [89/100], Val Loss: 0.0024\n",
            "Epoch [90/100], Val Loss: 0.0023\n",
            "Epoch [91/100], Val Loss: 0.0021\n",
            "Epoch [92/100], Val Loss: 0.0020\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [93/100], Val Loss: 0.0031\n",
            "Epoch [94/100], Val Loss: 0.0023\n",
            "Epoch [95/100], Val Loss: 0.0023\n",
            "Epoch [96/100], Val Loss: 0.0021\n",
            "Epoch [97/100], Val Loss: 0.0020\n",
            "Epoch [98/100], Val Loss: 0.0022\n",
            "Epoch [99/100], Val Loss: 0.0020\n",
            "Validation loss improved. Model saved to lstm_model_real_best.pth\n",
            "Epoch [100/100], Val Loss: 0.0021\n",
            "Applying Laplace Approximation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation finished on the final test set.\n",
            "Evaluation Metrics: {'MSE': 7.706765174865723, 'RMSE': 2.776106117363982, 'R2': 0.9919368624687195, 'Directional Accuracy': 0.9463087248322147}\n",
            "\n",
            "Training LSTM on WGAN-GP data...\n",
            "Using device: cuda\n",
            "Starting training for 100 epochs with patience 10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Val Loss: 0.0708\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [2/100], Val Loss: 0.0587\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [3/100], Val Loss: 0.0556\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [4/100], Val Loss: 0.0536\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [5/100], Val Loss: 0.0533\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [6/100], Val Loss: 0.0540\n",
            "Epoch [7/100], Val Loss: 0.0542\n",
            "Epoch [8/100], Val Loss: 0.0528\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [9/100], Val Loss: 0.0519\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [10/100], Val Loss: 0.0495\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [11/100], Val Loss: 0.0503\n",
            "Epoch [12/100], Val Loss: 0.0479\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [13/100], Val Loss: 0.0470\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [14/100], Val Loss: 0.0473\n",
            "Epoch [15/100], Val Loss: 0.0492\n",
            "Epoch [16/100], Val Loss: 0.0444\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [17/100], Val Loss: 0.0440\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [18/100], Val Loss: 0.0436\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [19/100], Val Loss: 0.0422\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [20/100], Val Loss: 0.0422\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [21/100], Val Loss: 0.0403\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [22/100], Val Loss: 0.0439\n",
            "Epoch [23/100], Val Loss: 0.0431\n",
            "Epoch [24/100], Val Loss: 0.0393\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [25/100], Val Loss: 0.0391\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [26/100], Val Loss: 0.0381\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [27/100], Val Loss: 0.0387\n",
            "Epoch [28/100], Val Loss: 0.0384\n",
            "Epoch [29/100], Val Loss: 0.0371\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [30/100], Val Loss: 0.0373\n",
            "Epoch [31/100], Val Loss: 0.0378\n",
            "Epoch [32/100], Val Loss: 0.0366\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [33/100], Val Loss: 0.0372\n",
            "Epoch [34/100], Val Loss: 0.0358\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [35/100], Val Loss: 0.0354\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [36/100], Val Loss: 0.0344\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [37/100], Val Loss: 0.0393\n",
            "Epoch [38/100], Val Loss: 0.0353\n",
            "Epoch [39/100], Val Loss: 0.0412\n",
            "Epoch [40/100], Val Loss: 0.0340\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [41/100], Val Loss: 0.0347\n",
            "Epoch [42/100], Val Loss: 0.0337\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [43/100], Val Loss: 0.0341\n",
            "Epoch [44/100], Val Loss: 0.0336\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [45/100], Val Loss: 0.0339\n",
            "Epoch [46/100], Val Loss: 0.0330\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [47/100], Val Loss: 0.0327\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [48/100], Val Loss: 0.0343\n",
            "Epoch [49/100], Val Loss: 0.0346\n",
            "Epoch [50/100], Val Loss: 0.0327\n",
            "Epoch [51/100], Val Loss: 0.0333\n",
            "Epoch [52/100], Val Loss: 0.0325\n",
            "Validation loss improved. Model saved to lstm_model_wgan_best.pth\n",
            "Epoch [53/100], Val Loss: 0.0329\n",
            "Epoch [54/100], Val Loss: 0.0334\n",
            "Epoch [55/100], Val Loss: 0.0335\n",
            "Epoch [56/100], Val Loss: 0.0336\n",
            "Epoch [57/100], Val Loss: 0.0332\n",
            "Epoch [58/100], Val Loss: 0.0328\n",
            "Epoch [59/100], Val Loss: 0.0338\n",
            "Epoch [60/100], Val Loss: 0.0335\n",
            "Epoch [61/100], Val Loss: 0.0341\n",
            "Epoch [62/100], Val Loss: 0.0334\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Applying Laplace Approximation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation finished on the final test set.\n",
            "Evaluation Metrics: {'MSE': 111.48689270019531, 'RMSE': 10.558735374096432, 'R2': 0.863612711429596, 'Directional Accuracy': 0.9066487575554063}\n",
            "\n",
            "Training LSTM on SMOTE-TS data...\n",
            "Using device: cuda\n",
            "Starting training for 100 epochs with patience 10...\n",
            "Epoch [1/100], Val Loss: 0.0335\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [2/100], Val Loss: 0.0245\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [3/100], Val Loss: 0.0240\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [4/100], Val Loss: 0.0213\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [5/100], Val Loss: 0.0219\n",
            "Epoch [6/100], Val Loss: 0.0208\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [7/100], Val Loss: 0.0210\n",
            "Epoch [8/100], Val Loss: 0.0209\n",
            "Epoch [9/100], Val Loss: 0.0212\n",
            "Epoch [10/100], Val Loss: 0.0212\n",
            "Epoch [11/100], Val Loss: 0.0206\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [12/100], Val Loss: 0.0205\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [13/100], Val Loss: 0.0220\n",
            "Epoch [14/100], Val Loss: 0.0204\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [15/100], Val Loss: 0.0203\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [16/100], Val Loss: 0.0203\n",
            "Epoch [17/100], Val Loss: 0.0214\n",
            "Epoch [18/100], Val Loss: 0.0202\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [19/100], Val Loss: 0.0203\n",
            "Epoch [20/100], Val Loss: 0.0212\n",
            "Epoch [21/100], Val Loss: 0.0212\n",
            "Epoch [22/100], Val Loss: 0.0208\n",
            "Epoch [23/100], Val Loss: 0.0203\n",
            "Epoch [24/100], Val Loss: 0.0198\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [25/100], Val Loss: 0.0202\n",
            "Epoch [26/100], Val Loss: 0.0199\n",
            "Epoch [27/100], Val Loss: 0.0204\n",
            "Epoch [28/100], Val Loss: 0.0205\n",
            "Epoch [29/100], Val Loss: 0.0205\n",
            "Epoch [30/100], Val Loss: 0.0198\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [31/100], Val Loss: 0.0197\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [32/100], Val Loss: 0.0196\n",
            "Validation loss improved. Model saved to lstm_model_smote_best.pth\n",
            "Epoch [33/100], Val Loss: 0.0201\n",
            "Epoch [34/100], Val Loss: 0.0203\n",
            "Epoch [35/100], Val Loss: 0.0198\n",
            "Epoch [36/100], Val Loss: 0.0199\n",
            "Epoch [37/100], Val Loss: 0.0202\n",
            "Epoch [38/100], Val Loss: 0.0197\n",
            "Epoch [39/100], Val Loss: 0.0197\n",
            "Epoch [40/100], Val Loss: 0.0196\n",
            "Epoch [41/100], Val Loss: 0.0198\n",
            "Epoch [42/100], Val Loss: 0.0202\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Applying Laplace Approximation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation finished on the final test set.\n",
            "Evaluation Metrics: {'MSE': 70.21871948242188, 'RMSE': 8.379661060115849, 'R2': 0.8800274729728699, 'Directional Accuracy': 0.9324055666003976}\n",
            "\n",
            "Training LSTM on CycleGAN data...\n",
            "Using device: cuda\n",
            "Starting training for 100 epochs with patience 10...\n",
            "Epoch [1/100], Val Loss: 0.0031\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [2/100], Val Loss: 0.0029\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [3/100], Val Loss: 0.0028\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [4/100], Val Loss: 0.0029\n",
            "Epoch [5/100], Val Loss: 0.0026\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [6/100], Val Loss: 0.0024\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [7/100], Val Loss: 0.0029\n",
            "Epoch [8/100], Val Loss: 0.0023\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [9/100], Val Loss: 0.0025\n",
            "Epoch [10/100], Val Loss: 0.0017\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [11/100], Val Loss: 0.0019\n",
            "Epoch [12/100], Val Loss: 0.0018\n",
            "Epoch [13/100], Val Loss: 0.0018\n",
            "Epoch [14/100], Val Loss: 0.0020\n",
            "Epoch [15/100], Val Loss: 0.0020\n",
            "Epoch [16/100], Val Loss: 0.0015\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [17/100], Val Loss: 0.0013\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [18/100], Val Loss: 0.0013\n",
            "Epoch [19/100], Val Loss: 0.0010\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [20/100], Val Loss: 0.0019\n",
            "Epoch [21/100], Val Loss: 0.0009\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [22/100], Val Loss: 0.0012\n",
            "Epoch [23/100], Val Loss: 0.0012\n",
            "Epoch [24/100], Val Loss: 0.0011\n",
            "Epoch [25/100], Val Loss: 0.0017\n",
            "Epoch [26/100], Val Loss: 0.0008\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [27/100], Val Loss: 0.0011\n",
            "Epoch [28/100], Val Loss: 0.0008\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [29/100], Val Loss: 0.0009\n",
            "Epoch [30/100], Val Loss: 0.0008\n",
            "Epoch [31/100], Val Loss: 0.0009\n",
            "Epoch [32/100], Val Loss: 0.0009\n",
            "Epoch [33/100], Val Loss: 0.0009\n",
            "Epoch [34/100], Val Loss: 0.0010\n",
            "Epoch [35/100], Val Loss: 0.0010\n",
            "Epoch [36/100], Val Loss: 0.0009\n",
            "Epoch [37/100], Val Loss: 0.0008\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [38/100], Val Loss: 0.0010\n",
            "Epoch [39/100], Val Loss: 0.0008\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [40/100], Val Loss: 0.0008\n",
            "Epoch [41/100], Val Loss: 0.0008\n",
            "Epoch [42/100], Val Loss: 0.0007\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [43/100], Val Loss: 0.0008\n",
            "Epoch [44/100], Val Loss: 0.0008\n",
            "Epoch [45/100], Val Loss: 0.0007\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [46/100], Val Loss: 0.0008\n",
            "Epoch [47/100], Val Loss: 0.0008\n",
            "Epoch [48/100], Val Loss: 0.0013\n",
            "Epoch [49/100], Val Loss: 0.0010\n",
            "Epoch [50/100], Val Loss: 0.0008\n",
            "Epoch [51/100], Val Loss: 0.0008\n",
            "Epoch [52/100], Val Loss: 0.0007\n",
            "Epoch [53/100], Val Loss: 0.0007\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [54/100], Val Loss: 0.0008\n",
            "Epoch [55/100], Val Loss: 0.0008\n",
            "Epoch [56/100], Val Loss: 0.0009\n",
            "Epoch [57/100], Val Loss: 0.0008\n",
            "Epoch [58/100], Val Loss: 0.0015\n",
            "Epoch [59/100], Val Loss: 0.0007\n",
            "Validation loss improved. Model saved to lstm_model_cyclegan_best.pth\n",
            "Epoch [60/100], Val Loss: 0.0008\n",
            "Epoch [61/100], Val Loss: 0.0008\n",
            "Epoch [62/100], Val Loss: 0.0011\n",
            "Epoch [63/100], Val Loss: 0.0008\n",
            "Epoch [64/100], Val Loss: 0.0007\n",
            "Epoch [65/100], Val Loss: 0.0007\n",
            "Epoch [66/100], Val Loss: 0.0008\n",
            "Epoch [67/100], Val Loss: 0.0008\n",
            "Epoch [68/100], Val Loss: 0.0007\n",
            "Epoch [69/100], Val Loss: 0.0009\n",
            "Early stopping triggered after 10 epochs without improvement.\n",
            "Applying Laplace Approximation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation finished on the final test set.\n",
            "Evaluation Metrics: {'MSE': 2.301849842071533, 'RMSE': 1.5171848411026039, 'R2': 0.9975034594535828, 'Directional Accuracy': 0.9832102081934184}\n",
            "\n",
            "Final Results Comparison:\n",
            "Metrics from LSTM on REAL data: {'MSE': 7.706765174865723, 'RMSE': 2.776106117363982, 'R2': 0.9919368624687195, 'Directional Accuracy': 0.9463087248322147}\n",
            "Metrics from LSTM on WGAN-GP data: {'MSE': 111.48689270019531, 'RMSE': 10.558735374096432, 'R2': 0.863612711429596, 'Directional Accuracy': 0.9066487575554063}\n",
            "Metrics from LSTM on SMOTE-TS data: {'MSE': 70.21871948242188, 'RMSE': 8.379661060115849, 'R2': 0.8800274729728699, 'Directional Accuracy': 0.9324055666003976}\n",
            "Metrics from LSTM on CycleGAN data: {'MSE': 2.301849842071533, 'RMSE': 1.5171848411026039, 'R2': 0.9975034594535828, 'Directional Accuracy': 0.9832102081934184}\n"
          ]
        }
      ]
    }
  ]
}1_(1)_(14) (5).ipynb…]

